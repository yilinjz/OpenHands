{"instance_id": "astropy__astropy-12907", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "astropy__astropy-14182", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/astropy/io/ascii/rst.py\n+++ b/astropy/io/ascii/rst.py\n@@ -11,15 +11,21 @@\n \n     \"\"\"\n \n+    # Need to inherit from FixedFixedWidth to get header_rows functionality\n+    # through BaseHeader/BaseWriter easily.\n+    # SimpleRSTHeader inherits FixedFixedWidthHeader.\n+    # SimpleRSTData inherits FixedFixedWidthData.\n     _format_name = \"rst\"\n     _description = \"reStructuredText simple table\"\n     data_class = SimpleRSTData\n     header_class = SimpleRSTHeader\n \n-    def __init__(self):\n-        super().__init__(delimiter_pad=None, bookend=False)\n+    def __init__(self, **kwargs):\n+        # Ensure default RST behavior if not overridden\n+        kwargs.setdefault(\"delimiter_pad\", None)\n+        kwargs.setdefault(\"bookend\", False)\n+        super().__init__(**kwargs)\n \n     def write(self, lines):\n-        lines = super().write(lines)\n-        lines = [lines[1]] + lines + [lines[1]]\n-        return lines\n+        # lines has [header1, ..., headerN, separator, data1, ..., dataM]\n+        n_header = len(self.header.header_rows)\n+        # Separator should always exist unless table is completely empty.\n+        # Handle cases where lines might be short or separator missing.\n+        if n_header < len(lines):\n+            separator_line = lines[n_header]\n+        else:\n+            # Attempt to generate separator if missing (e.g., empty table)\n+            separator_line = self.header.get_ruler_line()\n+            # If table is truly empty (no cols), separator might be empty string.\n+            # If lines is also empty, return empty list.\n+            if not lines and not separator_line:\n+                return []\n+\n+        # Construct the final list of lines for RST format:\n+        # [separator, header1..N, separator, data1..M, separator]\n+        processed_lines = (\n+            [separator_line]\n+            + lines[:n_header]\n+            + [separator_line]\n+            + lines[n_header + 1 :]\n+            + [separator_line]\n+        )\n+        return processed_lines\n```\n"}
{"instance_id": "astropy__astropy-14365", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@ -219,8 +219,19 @@\n             current_table_comments = []\n             in_initial_comments = False\n         else:\n-            # If it's not a comment, data, new table, raise error\n-            raise ValueError(f'Unrecognized QDP line: {line}')\n+            # Potentially a command line\n+            line_lower = line.lower()\n+            if line_lower.startswith(\"read terr\"):\n+                in_initial_comments = False\n+                try:\n+                    terr_cols = [int(i) for i in line.split()[2:]]\n+                except Exception:\n+                    raise ValueError(f\"Could not parse TERR command: {line}\")\n+            elif line_lower.startswith(\"read serr\"):\n+                in_initial_comments = False\n+                try:\n+                    serr_cols = [int(i) for i in line.split()[2:]]\n+                except Exception:\n+                    raise ValueError(f\"Could not parse SERR command: {line}\")\n+            else:\n+                # If it's not a comment, data, new table, or known command, raise error\n+                raise ValueError(f\"Unrecognized QDP line: {line}\")\n \n     # Process the last table if file doesn't end with NO NO NO\n     if current_table_lines != []:\n```\n"}
{"instance_id": "astropy__astropy-14995", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/astropy/nddata/mixins/ndarithmetic.py\n+++ b/astropy/nddata/mixins/ndarithmetic.py\n@@ -33,19 +33,25 @@\n              If neither had a mask ``None`` is returned. Otherwise\n              ``handle_mask`` must create (and copy) the returned mask.\n          \"\"\"\n-         # If only one mask is present we need not bother about any type checks\n-         if (\n-             self.mask is None and operand is not None and operand.mask is None\n-         ) or handle_mask is None:\n+         # Check if handle_mask is None first\n+         if handle_mask is None:\n              return None\n-         elif self.mask is None and operand is not None:\n+ \n+         # Determine operand_mask, handling the case where operand might be None\n+         operand_mask = None\n+         if operand is not None:\n+             operand_mask = operand.mask\n+ \n+         # Case 1: Both masks are None\n+         if self.mask is None and operand_mask is None:\n+             return None\n+         # Case 2: Only self.mask exists\n+         elif self.mask is not None and operand_mask is None:\n              # Make a copy so there is no reference in the result.\n+             return deepcopy(self.mask)\n+         # Case 3: Only operand_mask exists\n+         elif self.mask is None and operand_mask is not None:\n+             # Make a copy so there is no reference in the result.\n              return deepcopy(operand.mask)\n-         elif operand is None:\n-             return deepcopy(self.mask)\n-         else:\n+         # Case 4: Both masks exist\n+         else:  # self.mask is not None and operand_mask is not None\n              # Now lets calculate the resulting mask (operation enforces copy)\n              return handle_mask(self.mask, operand.mask, **kwds)\n- \n+ \n      def _arithmetic_wcs(self, operation, operand, compare_wcs, **kwds):\n          \"\"\"\n\n```\n"}
{"instance_id": "astropy__astropy-6938", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/astropy/io/fits/fitsrec.py\n+++ b/astropy/io/fits/fitsrec.py\n@@ -17,11 +17,11 @@\n                 # decimal point\n                 value = value[1:] + '.'\n \n+            # Replace exponent separator 'E' with 'D' if format requires it\n+            if 'D' in format:\n+                value = value.replace('E', 'D')\n+\n             output_field[jdx] = value\n-\n-        # Replace exponent separator in floating point numbers\n-        if 'D' in format:\n-            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n\n\n def _get_recarray_field(array, key):\n```\n"}
{"instance_id": "astropy__astropy-7746", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/astropy/wcs/wcs.py\n+++ b/astropy/wcs/wcs.py\n@@ -1221,6 +1221,17 @@\n \n             xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n \n+            # Handle empty input early: return list of empty arrays matching\n+            # the broadcasted input shape.\n+            if xy.size == 0:\n+                empty_array = np.array([], dtype=np.float64).reshape(axes[0].shape)\n+                # If ra_dec_order is requested for output, and WCS is celestial,\n+                # return a tuple of two empty arrays.\n+                if ra_dec_order and sky == 'output' and self.has_celestial:\n+                    return (empty_array, empty_array)\n+                # Otherwise, return a list of naxis empty arrays.\n+                return [empty_array] * self.naxis\n+\n             if ra_dec_order and sky == 'input':\n                 xy = self._denormalize_sky(xy)\n             output = func(xy, origin)\n@@ -1236,6 +1247,11 @@\n                 raise ValueError(\n                     \"When providing two arguments, the array must be \"\n                     \"of shape (N, {0})\".format(self.naxis))\n+\n+            # Handle empty input early: return an empty array with shape (0, naxis).\n+            if xy.size == 0:\n+                return np.zeros((0, self.naxis), dtype=np.float64)\n+\n             if ra_dec_order and sky == 'input':\n                 xy = self._denormalize_sky(xy)\n             result = func(xy, origin)\n```\n"}
{"instance_id": "django__django-10914", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/conf/global_settings.py\n+++ b/django/conf/global_settings.py\n@@ -23,7 +23,7 @@\n \n # The numeric mode to set newly-uploaded files to. The value should be a mode\n # you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.\n-FILE_UPLOAD_PERMISSIONS = None\n+FILE_UPLOAD_PERMISSIONS = 0o644\n \n # The numeric mode to assign to newly-created directories, when uploading files.\n # The value should be a mode as you'd pass to os.chmod;\n```\n"}
{"instance_id": "django__django-10924", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1,13 +1,44 @@\n-                    \"FilePathFields must have either 'allow_files' or 'allow_folders' set to True.\",\n+import os\n+from django import forms\n+from django.core import checks\n+from django.core.exceptions import ImproperlyConfigured\n+from django.utils.translation import gettext_lazy as _\n+\n+# Assuming Field is imported correctly above this snippet\n+# from django.db.models.fields import Field\n+\n+class FilePathField(Field):\n+    description = _(\"File path\")\n+\n+    def __init__(self, verbose_name=None, name=None, path='', match=None,  # path can be str or callable\n+                 recursive=False, allow_files=True, allow_folders=False, **kwargs):\n+        # Store path as provided (string or callable)\n+        self.path = path\n+        self.match = match\n+        self.recursive = recursive\n+        self.allow_files = allow_files\n+        self.allow_folders = allow_folders\n+        kwargs['max_length'] = kwargs.get('max_length', 100)\n+        super().__init__(verbose_name, name, **kwargs)\n+\n+    # Method to get the actual path string, evaluating callables\n+    @property\n+    def effective_path(self):\n+        path = self.path\n+        if callable(path):\n+            path = path()\n+        return path\n+\n+    def _check_path_attribute(self):\n+        # Skip the check if path is callable, as it will be evaluated later.\n+        if callable(self.path):\n+            return []\n+        # Check if the path is a directory only if it's a string\n+        if not os.path.isdir(self.path):\n+            return [\n+                checks.Error(\n+                    \"FilePathField's path argument must point to a directory.\",\n+                    hint=\"The path '{}' is not a directory.\".format(self.path),\n                     obj=self,\n-                    id='fields.E140',\n+                    id='fields.E141',\n                 )\n             ]\n         return []\n@@ -15,9 +46,20 @@\n     def check(self, **kwargs):\n         errors = super().check(**kwargs)\n         errors.extend(self._check_path_attribute())\n+        # Use a single check for allow_files/allow_folders\n         if not self.allow_files and not self.allow_folders:\n+            errors.append(\n+                checks.Error(\n+                    \"FilePathFields must have either 'allow_files' or 'allow_folders' set to True.\",\n+                    obj=self,\n+                    id='fields.E140',\n+                )\n+            )\n+        return errors\n+\n+    def deconstruct(self):\n+        name, import_path, args, kwargs = super().deconstruct() # Use import_path as variable name\n         if self.path != '':\n-            kwargs['path'] = self.path\n+            kwargs['path'] = self.path  # Will be serialized by migration framework\n         if self.match is not None:\n             kwargs['match'] = self.match\n         if self.recursive is not False:\n@@ -26,9 +68,10 @@\n             kwargs['allow_files'] = self.allow_files\n         if self.allow_folders is not False:\n             kwargs['allow_folders'] = self.allow_folders\n+        # Remove default max_length if present\n         if kwargs.get(\"max_length\") == 100:\n             del kwargs[\"max_length\"]\n-        return name, path, args, kwargs\n+        return name, import_path, args, kwargs\n \n     def get_prep_value(self, value):\n         value = super().get_prep_value(value)\n@@ -37,13 +80,26 @@\n         return str(value)\n \n     def formfield(self, **kwargs):\n+        # Evaluate the path when the form field is needed\n+        try:\n+            path = self.effective_path\n+            # Check if the evaluated path is a directory *here*\n+            if not os.path.isdir(path):\n+                raise ValueError(\n+                    \"FilePathField's path '{}' is not a directory.\".format(path)\n+                )\n+        except Exception as e:\n+            # Handle potential errors during callable evaluation or path checking\n+            raise ImproperlyConfigured(\n+                f\"FilePathField's path callable failed to return a valid directory: {e}\"\n+            ) from e\n+\n+        # Pass the evaluated path to the form field\n         return super().formfield(**{\n-            'path': self.path,\n+            'path': path,\n             'match': self.match,\n             'recursive': self.recursive,\n             'form_class': forms.FilePathField,\n             'allow_files': self.allow_files,\n             'allow_folders': self.allow_folders,\n             **kwargs,\n         })\n```\n"}
{"instance_id": "django__django-11001", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -30,14 +30,20 @@\n                     raise DatabaseError('ORDER BY term does not match any column in the result set.')\n             sql, params = self.compile(resolved)\n             # Don't add the same column twice, but the order direction is\n-            # not taken into account so we strip it. When this entire method\n-            # is refactored into expressions, then we can check each part as we\n-            # generate it.\n-            without_ordering = self.ordering_parts.search(sql).group(1)\n+            # not taken into account so we strip it.\n+            # To handle multiline SQL, replace newlines with spaces and strip\n+            # leading/trailing whitespace before applying the regex.\n+            sql_oneline = ' '.join(sql.splitlines()).strip()\n+            match = self.ordering_parts.match(sql_oneline)\n+            if match:\n+                without_ordering = match.group(1).strip()\n+            else:\n+                # Fall back to the whole string if ASC/DESC wasn't found\n+                # (e.g., backend specific syntax).\n+                without_ordering = sql_oneline\n             params_hash = make_hashable(params)\n             if (without_ordering, params_hash) in seen:\n                 continue\n             seen.add((without_ordering, params_hash))\n             result.append((resolved, (sql, params, is_ref)))\n         return result\n-\n-    def get_extra_select(self, order_by, select):\n-        extra_select = []\n-        if self.query.distinct and not self.query.distinct_fields:\n-            select_sql = [t[1] for t in select]\n-            for expr, (sql, params, is_ref) in order_by:\n-                without_ordering = self.ordering_parts.search(sql).group(1)\n-                if not is_ref and (without_ordering, params) not in select_sql:\n+@@ -43,10 +49,16 @@\n+     def get_extra_select(self, order_by, select):\n+         extra_select = []\n+         if self.query.distinct and not self.query.distinct_fields:\n+-            select_sql = [t[1] for t in select]\n++            # Create a set of the existing select clauses' SQL and params.\n++            select_sql = {(sql, params) for _, (sql, params), _ in select}\n+             for expr, (sql, params, is_ref) in order_by:\n+-                without_ordering = self.ordering_parts.search(sql).group(1)\n++                # To handle multiline SQL, replace newlines with spaces and strip\n++                # leading/trailing whitespace before applying the regex.\n++                sql_oneline = ' '.join(sql.splitlines()).strip()\n++                match = self.ordering_parts.match(sql_oneline)\n++                if match:\n++                    without_ordering = match.group(1).strip()\n++                else:\n++                    # Fall back to the whole string if ASC/DESC wasn't found.\n++                    without_ordering = sql_oneline\n+                 if not is_ref and (without_ordering, params) not in select_sql:\n+                     # Add the ordering clause to the select clause if it isn't\n+                     # already present and not a reference to an existing item.\n```\n"}
{"instance_id": "django__django-11019", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/widgets.py\n+++ b/django/forms/widgets.py\n@@ -1,5 +1,6 @@\n \"\"\"\n HTML Widget classes\n \"\"\"\n+import collections\n import copy\n import datetime\n import json\n@@ -26,6 +27,10 @@\n logger = logging.getLogger('django.forms.widgets')\n \n MEDIA_TYPES = ('css', 'js')\n+# Increase stacklevel on warnings to show the caller of the Media properties,\n+# rather than the internals of the property itself.\n+MEDIA_PROPERTY_STACKLEVEL = 3\n+MERGE_LISTS_STACKLEVEL = MEDIA_PROPERTY_STACKLEVEL + 1\n \n \n class MediaOrderConflictWarning(RuntimeWarning):\n@@ -63,6 +68,73 @@\n     def __str__(self):\n         return self.render()\n \n+    @staticmethod\n+    def _merge_lists(lists):\n+        \"\"\"\n+        Merge lists of paths, preserving partial orderings and detecting\n+        cycles.\n+\n+        Combine all lists into a single list of lists, filtering empty ones.\n+        \"\"\"\n+        all_lists = [lst for lst in lists if lst]\n+        if not all_lists:\n+            return []\n+\n+        # 1. Collect all unique items and build initial graph structures.\n+        all_items = set()\n+        for lst in all_lists:\n+            all_items.update(lst)\n+\n+        # Use OrderedDict for deterministic iteration, avoids sorting later.\n+        successors = collections.OrderedDict((item, set()) for item in all_items)\n+        in_degree = collections.OrderedDict((item, 0) for item in all_items)\n+\n+        # 2. Populate graph based on ordering within each list.\n+        for lst in all_lists:\n+            for i in range(len(lst) - 1):\n+                u, v = lst[i], lst[i + 1]\n+                # Add edge u -> v if not already present.\n+                if u != v and v not in successors[u]:\n+                    # Check for immediate cycle (v -> u already exists).\n+                    # This detects direct pairwise conflicts early.\n+                    if u in successors.get(v, set()):\n+                        warnings.warn(\n+                            'Detected conflicting ordering constraints for %s '\n+                            'and %s.' % (v, u),\n+                            MediaOrderConflictWarning,\n+                            stacklevel=MERGE_LISTS_STACKLEVEL,\n+                        )\n+                        # Don't add the edge that creates the immediate cycle.\n+                        continue\n+                    successors[u].add(v)\n+                    in_degree[v] += 1\n+\n+        # 3. Topological Sort (Kahn's algorithm).\n+        # Use a standard list for the queue, items are added in discovery order.\n+        queue = collections.deque([item for item in all_items if in_degree[item] == 0])\n+        result = []\n+\n+        while queue:\n+            u = queue.popleft()\n+            result.append(u)\n+\n+            # Sort successors to ensure deterministic output for items with no\n+            # explicit order relative to each other.\n+            for v in sorted(list(successors[u])):\n+                in_degree[v] -= 1\n+                if in_degree[v] == 0:\n+                    queue.append(v)\n+\n+        # 4. Check for cycles (conflict detection).\n+        if len(result) < len(all_items):\n+            # Find items involved in cycles (those with in_degree > 0).\n+            involved_items = sorted([item for item in all_items if in_degree[item] > 0])\n+            warnings.warn(\n+                'Detected an ordering cycle among media files: %s. '\n+                'Relative order may be unreliable.' % (', '.join(involved_items)),\n+                MediaOrderConflictWarning, stacklevel=MERGE_LISTS_STACKLEVEL,\n+            )\n+            # Append remaining items (involved in cycles) sorted alphabetically\n+            # for deterministic, albeit potentially incorrect, order.\n+            result.extend(involved_items)\n+\n+        return result\n+\n     @property\n     def _css(self):\n         \"\"\"\n@@ -70,49 +142,25 @@\n         \"\"\"\n         # This is computationally intensive, so cache the result.\n         if not hasattr(self, '_css_cache'):\n-            css = self._css_lists[0]\n+            # Collect all media types and paths per media type.\n+            media_paths = collections.defaultdict(list)\n+            all_media_types = set()\n             # filter(None, ...) avoids calling merge with empty dicts.\n-            for obj in filter(None, self._css_lists[1:]):\n-                css = {\n-                    medium: self.merge(css.get(medium, []), obj.get(medium, []))\n-                    for medium in css.keys() | obj.keys()\n-                }\n-            self._css_cache = css\n+            for css_dict in filter(None, self._css_lists):\n+                all_media_types.update(css_dict.keys())\n+                for medium, paths in css_dict.items():\n+                    if paths:  # Only add if there are paths.\n+                        media_paths[medium].append(paths)\n+\n+            # Merge paths for each media type.\n+            merged_css = {}\n+            # Sort media types for consistent output order of <link> tags.\n+            for medium in sorted(list(all_media_types)):\n+                paths_for_medium = self._merge_lists(media_paths[medium])\n+                if paths_for_medium:  # Only include media types with files.\n+                    merged_css[medium] = paths_for_medium\n+            self._css_cache = merged_css\n         return self._css_cache\n-\n-    @staticmethod\n-    def merge(list_1, list_2):\n-        \"\"\"\n-        Merge two lists while trying to keep the relative order of the elements.\n-        Warn if the lists have the same two elements in a different relative\n-        order.\n-\n-        For static assets it can be important to have them included in the DOM\n-        in a certain order. In JavaScript you may not be able to reference a\n-        global or in CSS you might want to override a style.\n-        \"\"\"\n-        # Start with a copy of list_1.\n-        combined_list = list(list_1)\n-        last_insert_index = len(list_1)\n-        # Walk list_2 in reverse, inserting each element into combined_list if\n-        # it doesn't already exist.\n-        for path in reversed(list_2):\n-            try:\n-                # Does path already exist in the list?\n-                index = combined_list.index(path)\n-            except ValueError:\n-                # Add path to combined_list since it doesn't exist.\n-                combined_list.insert(last_insert_index, path)\n-            else:\n-                if index > last_insert_index:\n-                    warnings.warn(\n-                        'Detected duplicate Media files in an opposite order:\\n'\n-                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n-                        MediaOrderConflictWarning,\n-                    )\n-                # path already exists in the list. Update last_insert_index so\n-                # that the following elements are inserted in front of this one.\n-                last_insert_index = index\n-        return combined_list\n \n     @property\n     def _js(self):\n@@ -121,11 +169,8 @@\n         \"\"\"\n         # This is computationally intensive, so cache the result.\n         if not hasattr(self, '_js_cache'):\n-            js = self._js_lists[0]\n             # filter(None, ...) avoids calling merge() with empty lists.\n-            for obj in filter(None, self._js_lists[1:]):\n-                js = self.merge(js, obj)\n-            self._js_cache = js\n+            self._js_cache = self._merge_lists(filter(None, self._js_lists))\n         return self._js_cache\n \n     def render(self):\n@@ -171,32 +216,6 @@\n             return Media(**{str(name): getattr(self, '_' + name)})\n         raise KeyError('Unknown media type \"%s\"' % name)\n \n-    @staticmethod\n-    def merge(list_1, list_2):\n-        \"\"\"\n-        Merge two lists while trying to keep the relative order of the elements.\n-        Warn if the lists have the same two elements in a different relative\n-        order.\n-\n-        For static assets it can be important to have them included in the DOM\n-        in a certain order. In JavaScript you may not be able to reference a\n-        global or in CSS you might want to override a style.\n-        \"\"\"\n-        # Start with a copy of list_1.\n-        combined_list = list(list_1)\n-        last_insert_index = len(list_1)\n-        # Walk list_2 in reverse, inserting each element into combined_list if\n-        # it doesn't already exist.\n-        for path in reversed(list_2):\n-            try:\n-                # Does path already exist in the list?\n-                index = combined_list.index(path)\n-            except ValueError:\n-                # Add path to combined_list since it doesn't exist.\n-                combined_list.insert(last_insert_index, path)\n-            else:\n-                if index > last_insert_index:\n-                    warnings.warn(\n-                        'Detected duplicate Media files in an opposite order:\\n'\n-                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n-                        MediaOrderConflictWarning,\n-                    )\n-                # path already exists in the list. Update last_insert_index so\n-                # that the following elements are inserted in front of this one.\n-                last_insert_index = index\n-        return combined_list\n-\n     def __add__(self, other):\n         combined = Media()\n         combined._css_lists = self._css_lists + other._css_lists\n@@ -213,7 +232,7 @@\n         try:\n             base = sup_cls.media\n         except AttributeError:\n-            base = Media()\n+            base = Media(css={}, js=[])\n \n         # Get the media definition for this class\n         definition = getattr(cls, 'Media', None)\n```\n"}
{"instance_id": "django__django-11039", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/management/commands/sqlmigrate.py\n+++ b/django/core/management/commands/sqlmigrate.py\n@@ -52,7 +52,7 @@\n         schema_editor = connection.schema_editor(collect_sql=True, atomic=migration.atomic)\n \n         # Show begin/commit statements?\n-        self.output_transaction = migration.atomic\n+        self.output_transaction = migration.atomic and connection.features.can_rollback_ddl\n \n         # Make internal list of operations - ones that are overridden\n         # by a database-specific implementation take precedence.\n--- a/tests/migrations/test_commands.py\n+++ b/tests/migrations/test_commands.py\n@@ -1,6 +1,7 @@\n import io\n import os\n import sys\n+from unittest.mock import patch\n \n from django.apps import apps\n from django.core.management import CommandError, call_command\n@@ -252,6 +253,23 @@\n         self.assertIn(\"COMMIT;\", output)\n         self.assertIn(\"CREATE TABLE\", output)  # And some SQL\n \n+    @override_settings(MIGRATION_MODULES={'migrations': 'migrations.test_migrations'})\n+    def test_sqlmigrate_atomic_no_transaction_on_incapable_db(self):\n+        \"\"\"\n+        sqlmigrate doesn't wrap atomic migrations in a transaction if the\n+        database backend doesn't support transactional DDL.\n+        \"\"\"\n+        # Use a migration known to be atomic (the default)\n+        app_label = 'migrations'\n+        migration_name = '0001_initial'\n+        # Mock can_rollback_ddl to False\n+        with patch.object(connection.features, 'can_rollback_ddl', False):\n+            out = io.StringIO()\n+            call_command('sqlmigrate', app_label, migration_name, stdout=out)\n+            output = out.getvalue()\n+            self.assertNotIn(connection.ops.start_transaction_sql(), output)\n+            self.assertNotIn(\"COMMIT;\", output)\n+            self.assertIn(\"CREATE TABLE\", output)  # And some SQL\n \n @override_settings(MIGRATION_MODULES={'migrations': 'migrations.test_migrations_non_atomic'})\n class NonAtomicSQLMigrateTests(TestCase):\n```\n"}
{"instance_id": "django__django-11049", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/fields.py\n+++ b/django/forms/fields.py\n@@ -29,7 +29,8 @@\n class DurationField(Field):\n     default_error_messages = {\n         'invalid': _('Enter a valid duration.'),\n-        'overflow': _('The number of days must be between {min_days} and {max_days}.')\n+        'overflow': _('The number of days must be between {min_days} and {max_days}.'),\n+        'invalid_format': _('Enter a valid duration. Format: [DD] [[HH:]MM:]ss[.uuuuuu]'),\n     }\n \n     def prepare_value(self, value):\n@@ -45,7 +46,10 @@\n         if isinstance(value, datetime.timedelta):\n             return value\n         try:\n-            value = parse_duration(str(value))\n+            value = parse_duration(str(value)) # parse_duration raises ValueError on invalid format\n+            if value is None:\n+                # parse_duration returns None for invalid formats like '1:2:3:4'\n+                raise ValueError()\n         except OverflowError:\n             raise ValidationError(self.error_messages['overflow'].format(\n                 min_days=datetime.timedelta.min.days,\n@@ -53,3 +57,5 @@\n             ), code='overflow')\n         except ValueError:\n             raise ValidationError(self.error_messages['invalid_format'], code='invalid_format')\n+        if value is None:\n+            raise ValidationError(self.error_messages['invalid'], code='invalid')\n```\n"}
{"instance_id": "django__django-11099", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/auth/validators.py\n+++ b/django/contrib/auth/validators.py\n@@ -6,7 +6,7 @@\n \n @deconstructible\n class ASCIIUsernameValidator(validators.RegexValidator):\n-    regex = r'^[\\w.@+-]+$'\n+    regex = r'\\A[\\w.@+-]+\\Z'\n     message = _(\n         'Enter a valid username. This value may contain only English letters, '\n         'numbers, and @/./+/-/_ characters.'\n@@ -16,7 +16,7 @@\n \n @deconstructible\n class UnicodeUsernameValidator(validators.RegexValidator):\n-    regex = r'^[\\w.@+-]+$'\n+    regex = r'\\A[\\w.@+-]+\\Z'\n     message = _(\n         'Enter a valid username. This value may contain only letters, '\n         'numbers, and @/./+/-/_ characters.'\n```\n"}
{"instance_id": "django__django-11133", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -28,15 +28,18 @@\n \n     @content.setter\n     def content(self, value):\n-        # Consume iterators upon assignment to allow repeated iteration.\n-        if hasattr(value, '__iter__') and not isinstance(value, (bytes, str)):\n+        # Handle memoryview explicitly by converting it to bytes.\n+        if isinstance(value, memoryview):\n+            content = bytes(value)\n+        # Consume other iterators upon assignment to allow repeated iteration.\n+        elif hasattr(value, '__iter__') and not isinstance(value, (bytes, str)):\n             content = b''.join(self.make_bytes(chunk) for chunk in value)\n             if hasattr(value, 'close'):\n                 try:\n                     value.close()\n                 except Exception:\n                     pass\n-        else:\n+        else:  # Handle bytes and str.\n             content = self.make_bytes(value)\n         # Create a list of properly encoded bytestrings to support write().\n         self._container = [content]\n```\n"}
{"instance_id": "django__django-11179", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -31,14 +31,15 @@\n         deleted_counter = Counter()\n \n         # Optimize for the case with a single obj and no dependencies\n-        if len(self.data) == 1 and len(instances) == 1:\n-            instance = list(instances)[0]\n-            if self.can_fast_delete(instance):\n-                with transaction.mark_for_rollback_on_error():\n-                    count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n-                return count, {model._meta.label: count}\n+        if len(self.data) == 1:\n+            model = list(self.data.keys())[0]\n+            instances = list(self.data.values())[0]\n+            if len(instances) == 1:\n+                instance = instances[0]\n+                if self.can_fast_delete(instance):\n+                    with transaction.mark_for_rollback_on_error(using=self.using):\n+                        count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n+                    setattr(instance, model._meta.pk.attname, None)\n+                    return count, {model._meta.label: count}\n \n         with transaction.atomic(using=self.using, savepoint=False):\n             # send pre_delete signals\n```\n"}
{"instance_id": "django__django-11283", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/auth/migrations/0011_update_proxy_permissions.py\n+++ b/django/contrib/auth/migrations/0011_update_proxy_permissions.py\n@@ -1,30 +1,66 @@\n from django.db import migrations\n-from django.db.models import Q\n \n \n def update_proxy_model_permissions(apps, schema_editor, reverse=False):\n     \"\"\"\n     Update the content_type of proxy model permissions to use the ContentType\n-    of the proxy model.\n+    of the proxy model, or revert them to the concrete model's ContentType.\n+\n+    If a permission exists for the target content_type, the permission for the\n+    source content_type is removed.\n     \"\"\"\n     Permission = apps.get_model('auth', 'Permission')\n     ContentType = apps.get_model('contenttypes', 'ContentType')\n+    db_alias = schema_editor.connection.alias\n+\n     for Model in apps.get_models():\n         opts = Model._meta\n         if not opts.proxy:\n             continue\n-        proxy_default_permissions_codenames = [\n-            '%s_%s' % (action, opts.model_name)\n-            for action in opts.default_permissions\n-        ]\n-        permissions_query = Q(codename__in=proxy_default_permissions_codenames)\n+\n+        concrete_model = opts.concrete_model\n+        try:\n+            # Ensure we are using the ContentType from the correct app state\n+            concrete_content_type = ContentType.objects.using(db_alias).get_for_model(\n+                concrete_model, for_concrete_model=True\n+            )\n+            proxy_content_type = ContentType.objects.using(db_alias).get_for_model(\n+                Model, for_concrete_model=False\n+            )\n+        except ContentType.DoesNotExist:\n+            # Content types might not exist in edge cases (e.g. removing app\n+            # without migrating first).\n+            continue\n+\n+        if concrete_content_type == proxy_content_type:\n+            # This model's proxy permissions are already correct.\n+            continue\n+\n+        if reverse:\n+            source_content_type = proxy_content_type\n+            target_content_type = concrete_content_type\n+        else:\n+            source_content_type = concrete_content_type\n+            target_content_type = proxy_content_type\n+\n+        # Permissions codenames to check\n+        permissions_codenames = set()\n+        # Default permissions\n+        for action in opts.default_permissions:\n+            permissions_codenames.add('%s_%s' % (action, opts.model_name))\n+        # Custom permissions\n         for codename, name in opts.permissions:\n-            permissions_query = permissions_query | Q(codename=codename, name=name)\n-        concrete_content_type = ContentType.objects.get_for_model(Model, for_concrete_model=True)\n-        proxy_content_type = ContentType.objects.get_for_model(Model, for_concrete_model=False)\n-        old_content_type = proxy_content_type if reverse else concrete_content_type\n-        new_content_type = concrete_content_type if reverse else proxy_content_type\n-        Permission.objects.filter(\n-            permissions_query,\n-            content_type=old_content_type,\n-        ).update(content_type=new_content_type)\n+            permissions_codenames.add(codename)\n+\n+        # Find permissions associated with the source content type.\n+        source_permissions = Permission.objects.using(db_alias).filter(\n+            content_type=source_content_type,\n+            codename__in=permissions_codenames,\n+        )\n+        # Find existing permissions associated with the target content type.\n+        target_permission_codenames = set(\n+            Permission.objects.using(db_alias).filter(\n+                content_type=target_content_type,\n+                codename__in=permissions_codenames,\n+            ).values_list('codename', flat=True)\n+        )\n+\n+        permissions_to_delete_pks = []\n+        permissions_to_update_pks = []\n+        for perm in source_permissions:\n+            if perm.codename in target_permission_codenames:\n+                # Permission exists on the target content type, remove the\n+                # source permission.\n+                permissions_to_delete_pks.append(perm.pk)\n+            else:\n+                # Permission doesn't exist on the target content type, update\n+                # the source permission.\n+                permissions_to_update_pks.append(perm.pk)\n+                # Add codename to target set to handle potential duplicates\n+                # within the source_permissions list itself (which shouldn't\n+                # happen with the unique constraint but is safer).\n+                target_permission_codenames.add(perm.codename)\n+\n+        # Perform bulk operations.\n+        if permissions_to_delete_pks:\n+            Permission.objects.using(db_alias).filter(pk__in=permissions_to_delete_pks).delete()\n+        if permissions_to_update_pks:\n+            Permission.objects.using(db_alias).filter(pk__in=permissions_to_update_pks).update(\n+                content_type=target_content_type\n+            )\n \n \n def revert_proxy_model_permissions(apps, schema_editor):\n```\n"}
{"instance_id": "django__django-11422", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/utils/autoreload.py\n+++ b/django/utils/autoreload.py\n@@ -243,7 +243,7 @@\n             sys_file_paths.append(origin)\n \n     results = set()\n-    for filename in itertools.chain(sys_file_paths, extra_files):\n+    for filename in itertools.chain(sys_file_paths, extra_files, [sys.argv[0]]):\n         if not filename:\n             continue\n         path = pathlib.Path(filename)\n\n```\n"}
{"instance_id": "django__django-11564", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/templatetags/static.py\n+++ b/django/templatetags/static.py\n@@ -1,10 +1,12 @@\n from urllib.parse import quote, urljoin\n \n from django import template\n-from django.apps import apps\n from django.conf import settings\n from django.templatetags.static import PrefixNode as OldPrefixNode\n from django.utils.encoding import iri_to_uri\n from django.utils.html import conditional_escape\n+from django.utils.http import prepend_script_name_if_relative\n+\n+from django.apps import apps # Keep this import\n \n register = template.Library()\n \n@@ -29,10 +31,17 @@\n         return prefix\n \n     def render(self, context):\n+        # Get request and script_name from context if available\n+        request = context.get('request')\n+        script_name = getattr(request, 'META', {}).get('SCRIPT_NAME') if request else None\n+\n+        # Get the base prefix (STATIC_URL or MEDIA_URL) from settings\n         prefix = self.handle_simple(self.name)\n+\n+        # Prepend script_name if the prefix is relative\n+        if script_name:\n+            prefix = prepend_script_name_if_relative(prefix, script_name)\n+\n         if self.varname is None:\n             return prefix\n         context[self.varname] = prefix\n@@ -68,12 +77,26 @@\n         self.varname = varname\n \n     def url(self, context):\n+        # Get request and script_name from context if available\n+        request = context.get('request')\n+        script_name = getattr(request, 'META', {}).get('SCRIPT_NAME') if request else None\n+\n+        # Resolve the path from the template variable\n         path = self.path.resolve(context)\n-        return self.handle_simple(path)\n+        quoted_path = quote(path)\n+\n+        if apps.is_installed('django.contrib.staticfiles'):\n+            from django.contrib.staticfiles.storage import staticfiles_storage\n+            # Get the URL from the staticfiles storage backend\n+            url = staticfiles_storage.url(path)\n+        else:\n+            # Build the URL manually using STATIC_URL from settings\n+            base_url = PrefixNode.handle_simple(\"STATIC_URL\")\n+            url = urljoin(base_url, quoted_path)\n+\n+        # Prepend script_name if the resulting URL is relative\n+        if script_name:\n+            url = prepend_script_name_if_relative(url, script_name)\n+\n+        return url\n \n     def render(self, context):\n         url = self.url(context)\n         if context.autoescape:\n+            # Ensure conditional_escape is still applied correctly\n             url = conditional_escape(url)\n         if self.varname is None:\n             return url\n@@ -81,16 +104,6 @@\n         return ''\n \n     @classmethod\n-    def handle_simple(cls, path):\n-        if apps.is_installed('django.contrib.staticfiles'):\n-            from django.contrib.staticfiles.storage import staticfiles_storage\n-            return staticfiles_storage.url(path)\n-        else:\n-            # This logic is now moved into the url method to access context/request\n-            # return urljoin(PrefixNode.handle_simple(\"STATIC_URL\"), quote(path))\n-            raise RuntimeError(\"This method should not be called directly anymore if context is needed.\") # Or adjust if needed elsewhere\n-\n-    @classmethod\n     def handle_token(cls, parser, token):\n         \"\"\"\n         Class method to parse prefix node and return a Node.\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -1,5 +1,6 @@\n import datetime\n import re\n+from urllib.parse import urljoin, urlparse\n from urllib.parse import quote as urlquote\n from urllib.parse import urlsplit, urlunsplit\n \n@@ -10,6 +11,22 @@\n from django.utils.regex_helper import _lazy_re_compile\n from django.utils.translation import get_language\n \n+\n+def prepend_script_name_if_relative(url_string, script_name):\n+    \"\"\"\n+    Prepend SCRIPT_NAME to a URL path if the URL is relative and SCRIPT_NAME\n+    is not empty.\n+    \"\"\"\n+    if not script_name:\n+        return url_string\n+    # Don't modify absolute URLs (with scheme or netloc)\n+    parsed = urlparse(url_string)\n+    if parsed.scheme or parsed.netloc:\n+        return url_string\n+    # Ensure script_name ends with a slash for urljoin.\n+    script_prefix = script_name.rstrip('/') + '/'\n+    return urljoin(script_prefix, url_string.lstrip('/'))\n+\n RFC3986_SUBDELIMS = \"!$&'()*+,;=\"\n RFC3986_GENDELIMS = \":/?#[]@\"\n \n```\n"}
{"instance_id": "django__django-11583", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "django__django-11620", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/views/debug.py\n+++ b/django/views/debug.py\n@@ -42,7 +42,10 @@\n     caller = ''\n     try:\n         resolver_match = resolve(request.path)\n-    except Resolver404:\n+    except Http404:  # catch plain Http404 from converters during resolve()\n+        # If the Http404 was raised during URL resolution, trying to\n+        # resolve the path again will likely fail with the same error.\n+        # In this case, we cannot determine the view function name.\n         pass\n     else:\n         obj = resolver_match.func\n```\n"}
{"instance_id": "django__django-11630", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/checks/model_checks.py\n+++ b/django/core/checks/model_checks.py\n@@ -1,10 +1,18 @@\n+# This patch modifies the creation of the check message for E028/W045.\n+# Assumes `settings` and `Warning` are imported at the top of the file.\n+# Assumes this code runs inside a loop (`for db_table...`) and conditional\n+# (`if len(db_table_models[db_table]) > 1:`).\n+            from django.conf import settings\n+            from django.core.checks import Warning\n+\n+            is_multi_db = len(settings.DATABASES) > 1\n+            model_list = ', '.join(db_table_models[db_table])\n+            msg = \"db_table '%s' is used by multiple models: %s.\" % (db_table, model_list)\n+            # Downgrade to Warning if multiple databases are configured, assuming routing might handle it.\n+            check_class = Warning if is_multi_db else Error\n+            # Use a new ID 'models.W045' for the warning to allow specific silencing.\n+            check_id = 'models.W045' if is_multi_db else 'models.E028'\n+            hint = \"Ensure database routing prevents clashes in multi-DB setups.\" if is_multi_db else None\n+            check_class(\n+                msg,\n+                obj=db_table,\n+                id=check_id,\n+                hint=hint,\n+            )\n                 Error(\n-                     \"db_table '%s' is used by multiple models: %s.\"\n-                     % (db_table, ', '.join(db_table_models[db_table])),\n-                     obj=db_table,\n-                     id='models.E028',\n-                 )\n-             )\n      for index_name, model_labels in indexes.items():\n          if len(model_labels) > 1:\n              model_labels = set(model_labels)\n```\n"}
{"instance_id": "django__django-11742", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1,4 +1,5 @@\n \"\"\"\n+from itertools import chain\n\n from django.core import checks, exceptions, validators\n from django.db import connection, connections, router\n@@ -285,6 +286,13 @@\n class CharField(Field):\n     description = _(\"String (up to %(max_length)s)\")\n\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        if self.max_length is not None:\n+            self.validators.append(validators.MaxLengthValidator(self.max_length))\n+\n+    def check(self, **kwargs):\n+        errors = super().check(**kwargs)\n+        errors.extend(self._check_max_length_attribute(**kwargs))\n+        errors.extend(self._check_choices_max_length())\n+        return errors\n\n     def _check_max_length_attribute(self, **kwargs):\n         if self.max_length is None:\n@@ -307,6 +315,34 @@\n         else:\n             return []\n\n+    def _check_choices_max_length(self):\n+        if not self.choices or not isinstance(self.max_length, int) or self.max_length <= 0:\n+            # Either choices or max_length is not set, or max_length is invalid.\n+            # No check needed or max_length check will fail elsewhere.\n+            return []\n+\n+        # Use flatten_choices to handle nested structures.\n+        choices_list = list(self.flatten_choices(self.choices))\n+        if not choices_list:\n+            return []\n+\n+        max_choice_length = 0\n+        # Iterate through flattened choices (value, display_name)\n+        for value, _ in choices_list:\n+            # Ensure value is treated as string for length check\n+            choice_len = len(str(value))\n+            if choice_len > max_choice_length:\n+                max_choice_length = choice_len\n+\n+        if max_choice_length > self.max_length:\n+            return [\n+                checks.Error(\n+                    \"'max_length' is too small to fit the longest value in 'choices'.\",\n+                    hint=f\"The longest choice value has length {max_choice_length}, but 'max_length' is {self.max_length}.\",\n+                    obj=self,\n+                    id='fields.E122',\n+                )\n+            ]\n+        return []\n\n     def cast_db_type(self, connection):\n         if self.max_length is None:\n```\n"}
{"instance_id": "django__django-11797", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1311,7 +1311,11 @@ class Query(BaseExpression):\n                         annotation for annotation in existing_annotations\n                         if getattr(annotation, 'contains_aggregate', True)\n                     )\n-                    if inner_query.default_cols and has_existing_aggregate_annotations:\n+                    # Don't override an existing explicit GROUP BY clause when\n+                    # the inner query doesn't use default columns. The GROUP BY\n+                    # PK is only needed when the inner query selects default\n+                    # columns and aggregates are present.\n+                    if inner_query.default_cols and has_existing_aggregate_annotations and inner_query.group_by is None:\n                         inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n                     inner_query.default_cols = False\n \n```\n"}
{"instance_id": "django__django-11815", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/serializer.py\n+++ b/django/db/migrations/serializer.py\n@@ -161,9 +161,9 @@\n     def serialize(self):\n         enum_class = self.value.__class__\n         module = enum_class.__module__\n-        v_string, v_imports = serializer_factory(self.value.value).serialize()\n-        imports = {'import %s' % module, *v_imports}\n-        return \"%s.%s(%s)\" % (module, enum_class.__name__, v_string), imports\n+        member_name = self.value.name\n+        imports = {'import %s' % module}\n+        return \"%s.%s[%r]\" % (module, enum_class.__name__, member_name), imports\n\n\n class FloatSerializer(BaseSimpleSerializer):\n```\n"}
{"instance_id": "django__django-11848", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -15,11 +15,17 @@\n     try:\n         year = int(m.group('year'))\n         if year < 100:\n-            if year < 70:\n-                year += 2000\n-            else:\n-                year += 1900\n+            # RFC 7231 Section 7.1.1.1:\n+            # Recipients of a timestamp value in rfc850-date format, which\n+            # uses a two-digit year, MUST interpret a timestamp that appears\n+            # to be more than 50 years in the future as representing the most\n+            # recent year in the past that had the same last two digits.\n+            current_year = datetime.datetime.utcnow().year\n+            # Assume the year is in the current century\n+            year += (current_year // 100) * 100\n+            # If that interpretation is more than 50 years in the future,\n+            # subtract 100 years.\n+            if year > current_year + 50:\n+                year -= 100\n         month = MONTHS.index(m.group('mon').lower()) + 1\n         day = int(m.group('day'))\n         hour = int(m.group('hour'))\n```\n"}
{"instance_id": "django__django-11905", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -27,8 +27,13 @@\n         else:\n             col = _get_col(targets[0], join_info.final_field, alias, simple_col)\n \n+        lookup_type = lookups[-1]\n+        if lookup_type == 'isnull' and not isinstance(value, bool):\n+            raise ValueError(\n+                'The __isnull lookup must be used with a boolean value '\n+                '(True or False).'\n+            )\n         condition = self.build_lookup(lookups, col, value)\n-        lookup_type = condition.lookup_name\n         clause.add(condition, AND)\n \n         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated\n```\n"}
{"instance_id": "django__django-11910", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/operations/fields.py\n+++ b/django/db/migrations/operations/fields.py\n@@ -1,6 +1,8 @@\n from django.core.exceptions import FieldDoesNotExist\n from django.db.models.fields import NOT_PROVIDED\n+from django.db.models.fields.related import (\n+    ForeignObjectRel, ManyToManyField, OneToOneField,\n+)\n from django.utils.functional import cached_property\n\n from .base import Operation\n@@ -137,6 +139,7 @@\n             raise FieldDoesNotExist(\n                 \"%s.%s has no field named '%s'\" % (app_label, self.model_name, self.old_name)\n             )\n+        was_primary_key = old_field_instance.primary_key\n\n         # Rename the field\n         model_state = state.models[app_label, self.model_name_lower]\n@@ -182,6 +185,45 @@\n         if options.get('ordering') == ['-' + self.old_name]:\n             options['ordering'] = ['-' + self.new_name]\n\n+        # Update referring ForeignKey/OneToOneFields fields state.\n+        target_model_ref_str = '%s.%s' % (app_label, self.model_name_lower)\n+        for (other_app_label, other_model_name), other_model_state in state.models.items():\n+            current_other_fields = list(other_model_state.fields)\n+            fields_changed = False\n+            new_other_fields = []\n+            for field_name, field in current_other_fields:\n+                field_to_add = field\n+                if (\n+                    hasattr(field, 'remote_field') and field.remote_field and\n+                    not isinstance(field.remote_field, ForeignObjectRel) and\n+                    not isinstance(field, ManyToManyField)\n+                ):\n+                    # Resolve the remote model reference string/class.\n+                    remote_model_key_str = None\n+                    if isinstance(field.remote_field.model, str):\n+                        remote_model_key_str = field.remote_field.model.lower()\n+                        if '.' not in remote_model_key_str:\n+                            remote_model_key_str = '%s.%s' % (other_app_label, remote_model_key_str)\n+                    else:\n+                        remote_model_key_str = '%s.%s' % (\n+                            field.remote_field.model._meta.app_label,\n+                            field.remote_field.model._meta.model_name.lower(),\n+                        )\n+                    # Check if related model matches the model where the field\n+                    # was renamed.\n+                    if remote_model_key_str == target_model_ref_str:\n+                        current_to_field = field.remote_field.field_name\n+                        target_field_matches = (\n+                            current_to_field == self.old_name or\n+                            (current_to_field is None and was_primary_key)\n+                        )\n+                        if target_field_matches:\n+                            cloned_field = field.clone()\n+                            cloned_field.remote_field.field_name = self.new_name\n+                            field_to_add = cloned_field\n+                            fields_changed = True\n+                new_other_fields.append((field_name, field_to_add))\n+            if fields_changed:\n+                other_model_state.fields = new_other_fields\n+\n     def database_forwards(self, app_label, schema_editor, from_state, to_state):\n         to_model = to_state.apps.get_model(app_label, self.model_name)\n         if self.allow_migrate_model(schema_editor.connection.alias, to_model):\n```\n"}
{"instance_id": "django__django-11964", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1,6 +1,10 @@\n+import enum\n+import inspect\n import warnings\n from datetime import date, datetime, time, timezone\n from decimal import Decimal, InvalidOperation\n+\n+from django.db.models.query_utils import DeferredAttribute\n from django.utils.functional import Promise, cached_property\n\n from django.core import checks, exceptions, validators\n@@ -57,6 +61,7 @@\n     empty_strings_allowed = True\n     empty_values = list(validators.EMPTY_VALUES)\n     creation_counter = 0\n+    descriptor_class = DeferredAttribute\n\n     # Generic field type description, usually overridden by subclasses.\n     # Used by admin documentation generator.\n@@ -207,19 +212,37 @@\n         else:\n             cls._meta.add_field(self)\n\n+        # Check if choices is an Enum class derived from enum.Enum.\n+        # This is the check used elsewhere in Django for enum choices.\n+        is_enum_choices = bool(\n+            self.choices and\n+            inspect.isclass(self.choices) and\n+            issubclass(self.choices, enum.Enum)\n+        )\n+\n+        if is_enum_choices:\n+            # Use a custom descriptor that stores the enum's value on __set__\n+            # for fields with enum choices.\n+            class EnumAwareDeferredAttribute(self.descriptor_class):\n+                def __set__(self, instance, value):\n+                    if isinstance(value, self.field.choices):\n+                        instance.__dict__[self.field.attname] = value.value\n+                    else:\n+                        super().__set__(instance, value)\n+            descriptor = EnumAwareDeferredAttribute(self)\n+        else:\n+            # Default descriptor for other fields or fields with tuple choices.\n+            descriptor = self.descriptor_class(self)\n+\n         if self.column:\n             # Ensure the target column is represented by a descriptor.\n             # If the field is presenting a different model field (e.g.\n             # parent_link), include that field in the calculation.\n             related_field = getattr(self.remote_field, 'field', None)\n             if not private_only or not getattr(cls, self.attname, None):\n-                setattr(cls, self.attname, self.descriptor_class(self, related_field))\n+                setattr(cls, self.attname, descriptor)\n         elif not private_only:\n-            # If self.column is None, this is an F() expression defined\n-            # models.py, not contributing an attribute to the real model. Let\n-            # the default manager find this field instead.\n-            # FIXME: Remove this condition when F() expressions defined in\n-            # models.py are removed (deprecated in Django 3.1).\n-            setattr(cls, self.attname, self.descriptor_class(self))\n+            setattr(cls, self.attname, descriptor)\n\n     def set_attributes_from_name(self, name):\n         if not self.name:\n```\n"}
{"instance_id": "django__django-11999", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/admin/utils.py\n+++ b/django/contrib/admin/utils.py\n@@ -108,7 +108,18 @@\n                 accessor = attrgetter(field.remote_field.name)\n             else:\n                 accessor = attrgetter(field.attname)\n-            contents_func = lambda obj: display_func(accessor(obj))\n+            # For choice fields, check for overridden get_..._display() method on the instance.\n+            if field.choices:\n+                display_method_name = 'get_%s_display' % name\n+                empty_value_display = model_admin.get_empty_value_display() if model_admin else _('None')\n+\n+                def contents_func(obj):\n+                    # Check if the override method exists directly on the class (avoids Django's default).\n+                    if hasattr(obj, display_method_name) and display_method_name in obj.__class__.__dict__:\n+                        return getattr(obj, display_method_name)()\n+                    # Fallback to default display logic for choices (using flatchoices).\n+                    return dict(field.flatchoices).get(accessor(obj), empty_value_display)\n+            else:\n+                contents_func = lambda obj: display_func(accessor(obj))\n         else:  # field is None, name is attribute or method on model or admin\n             attr = None\n             if callable(name):\n```\n"}
{"instance_id": "django__django-12113", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/backends/sqlite3/creation.py\n+++ b/django/db/backends/sqlite3/creation.py\n@@ -23,6 +23,11 @@\n         test_database_name = self._get_test_db_name()\n \n         if keepdb:\n+            # If the database file already exists, ensure any existing\n+            # connection is closed before returning the name. This prevents\n+            # potential locking issues when the test runner later tries to\n+            # connect to the persistent database file.\n+            if not self.is_in_memory_db(test_database_name) and os.path.exists(test_database_name):\n+                self.connection.close()\n             return test_database_name\n         if not self.is_in_memory_db(test_database_name):\n             # Erase the old test database\n```\n"}
{"instance_id": "django__django-12125", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/serializer.py\n+++ b/django/db/migrations/serializer.py\n@@ -22,7 +22,7 @@\n         if getattr(self.value, \"__self__\", None) and isinstance(self.value.__self__, type):\n             klass = self.value.__self__\n             module = klass.__module__\n-            return \"%s.%s.%s\" % (module, klass.__name__, self.value.__name__), {\"import %s\" % module}\n+            return \"%s.%s.%s\" % (module, klass.__qualname__, self.value.__name__), {\"import %s\" % module}\n         # Further error checking\n         if self.value.__name__ == '<lambda>':\n             raise ValueError(\"Cannot serialize function: lambda\")\n```\n"}
{"instance_id": "django__django-12184", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/urls/resolvers.py\n+++ b/django/urls/resolvers.py\n@@ -236,10 +236,16 @@\n         # captured values.\n         for k, v in captured_kwargs.items():\n             if v is None:\n-                # If the view parameter has a default value, don't override it\n-                # with None.\n-                if k not in self.func.view_initkwargs and k not in view_args:\n+                # If the view parameter has a default value, don't override it.\n+                has_default = k in self.func.view_initkwargs\n+                if not has_default:\n+                    try:\n+                        inspect.signature(self.func).parameters[k].default\n+                    except (KeyError, ValueError):\n+                        pass  # No parameter or no default.\n+                    else:\n+                        has_default = True\n+                if not has_default:\n                     kwargs[k] = v\n             else:\n                 kwargs[k] = v\n```\n"}
{"instance_id": "django__django-12284", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -27,16 +27,23 @@\n  \n      def _get_flatchoices(self):\n          \"\"\"Flattened version of choices tuple.\"\"\"\n+         # Check if choices is None first for performance.\n          if self.choices is None:\n+             # Don't set self._flatchoices if choices is None. Use getattr\n+             # instead of hasattr because hasattr would cache the negative\n+             # result preventing future assignment. Refs #20819.\n+             if getattr(self, '_flatchoices', None) is None:\n+                 self._flatchoices = []\n+             return self._flatchoices\n+         # If choices is set, force evaluation locally before caching.\n+         choices = list(self.choices)\n+         flat = []\n+         # Check for optgroups\n+         for choice, value in choices:\n+             if isinstance(value, (list, tuple)):\n+                 # This is an optgroup, recurse.\n+                 flat.extend(value)\n+             else:\n+                 flat.append((choice, value))\n+         self._flatchoices = flat  # Cache the result on the field instance\n              return []\n-         flat = []\n-         for choice, value in self.choices:\n-             if isinstance(value, (list, tuple)):\n-                 flat.extend(value)\n-             else:\n-                 flat.append((choice, value))\n-         return flat\n-     flatchoices = property(_get_flatchoices)\n+         return self._flatchoices\n+     flatchoices = property(_get_flatchoices)\n  \n      def save_form_data(self, instance, data):\n          setattr(instance, self.name, data)\n\n```\n"}
{"instance_id": "django__django-12286", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/checks/translation.py\n+++ b/django/core/checks/translation.py\n@@ -1,5 +1,6 @@\n from django.conf import settings\n from django.utils.translation.trans_real import language_code_re\n+from django.utils.translation import get_supported_language_variant\n \n from . import Error, Tags, register\n \n@@ -50,3 +51,17 @@\n         Error(E003.msg.format(tag), id=E003.id)\n         for tag in settings.LANGUAGES_BIDI if not isinstance(tag, str) or not language_code_re.match(tag)\n     ]\n+\n+\n+@register(Tags.translation, deploy=True)\n+def check_language_code_in_languages(app_configs, **kwargs):\n+    \"\"\"\n+    Check that LANGUAGE_CODE is compatible with LANGUAGES.\n+    Allow fallback to base language.\n+    \"\"\"\n+    try:\n+        # get_supported_language_variant will raise LookupError if\n+        # LANGUAGE_CODE is not in LANGUAGES and its base language isn't either.\n+        get_supported_language_variant(settings.LANGUAGE_CODE)\n+    except LookupError:\n+        return [Error(E004.msg, id=E004.id)]\n+    return []\n```\n"}
{"instance_id": "django__django-12308", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/admin/utils.py\n+++ b/django/contrib/admin/utils.py\n@@ -1,6 +1,8 @@\n import datetime\n import decimal\n+import json\n \n+from django import forms\n from django.conf import settings\n from django.contrib.admin.views.main import IS_POPUP_VAR, TO_FIELD_VAR\n from django.core.exceptions import FieldDoesNotExist, ObjectDoesNotExist\n@@ -10,7 +12,7 @@\n from django.urls import NoReverseMatch, reverse\n from django.utils import formats, timezone\n from django.utils.html import format_html, format_html_join, conditional_escape\n-from django.utils.safestring import mark_safe\n+from django.utils.safestring import mark_safe, SafeData\n from django.utils.text import capfirst\n from django.utils.translation import gettext as _, ngettext\n \n@@ -106,6 +108,25 @@\n         return formats.number_format(value)\n     elif isinstance(field, models.URLField) and value:\n         return format_html('<a href=\"{0}\">{0}</a>', value)\n+    elif isinstance(field, models.JSONField) and value is not None:\n+        try:\n+            # Use the form field's prepare_value for proper JSON serialization\n+            # and configured encoder.\n+            form_field = field.formfield()\n+            json_str = form_field.prepare_value(value)\n+            # prepare_value returns a string, potentially InvalidJSONInput.\n+            # If it's valid JSON string, parse and dump with indent=2.\n+            # If it's InvalidJSONInput, display as is.\n+            if isinstance(json_str, forms.InvalidJSONInput):\n+                display_value = json_str\n+            else:\n+                parsed_value = json.loads(json_str)\n+                display_value = json.dumps(parsed_value, indent=2, cls=field.encoder)\n+            # Wrap in <pre> for formatting.\n+            return format_html('<pre>{}</pre>', conditional_escape(display_value))\n+        except (json.JSONDecodeError, TypeError):\n+            # Value cannot be serialized. Display the raw string representation.\n+            return conditional_escape(str(value))\n     elif isinstance(field, ManyToManyField) and isinstance(value, (list, tuple)):\n         return ', '.join(str(v) for v in value)\n     else:\n\n```\n"}
{"instance_id": "django__django-12453", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/backends/base/creation.py\n+++ b/django/db/backends/base/creation.py\n@@ -4,7 +4,7 @@ import time\n from django.apps import apps\n from django.conf import settings\n from django.core import serializers\n-from django.db import router\n+from django.db import router, transaction\n from django.utils.six import StringIO\n from django.utils.six.moves import input\n \n@@ -128,8 +128,9 @@ class BaseDatabaseCreation(object):\n \t\t the serialize_db_to_string method.\n \t\t \"\"\"\n \t\t data = StringIO(data)\n-\t\tfor obj in serializers.deserialize(\"json\", data, using=self.connection.alias):\n-\t\t\tobj.save()\n+\t\t# Use transaction.atomic to defer constraint checks until the end.\n+\t\twith transaction.atomic(using=self.connection.alias):\n+\t\t\tfor obj in serializers.deserialize(\"json\", data, using=self.connection.alias):\n+\t\t\t\tobj.save()\n \n \t def _get_database_display_str(self, verbosity, database_name):\n \t\t \"\"\"\n```\n"}
{"instance_id": "django__django-12470", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/options.py\n+++ b/django/db/models/options.py\n@@ -36,6 +36,27 @@\n         else:\n             self.order_with_respect_to = None\n \n+        # If the ordering refers to the primary key and the primary key is\n+        # inherited from a parent model, update the ordering to refer explicitly\n+        # to the parent primary key field. This prevents the ordering logic\n+        # from resolving the child's PK name (e.g., parent_ptr) incorrectly\n+        # in the context of the Meta.ordering defined on the parent.\n+        if self.ordering and self.pk and self.parents and self.pk in self.parents.values():\n+            # The PK field is a pointer to a parent model.\n+            # Get the name of the actual primary key field on the parent model.\n+            parent_pk_name = self.pk.remote_field.get_related_field().name\n+            new_ordering = []\n+            for item in self.ordering:\n+                if isinstance(item, str):\n+                    prefix = '-' if item.startswith('-') else ''\n+                    field_name = item[len(prefix):]\n+                    if field_name == 'pk':\n+                        # Replace 'pk' with the actual parent PK field name.\n+                        new_ordering.append(prefix + parent_pk_name)\n+                        continue\n+                new_ordering.append(item)\n+            self.ordering = new_ordering\n+\n         if self.pk is None:\n             if self.parents:\n                 # Promote the first parent link in lieu of adding yet another\n```\n"}
{"instance_id": "django__django-12497", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/related.py\n+++ b/django/db/models/fields/related.py\n@@ -1111,11 +1111,11 @@\n                              \"to '%s', which is ambiguous. You must specify \"\n                              \"which foreign key Django should use via the \"\n                              \"through_fields keyword argument.\" % (self, to_model_name),\n-                            hint=(\n-                                'If you want to create a recursive relationship, '\n-                                'use ForeignKey(\"%s\", symmetrical=False, through=\"%s\").'\n-                            ) % (\n-                                RECURSIVE_RELATIONSHIP_CONSTANT,\n+                            hint=(\n+                                'If this is a recursive relationship, use '\n+                                'ManyToManyField(\"%s\", through=\"%s\"). Use the '\n+                                '`through_fields` argument to specify the '\n+                                'two foreign keys connecting to the model.'\n+                            ) % (\n+                                RECURSIVE_RELATIONSHIP_CONSTANT,\n                                 relationship_model_name,\n                             ),\n                             obj=self,\n@@ -1157,11 +1157,11 @@\n                               \"from '%s', which is ambiguous. You must specify \"\n                               \"which foreign key Django should use via the \"\n                               \"through_fields keyword argument.\") % (self, from_model_name),\n-                            hint=(\n-                                'If you want to create a recursive relationship, '\n-                                'use ForeignKey(\"%s\", symmetrical=False, through=\"%s\").'\n-                            ) % (\n-                                RECURSIVE_RELATIONSHIP_CONSTANT,\n+                            hint=(\n+                                'If this is a recursive relationship, use '\n+                                'ManyToManyField(\"%s\", through=\"%s\"). Use the '\n+                                '`through_fields` argument to specify the '\n+                                'two foreign keys connecting to the model.'\n+                            ) % (\n+                                RECURSIVE_RELATIONSHIP_CONSTANT,\n                                 relationship_model_name,\n                             ),\n                             obj=self,\n```\n"}
{"instance_id": "django__django-12589", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "django__django-12700", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/views/debug.py\n+++ b/django/views/debug.py\n@@ -42,22 +42,31 @@\n         \"\"\"\n         Cleanse an individual setting key/value of sensitive content. If the\n         value is a dictionary, recursively cleanse the keys in that dictionary.\n+        If the value is a list or tuple, recursively cleanse the items in it.\n         \"\"\"\n         try:\n             if self.hidden_settings.search(key):\n                 cleansed = self.cleansed_substitute\n-            elif isinstance(value, dict):\n-                cleansed = {k: self.cleanse_setting(k, v) for k, v in value.items()}\n             else:\n-                cleansed = value\n+                # Key is not sensitive, cleanse the value's structure recursively.\n+                cleansed = self._cleanse_value_recursive(value)\n         except TypeError:\n             # If the key isn't regex-able, just return as-is.\n             cleansed = value\n \n         if callable(cleansed):\n             cleansed = CallableSettingWrapper(cleansed)\n+\n         return cleansed\n+\n+    def _cleanse_value_recursive(self, value):\n+        \"\"\"Helper to recursively cleanse nested structures.\"\"\"\n+        if isinstance(value, dict):\n+            # For dicts, call cleanse_setting on each item to check keys.\n+            return {k: self.cleanse_setting(k, v) for k, v in value.items()}\n+        elif isinstance(value, (list, tuple)):\n+            # For lists/tuples, recursively cleanse each item's value structure.\n+            return type(value)(self._cleanse_value_recursive(item) for item in value)\n+        else:\n+            # Base case: return non-container values as is.\n+            return value\n \n     def get_safe_settings(self):\n         \"\"\"\n```\n"}
{"instance_id": "django__django-12708", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/backends/base/schema.py\n+++ b/django/db/backends/base/schema.py\n@@ -41,18 +41,29 @@\n             self.execute(self._create_index_sql(model, fields, suffix=\"_idx\"))\n \n     def _delete_composed_index(self, model, fields, constraint_kwargs, sql):\n+        \"\"\"\n+        Delete an index/unique constraint for a model defined using\n+        index_together/unique_together.\n+        \"\"\"\n         meta_constraint_names = {constraint.name for constraint in model._meta.constraints}\n         meta_index_names = {constraint.name for constraint in model._meta.indexes}\n         columns = [model._meta.get_field(field).column for field in fields]\n         constraint_names = self._constraint_names(\n             model, columns, exclude=meta_constraint_names | meta_index_names,\n             **constraint_kwargs\n         )\n+        # If removing an index and multiple constraints are found,\n+        # try to identify the non-unique one. This handles cases where\n+        # unique_together and index_together are defined for the same fields.\n+        if constraint_kwargs == {'index': True} and len(constraint_names) > 1:\n+            unique_constraint_names = self._constraint_names(\n+                model, columns, exclude=meta_constraint_names | meta_index_names,\n+                unique=True\n+            )\n+            non_unique_constraint_names = list(set(constraint_names) - set(unique_constraint_names))\n+            if len(non_unique_constraint_names) == 1:\n+                constraint_names = non_unique_constraint_names\n         if len(constraint_names) != 1:\n-            raise ValueError(\"Found wrong number (%s) of constraints for %s(%s)\" % (\n-                len(constraint_names),\n-                model._meta.db_table,\n-                \", \".join(columns),\n-            ))\n+            # Maybe the constraint doesn't exist? Raise ValueError.\n+            raise ValueError(\"Found wrong number (%s) of constraints for %s(%s)\" % (\n+                len(constraint_names), model._meta.db_table, \", \".join(columns)))\n         self.execute(self._delete_constraint_sql(sql, model, constraint_names[0]))\n \n     def alter_db_table(self, model, old_db_table, new_db_table):\n```\n"}
{"instance_id": "django__django-12747", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -317,7 +317,9 @@ class Collector:\n         # Delete fast-deletable objects.\n         for qs in self.fast_deletes:\n             count = qs._raw_delete(using=self.using)\n-            deleted_counter[qs.model._meta.label] += count\n+            # Only add to counter if objects were actually deleted to avoid\n+            # inconsistent {model: 0} results compared to non-fast-delete path.\n+            if count > 0:\n+                deleted_counter[qs.model._meta.label] += count\n \n         # Update fields for nullified references.\n         for field, updates in self.field_updates.items():\n```\n"}
{"instance_id": "django__django-12856", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/constraints.py\n+++ b/django/db/models/constraints.py\n@@ -1,5 +1,7 @@\n from django.db.models.expressions import F\n from django.db.models.query_utils import Q\n+from django.core import checks\n+from django.db.models.fields import FieldDoesNotExist\n \n __all__ = ['CheckConstraint', 'UniqueConstraint']\n \n@@ -100,6 +102,22 @@\n         )\n         return sql\n \n+    def check(self, model, **kwargs):\n+        errors = []\n+        fields = set(self.fields)\n+        opts = model._meta\n+        # Check that all fields are valid.\n+        for field_name in self.fields:\n+            try:\n+                opts.get_field(field_name)\n+            except FieldDoesNotExist:\n+                errors.append(checks.Error(\n+                    \"UniqueConstraint 'fields' refers to the nonexistent field '%s'.\" % field_name,\n+                    obj=self,\n+                    id='models.E019',\n+                ))\n+        return errors\n+\n     def validate(self, model, instance, exclude=None, using=None):\n         from django.core.exceptions import ValidationError, NON_FIELD_ERRORS\n         if exclude is None: exclude = []\n--- a/django/db/models/options.py\n+++ b/django/db/models/options.py\n@@ -702,6 +702,8 @@\n             if not isinstance(constraint, BaseConstraint):\n                 errors.append(checks.Error(\n                     \"'constraints' must be a sequence of CheckConstraint or UniqueConstraint instances.\",\n+                    # Removed obj=constraint because it might not be a\n+                    # constraint instance.\n                     obj=self.model,\n                     id='models.E018',\n                 ))\n@@ -709,6 +711,8 @@\n                     errors.extend(constraint.check(model=self.model))\n         return errors\n \n+    # TODO: Remove in Django 4.1.\n+    @property\n     def _forward_fields_map(self):\n         res = {}\n         fields = self._get_fields(forward=True, reverse=False, include_parents=True, include_hidden=False)\n@@ -717,6 +721,8 @@\n             res[field.name] = field\n         return res\n \n+    # TODO: Remove in Django 4.1.\n+    @property\n     def fields_map(self):\n         \"\"\"\n         Return a dictionary mapping field name to field instance.\n```\n"}
{"instance_id": "django__django-12908", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "django__django-12915", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/staticfiles/handlers.py\n+++ b/django/contrib/staticfiles/handlers.py\n@@ -1,10 +1,12 @@\n import mimetypes\n import os\n import posixpath\n+import sys\n from email.utils import parsedate_to_datetime\n from urllib.parse import unquote, urlparse\n from urllib.request import url2pathname\n \n+from asgiref.sync import sync_to_async\n from django.conf import settings\n from django.contrib.staticfiles import finders\n from django.contrib.staticfiles.views import serve\n@@ -12,6 +14,7 @@\n from django.core.handlers.asgi import ASGIHandler\n from django.core.handlers.base import BaseHandler\n from django.core.handlers.wsgi import WSGIHandler\n+from django.core.handlers.exception import response_for_exception\n from django.http import Http404\n from django.utils.encoding import uri_to_iri\n from django.utils.http import http_date\n@@ -65,6 +68,13 @@\n         except Http404 as e:\n             return response_for_exception(request, e)\n \n+    async def get_response_async(self, request):\n+        try:\n+            # Pass static file requests to sync serve() view.\n+            return await sync_to_async(self.serve, thread_sensitive=True)(request)\n+        except Http404 as e:\n+            return await sync_to_async(response_for_exception, thread_sensitive=True)(request, e)\n+\n \n class StaticFilesHandler(StaticFilesHandlerMixin, WSGIHandler):\n     \"\"\"\n```\n"}
{"instance_id": "django__django-12983", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/utils/text.py\n+++ b/django/utils/text.py\n@@ -34,19 +34,19 @@\n @keep_lazy_text\n def slugify(value, allow_unicode=False):\n     \"\"\"\n-    Convert to ASCII if 'allow_unicode' is False. Convert spaces to hyphens.\n-    Remove characters that aren't alphanumerics, underscores, or hyphens.\n-    Convert to lowercase. Also strip leading and trailing whitespace.\n+    Convert to ASCII if 'allow_unicode' is False. Convert spaces, underscores,\n+    and consecutive hyphens to single hyphens. Remove characters that aren't\n+    alphanumerics or hyphens. Convert to lowercase. Also strip leading and\n+    trailing hyphens.\n     \"\"\"\n     value = str(value)\n     if allow_unicode:\n         value = unicodedata.normalize('NFKC', value)\n     else:\n         value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n-    value = re.sub(r'[^\\w\\s-]', '', value.lower()).strip()\n-    return re.sub(r'[-\\s]+', '-', value)\n-\n+    value = value.lower()\n+    value = re.sub(r'[^\\w\\s-]', '', value).strip()\n+    value = re.sub(r'[-\\s_]+', '-', value)\n+    return value.strip('-')\n\n\n def camel_case_to_spaces(value):\n```\n"}
{"instance_id": "django__django-13028", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1261,7 +1261,9 @@\n         ) = self.names_to_path(parts, self.model._meta, allow_many=True, fail_on_missing=True)\n         field = path[-1].field\n         value = self.resolve_lookup_value(value, can_reuse, allow_joins)\n-        self.check_filterable(value)\n+        # Skip filterable check for non-expression values to avoid issues with\n+        # model instances having a field named 'filterable'.\n+        if isinstance(value, models.Expression):\n+            self.check_filterable(value)\n         self.check_query_object_type(value, opts, field)\n\n         if hasattr(value, 'resolve_expression'):\n```\n"}
{"instance_id": "django__django-13033", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -16,6 +16,7 @@\n from django.db.models.query_utils import FilteredRelation\n from django.db.models.sql.constants import (\n     CURSOR, GET_ITERATOR_CHUNK_SIZE, INNER, LOUTER, NO_RESULTS, ORDER_DIR,\n+    QUERY_TERMS, RECURSIVE_CTE_ALIAS, SINGLE,\n )\n from django.db.models.sql.datastructures import BaseTable, Empty, Join\n from django.db.models.sql.expressions import (\n@@ -1441,16 +1442,32 @@\n             else:\n                 # Model field ordering.\n                 try:\n-                    _, field_list, _, _ = self._find_ordering_parts(field.split(LOOKUP_SEP))\n-                    for target, final_descending in self.get_order_by(field_list, descending):\n-                        self.order_by.append(OrderBy(target, descending=final_descending))\n+                    lookup_parts = field.split(LOOKUP_SEP)\n+                    # Resolve path, potentially adding joins via names_to_path/setup_joins.\n+                    path, final_field_info, targets, alias = self._find_ordering_parts(lookup_parts)\n+                    # final_field_info = (resolved_field, targets, containing_alias, joins)\n+                    resolved_field = final_field_info[0]\n+                    containing_alias = final_field_info[2]\n+\n+                    # Check if ordering by the attname of a relation field.\n+                    is_ordering_by_attname = (\n+                        resolved_field.is_relation and\n+                        lookup_parts[-1] == resolved_field.attname\n+                    )\n+\n+                    if is_ordering_by_attname:\n+                        # Order directly by the FK column `resolved_field.column`\n+                        # on the table represented by `containing_alias`.\n+                        # This bypasses get_order_by() and avoids applying the\n+                        # related model's default ordering or adding an\n+                        # unnecessary join for ordering.\n+                        expression = Col(containing_alias, resolved_field.column, resolved_field)\n+                        self.order_by.append(OrderBy(expression, descending=descending))\n+                    else:\n+                        # Default behavior: use get_order_by().\n+                        field_list = path + [final_field_info]  # Reconstruct field_list.\n+                        for target, final_descending in self.get_order_by(field_list, descending):\n+                            self.order_by.append(OrderBy(target, descending=final_descending))\n                 except FieldError as e:\n                     if field in self.annotation_select:\n                         # It's an aggregate field. We need to make sure an aggregate\n```\n"}
{"instance_id": "django__django-13158", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -344,7 +344,10 @@\n         obj.explain_info = self.explain_info\n         obj.external_aliases = self.external_aliases.copy()\n         if self.combined_queries:\n-            obj.combined_queries = tuple(q.clone() for q in self.combined_queries)\n+            # When cloning combined query, we must ensure that query\n+            # underlying the clone is configured correctly.\n+            obj.combined_queries = tuple(q.clone(klass=Query) for q in self.combined_queries)\n+            obj.select = ()\n         return obj\n\n     def __getstate__(self):\n```\n"}
{"instance_id": "django__django-13220", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/exceptions.py\n+++ b/django/core/exceptions.py\n@@ -1,5 +1,7 @@\n from django.utils.translation import gettext_lazy as _\n \n+from collections import Counter\n+\n NON_FIELD_ERRORS = '__all__'\n \n \n@@ -49,3 +51,41 @@\n                 yield field, list(ValidationError(errors))\n         else:\n             # Yield individual messages for list-based errors\n-            # or single-message errors.\n+            for error in self.error_list:\n+                # Could be a plain string or ValidationError instance.\n+                if isinstance(error, ValidationError):\n+                    message = error.message\n+                else:\n+                    message = error\n+                yield message\n+\n+    def __eq__(self, other):\n+        if not isinstance(other, ValidationError):\n+            return NotImplemented\n+\n+        self_is_dict = hasattr(self, 'error_dict')\n+        other_is_dict = hasattr(other, 'error_dict')\n+\n+        if self_is_dict != other_is_dict:\n+            return False\n+\n+        if self_is_dict:\n+            if set(self.error_dict.keys()) != set(other.error_dict.keys()):\n+                return False\n+            for key in self.error_dict:\n+                # Compare lists of errors for each key order-independently\n+                try:\n+                    if Counter(self.error_dict[key]) != Counter(other.error_dict[key]):\n+                        return False\n+                except TypeError:  # Fallback if errors are not hashable\n+                    return sorted(self.error_dict[key], key=repr) == sorted(other.error_dict[key], key=repr)\n+            return True\n+        else:\n+            # Compare list-based errors order-independently\n+            try:\n+                return Counter(self.error_list) == Counter(other.error_list)\n+            except TypeError:  # Fallback if errors are not hashable\n+                return sorted(self.error_list, key=repr) == sorted(other.error_list, key=repr)\n+\n+    def __hash__(self):\n+        # Only hash instances representing a single error message.\n+        if hasattr(self, 'error_list') and len(self.error_list) == 1 and self.error_list[0] is self:\n+            # Ensure message is resolved if it's lazy.\n+            # Convert params dict to a hashable form (frozenset of items).\n+            params = frozenset(self.params.items()) if self.params else None\n+            return hash((str(self.message), self.code, params))\n+        else:\n+            # Container ValidationErrors (list or dict based) are mutable and not hashable.\n+            raise TypeError(\"ValidationError instances containing other errors cannot be hashed\")\n```\n"}
{"instance_id": "django__django-13230", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/syndication/views.py\n+++ b/django/contrib/syndication/views.py\n@@ -168,6 +168,7 @@\n                 author_link=author_link,\n                 categories=self._get_dynamic_attr('item_categories', item),\n                 item_copyright=self._get_dynamic_attr('item_copyright', item),\n+                comments=self._get_dynamic_attr('item_comments', item),\n                 **self.item_extra_kwargs(item)\n             )\n         return feed\n```\n"}
{"instance_id": "django__django-13265", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/autodetector.py\n+++ b/django/db/migrations/autodetector.py\n@@ -150,8 +150,8 @@\n         self._detect_unique_together_changes()\n         self._detect_index_together_changes()\n         self._detect_constraints_changes()\n+        self._detect_alter_order_with_respect_to_changes()\n         self._detect_index_changes()\n-        self._detect_alter_order_with_respect_to_changes()\n         self._detect_alter_db_table_changes()\n         self._detect_alter_db_table_comment_changes()\n         self._detect_alter_managed_changes()\n```\n"}
{"instance_id": "django__django-13315", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -440,6 +440,22 @@\n         if hasattr(self, '_choices'):\n             return self._choices\n \n+        # Prepare the queryset to be used for choices.\n+        queryset = self.queryset\n+        limit_choices_to = self.limit_choices_to\n+        needs_distinct = False\n+\n+        # Apply limit_choices_to if present and determine if distinct() is needed.\n+        if limit_choices_to is not None:\n+            if callable(limit_choices_to):\n+                limit_choices_to = limit_choices_to()\n+            queryset = queryset.complex_filter(limit_choices_to)\n+            # If limit_choices_to is a Q object, joins might add duplicates.\n+            if isinstance(limit_choices_to, models.Q):\n+                needs_distinct = True\n+        if needs_distinct:\n+            queryset = queryset.distinct()\n+\n         # Appending choices for the selected value guarantees that the\n         # selected value appears in the choices, even if it's not in the\n         # queryset.\n@@ -450,7 +466,7 @@\n         else:\n             selected_choices = set()\n \n-        choices = list(self.iterator)\n+        choices = list(self.iterator(queryset=queryset)) # Pass the prepared queryset\n         for i, (value, label) in enumerate(choices):\n             if value == '':\n                 # Keep the empty label option if it exists.\n@@ -470,7 +486,7 @@\n \n     choices = property(_get_choices, ChoiceField._set_choices)\n \n-    def iterator(self, field_name=None):\n+    def iterator(self, field_name=None, queryset=None):\n         \"\"\"\n         Yield tuples representing the choices for the field.\n \n@@ -478,7 +494,8 @@\n         queryset to allow customization of the choices in subclasses.\n         \"\"\"\n         if field_name is None:\n-            queryset = self.queryset\n+            if queryset is None:\n+                queryset = self.queryset\n         else:\n             queryset = getattr(self.queryset.model, field_name).get_queryset()\n         if self.empty_label is not None:\n```\n"}
{"instance_id": "django__django-13321", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/sessions/backends/base.py\n+++ b/django/contrib/sessions/backends/base.py\n@@ -114,10 +114,10 @@\n \n     def _legacy_decode(self, session_data):\n         # RemovedInDjango40Warning: pre-Django 3.1 format will be invalid.\n-        encoded_data = base64.b64decode(session_data.encode('ascii'))\n         try:\n+            encoded_data = base64.b64decode(session_data.encode('ascii'))\n             # could produce ValueError if there is no ':'\n-            hash, serialized = encoded_data.split(b':', 1)\n+            hash_prefix, serialized = encoded_data.split(b':', 1)\n             expected_hash = self._hash(serialized)\n             if not constant_time_compare(hash.decode(), expected_hash):\n                 raise SuspiciousSession(\"Session data corrupted\")\n@@ -125,8 +125,8 @@\n                 return self.serializer().loads(serialized)\n         except Exception as e:\n             # ValueError, SuspiciousOperation, unpickling exceptions. If any of\n-            # these happen, just return an empty dictionary (an empty session).\n-            if isinstance(e, SuspiciousOperation):\n+            # these or base64 errors happen, just return an empty dictionary\n+            # (an empty session).\n+            if isinstance(e, (SuspiciousOperation, binascii.Error)):\n                 logger = logging.getLogger('django.security.%s' % e.__class__.__name__)\n                 logger.warning(str(e))\n             return {}\n\n```\n"}
{"instance_id": "django__django-13401", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -186,20 +186,34 @@\n     def __eq__(self, other):\n         # Needed for @total_ordering\n         if isinstance(other, Field):\n-            return self.creation_counter == other.creation_counter\n+            # Fields are equal if they share the same creation counter and\n+            # belong to the same model.\n+            return (self.creation_counter == other.creation_counter and\n+                    self.model == other.model)\n         return NotImplemented\n \n     def __lt__(self, other):\n         # This is needed because bisect does not take a comparison function.\n         if isinstance(other, Field):\n+            # Sort by creation counter first, then by model.\n+            if self.creation_counter == other.creation_counter:\n+                # In case of duplicate creation counters (abstract models),\n+                # sort by model meta. This is necessary to ensure that\n+                # comparison is stable across different models (e.g. abstract\n+                # models).\n+                if self.model == other.model:\n+                    # If models are equal, the fields are equal.\n+                    return False\n+                # Use label_lower for stable ordering across different runs.\n+                return self.model._meta.label_lower < other.model._meta.label_lower\n             return self.creation_counter < other.creation_counter\n         return NotImplemented\n \n     def __hash__(self):\n-        return hash(self.creation_counter)\n+        return hash((self.creation_counter, self.model))\n \n     def __deepcopy__(self, memodict):\n-        # We don't have to deepcopy very much here, since most things are not\n+        # We don't have to deepcopy very much here since most things are not\n         # intended to be altered after initial creation.\n         obj = copy.copy(self)\n         if self.remote_field:\n```\n"}
{"instance_id": "django__django-13447", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/admin/sites.py\n+++ b/django/contrib/admin/sites.py\n@@ -198,7 +198,7 @@\n                 current_app=self.name,\n             )\n\n-    def _build_app_dict(self, request, label=None):\n+    def build_app_dict(self, request, label=None):\n         \"\"\"\n         Build the app dictionary. The optional `label` parameter filters models\n         of a specific app.\n@@ -227,6 +227,7 @@\n             model_dict = {\n                 'name': capfirst(model._meta.verbose_name_plural),\n                 'object_name': model._meta.object_name,\n+                'model': model,\n                 'perms': perms,\n                 'admin_url': None,\n                 'add_url': None,\n@@ -266,7 +267,7 @@\n         Return a sorted list of all the installed apps that have been\n         registered in this site.\n         \"\"\"\n-        app_dict = self._build_app_dict(request)\n+        app_dict = self.build_app_dict(request)\n\n         # Sort the apps alphabetically.\n         app_list = sorted(app_dict.values(), key=lambda x: x['name'].lower())\n@@ -301,7 +302,7 @@\n         return TemplateResponse(request, self.index_template or 'admin/index.html', context)\n\n     def app_index(self, request, app_label, extra_context=None):\n-        app_dict = self._build_app_dict(request, app_label)\n+        app_dict = self.build_app_dict(request, app_label)\n         if not app_dict:\n             raise Http404('The requested admin page does not exist.')\n         # Sort the models alphabetically within each app.\n```\n"}
{"instance_id": "django__django-13448", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/backends/base/creation.py\n+++ b/django/db/backends/base/creation.py\n@@ -19,21 +19,29 @@\n         amounts of data.\n         \"\"\"\n         # Iteratively return every object for all models to serialize.\n+        from django.core import serializers\n+        from django.db import router\n+        from django.db.migrations.loader import MigrationLoader\n+\n         def get_objects():\n-            from django.db.migrations.loader import MigrationLoader\n             loader = MigrationLoader(self.connection, ignore_no_migrations=True)\n+            # Determine the MIGRATE setting for the current database connection.\n+            migrate_setting = self.connection.settings_dict['TEST'].get('MIGRATE', True)\n+\n             for app_config in apps.get_app_configs():\n                 if (\n                     app_config.models_module is not None and\n-                    app_config.label in loader.migrated_apps and\n                     app_config.name not in settings.TEST_NON_SERIALIZED_APPS\n                 ):\n+                    has_migrations = app_config.label in loader.graph.nodes\n+                    # When MIGRATE=False, only apps without migrations are fully\n+                    # created via run_syncdb=True. Apps with migrations have no\n+                    # tables created. Skip serializing them.\n+                    if not migrate_setting and has_migrations:\n+                        continue\n+\n                     for model in app_config.get_models():\n                         if (\n                             model._meta.can_migrate(self.connection) and\n                             router.allow_migrate_model(self.connection.alias, model)\n                         ):\n-                            queryset = model._default_manager.using(\n-                                self.connection.alias,\n-                            ).order_by(model._meta.pk.name)\n-                            yield from queryset.iterator()\n+                            queryset = model._default_manager.using(self.connection.alias).order_by(model._meta.pk.name)\n+                            yield from queryset.iterator()\n+\n         # Serialize to a string\n         out = StringIO()\n         serializers.serialize(\"json\", get_objects(), indent=None, stream=out)\n```\n"}
{"instance_id": "django__django-13551", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/auth/tokens.py\n+++ b/django/contrib/auth/tokens.py\n@@ -39,23 +39,34 @@\n \n     def _make_hash_value(self, user, timestamp):\n         \"\"\"\n-        Hash the user's primary key and some user state that's sure to change\n-        after a password reset to produce a token that invalidated when it's\n+        Hash the user's primary key and some user state that is sure to change\n+        after a password reset or email change to produce a token that is\n         used:\n-        1. The password field will change upon a password reset (even if the\n-           same password is chosen, due to password salting).\n-        2. The last_login field will usually be updated very shortly after\n-           a password reset.\n+\n+        1. User's primary key\n+        2. User's password hash\n+        3. User's last login timestamp\n+        4. User's email address\n+        5. Current timestamp\n+\n+        This ensures that the token becomes invalid if the user changes their\n+        password, email address, or logs in.\n+\n         Failing those things, settings.PASSWORD_RESET_TIMEOUT eventually\n         invalidates the token.\n-\n-        Running this data through salted_hmac() prevents password cracking\n-        attempts using the reset token, provided the secret isn't compromised.\n         \"\"\"\n         # Truncate microseconds so that tokens are consistent even if the\n         # database doesn't support microseconds.\n         login_timestamp = '' if user.last_login is None else user.last_login.replace(microsecond=0, tzinfo=None)\n-        return str(user.pk) + user.password + str(login_timestamp) + str(timestamp)\n+\n+        # Get the email field name for the user model.\n+        try:\n+            email_field_name = user.get_email_field_name()\n+        except AttributeError:\n+            # User model doesn't define get_email_field_name().\n+            email = ''\n+        else:\n+            email = getattr(user, email_field_name, '') or ''\n+            email = str(email)\n+\n+        return str(user.pk) + user.password + str(login_timestamp) + str(timestamp) + email\n \n     def _num_seconds(self, dt):\n         return int((dt - datetime(2001, 1, 1)).total_seconds())\n\n```\n"}
{"instance_id": "django__django-13590", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1367,7 +1367,7 @@\n         if isinstance(value, (list, tuple)):\n             # The items of the list/tuple may be expressions that need resolving.\n             return type(value)(\n-                self.resolve_lookup_value(sub_value, can_reuse, allow_joins)\n+                *(self.resolve_lookup_value(sub_value, can_reuse, allow_joins)\n                 for sub_value in value)\n             )\n         if isinstance(value, QuerySet):\n```\n"}
{"instance_id": "django__django-13658", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/management/__init__.py\n+++ b/django/core/management/__init__.py\n@@ -166,7 +166,12 @@\n         # Preprocess options to extract --settings and --pythonpath.\n         # These options could affect the commands that are available, so they\n         # must be processed early.\n-        parser = CommandParser(usage='%(prog)s subcommand [options] [args]', add_help=False, allow_abbrev=False)\n+        parser = CommandParser(\n+            prog=self.prog_name,\n+            usage='%(prog)s subcommand [options] [args]',\n+            add_help=False,\n+            allow_abbrev=False,\n+        )\n         parser.add_argument('--settings')\n         parser.add_argument('--pythonpath')\n         parser.add_argument('args', nargs='*')  # catch-all\n\n```\n"}
{"instance_id": "django__django-13660", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/management/commands/shell.py\n+++ b/django/core/management/commands/shell.py\n@@ -43,14 +43,18 @@\n     def handle(self, **options):\n         # Execute the command and exit.\n         if options['command']:\n-            exec(options['command'])\n+            # Pass empty dictionaries for globals and locals to mimic\n+            # behavior of running a Python script.\n+            exec(options['command'], {})\n             return\n \n         # Execute stdin if it has anything to read and exit.\n         # Not supported on Windows due to select.select() limitations.\n         if sys.platform != 'win32' and not sys.stdin.isatty() and select.select([sys.stdin], [], [], 0)[0]:\n-            exec(sys.stdin.read())\n+            # Pass empty dictionaries for globals and locals to mimic\n+            # behavior of running a Python script.\n+            exec(sys.stdin.read(), {})\n             return\n \n         available_shells = [options['interface']] if options['interface'] else self.shells\n```\n"}
{"instance_id": "django__django-13710", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -1,5 +1,6 @@\n from django.conf import settings\n from django.contrib.admin import checks, widgets\n+from django.contrib.admin.exceptions import DisallowedModelAdminToField\n from django.contrib.admin.utils import (\n     NestedObjects, NotRelationField, flatten_fieldsets, get_deleted_objects,\n     lookup_needs_distinct, lookup_spawns_duplicates, models, quote,\n@@ -10,6 +11,7 @@\n from django.forms.models import BaseInlineFormSet, modelform_defines_fields\n from django.template.response import SimpleTemplateResponse, TemplateResponse\n from django.urls import NoReverseMatch, reverse\n+from django.utils.text import format_lazy\n from django.utils.translation import gettext as _, ngettext\n \n from .actions import delete_selected\n@@ -333,13 +335,19 @@\n         self.parent_model = parent_model\n         self.opts = self.model._meta\n         self.has_registered_model = admin_site.is_registered(self.model)\n+        # Store the originally set verbose_name before BaseModelAdmin.__init__\n+        # changes them (although it doesn't seem to).\n+        original_verbose_name = self.verbose_name\n         super().__init__()\n         if self.verbose_name is None:\n             self.verbose_name = self.model._meta.verbose_name\n         if self.verbose_name_plural is None:\n-            self.verbose_name_plural = self.model._meta.verbose_name_plural\n-\n+            # If verbose_name was manually set on this inline, pluralize that.\n+            # Otherwise, default to the model's verbose_name_plural.\n+            if original_verbose_name is not None:\n+                self.verbose_name_plural = format_lazy('{}s', self.verbose_name)\n+            else:\n+                self.verbose_name_plural = self.model._meta.verbose_name_plural\n \n     @property\n     def media(self):\n```\n"}
{"instance_id": "django__django-13757", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/json.py\n+++ b/django/db/models/fields/json.py\n@@ -7,14 +7,18 @@\n class KeyTransformIsNull(lookups.IsNull):\n     # key__isnull=False is the same as has_key='key'\n     def as_oracle(self, compiler, connection):\n+        has_key_lookup = HasKey(self.lhs.lhs, self.lhs.key_name)\n         if not self.rhs:\n-            return HasKey(self.lhs.lhs, self.lhs.key_name).as_oracle(compiler, connection)\n-        return super().as_sql(compiler, connection)\n+            # key__isnull=False: Check if key exists.\n+            return has_key_lookup.as_oracle(compiler, connection)\n+        # key__isnull=True: Check if key does NOT exist.\n+        sql, params = has_key_lookup.as_oracle(compiler, connection)\n+        return 'NOT (%s)' % sql, params\n \n     def as_sqlite(self, compiler, connection):\n+        has_key_lookup = HasKey(self.lhs.lhs, self.lhs.key_name)\n         if not self.rhs:\n-            return HasKey(self.lhs.lhs, self.lhs.key_name).as_sqlite(compiler, connection)\n-        return super().as_sql(compiler, connection)\n+            return has_key_lookup.as_sqlite(compiler, connection)\n+        sql, params = has_key_lookup.as_sqlite(compiler, connection)\n+        return 'NOT (%s)' % sql, params\n \n \n class KeyTransformIn(lookups.In):\n\n```\n"}
{"instance_id": "django__django-13768", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/dispatch/dispatcher.py\n+++ b/django/dispatch/dispatcher.py\n@@ -1,3 +1,4 @@\n+import logging\n import warnings\n from weakref import WeakKeyDictionary, WeakSet\n\n@@ -10,6 +11,8 @@\n # A very lightweight dispatch mechanism, modeled after PyDispatcher.\n # See http://pydispatcher.sourceforge.net/\n\n+logger = logging.getLogger('django.dispatch')\n+\n NO_RECEIVERS = object()\n\n\n@@ -49,6 +52,7 @@\n             try:\n                 response = receiver(signal=self, sender=sender, **named)\n             except Exception as err:\n+                logger.exception('Exception raised in receiver %r', receiver)\n                 responses.append((receiver, err))\n             else:\n                 responses.append((receiver, response))\n```\n"}
{"instance_id": "django__django-13925", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -19,6 +19,7 @@\n     @classmethod\n     def _check_default_pk(cls):\n         if (\n+            not getattr(cls._meta.pk.remote_field, 'parent_link', False) and\n             cls._meta.pk.auto_created and\n             not settings.is_overridden('DEFAULT_AUTO_FIELD') and\n             not cls._meta.app_config._is_default_auto_field_overridden\n```\n"}
{"instance_id": "django__django-13933", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -10,8 +10,8 @@\n     # This class is a subclass of ChoiceField for purity, but it doesn't\n     # actually use any of ChoiceField's implementation.\n     default_error_messages = {\n-        'invalid_choice': _('Select a valid choice. That choice is not one of'\n-                            ' the available choices.'),\n+        'invalid_choice': _('Select a valid choice. %(value)s is not one of the'\n+                            ' available choices.'),\n     }\n     widget = Select\n     iterator = ModelChoiceIterator\n@@ -48,13 +48,17 @@\n     # ... other methods ...\n\n     def to_python(self, value):\n+        original_value = value\n         if value in self.empty_values:\n             return None\n         try:\n             key = self.to_field_name or 'pk'\n             if isinstance(value, self.queryset.model):\n                 value = getattr(value, key)\n+            # Keep original value before get() potentially fails or returns an object\n             value = self.queryset.get(**{key: value})\n         except (ValueError, TypeError, self.queryset.model.DoesNotExist):\n-            raise ValidationError(self.error_messages['invalid_choice'], code='invalid_choice')\n+            raise ValidationError(\n+                self.error_messages['invalid_choice'],\n+                code='invalid_choice',\n+                params={'value': original_value},\n+            )\n         return value\n\n     def validate(self, value):\n```\n"}
{"instance_id": "django__django-13964", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -47,18 +47,13 @@\n                         \"related object '%s'.\" % (operation_name, field.name)\n                     )\n                 # A pk may have been assigned manually to a model instance not\n-                # saved to the database (or auto-generated in a case like\n-                # UUIDField), but we allow the save to proceed and rely on the\n-                # database to raise an IntegrityError if applicable. If\n-                # constraints aren't supported by the database, there's the\n-                # unavoidable risk of data corruption.\n-                elif getattr(self, field.attname) is None:\n-                    # Use pk from related object if it has been saved after\n-                    # an assignment.\n+                # saved to the database (or auto-generated). Update the\n+                # associated FK value if it doesn't match the related\n+                # instance's primary key.\n+                elif getattr(self, field.attname) != obj.pk:\n                     setattr(self, field.attname, obj.pk)\n-                # If the relationship's pk/to_field was changed, clear the\n-                # cached relationship.\n-                if getattr(obj, field.target_field.attname) != getattr(self, field.attname):\n-                    field.delete_cached_value(self)\n+                    # Setting the fk via setattr calls the descriptor which will\n+                    # invalidate the previously cached instance on the field.\n+                    # ForeignObjectCache shouldn't be accessed. Refs #19359.\n \n     def delete(self, using=None, keep_parents=False):\n         using = using or router.db_for_write(self.__class__, instance=self)\n```\n"}
{"instance_id": "django__django-14016", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/query_utils.py\n+++ b/django/db/models/query_utils.py\n@@ -1,10 +1,16 @@\n import copy\n+import collections.abc\n from functools import partialmethod\n \n from django.core.exceptions import FieldError, EmptyResultSet\n from django.db.models.constants import LOOKUP_SEP\n from django.utils.tree import Node\n \n+# Dictionary view types are not pickleable. Convert them to lists before\n+# storing them in Q objects.\n+dict_view_types = (type({}.keys()), type({}.values()), type({}.items()))\n+\n+\n class Q(Node):\n     \"\"\"Encapsulate filters as objects that can be combined logically.\"\"\"\n     # Connection types\n@@ -12,6 +18,11 @@\n     OR = 'OR'\n     default = AND\n \n+    def __init__(self, *args, _connector=None, _negated=False, **kwargs):\n+        # Convert dictionary view objects to lists to avoid problems with pickling.\n+        for key, value in kwargs.items():\n+            if isinstance(value, dict_view_types):\n+                kwargs[key] = list(value)\n+        super().__init__(children=[*args, *sorted(kwargs.items())], connector=_connector, negated=_negated)\n+\n     def _combine(self, other, conn):\n         if not isinstance(other, Q):\n             raise TypeError(other)\n\n```\n"}
{"instance_id": "django__django-14017", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/query_utils.py\n+++ b/django/db/models/query_utils.py\n@@ -4,7 +4,7 @@\n from collections.abc import Collection, Iterable\n\n from django.core.exceptions import FieldError, MultipleObjectsReturned, ObjectDoesNotExist\n-from django.db.models.constants import LOOKUP_SEP\n+from django.db.models.constants import LOOKUP_SEP\n from django.db.models.expressions import BaseExpression\n from django.utils import tree\n from django.utils.deconstruct import deconstructible\n@@ -67,8 +67,12 @@\n         return obj\n \n     def _combine(self, other, conn):\n-        if not isinstance(other, Q):\n+        # Allow combining Q objects with Expression objects.\n+        if isinstance(other, BaseExpression):\n+            # Wrap the expression in a Q object.\n+            other = Q(other)\n+        elif not isinstance(other, Q):\n+            # Raise an error for non-Q, non-Expression types.\n             raise TypeError(other)\n \n         # If the other Q() is empty, ignore it and just use `self`.\n```\n"}
{"instance_id": "django__django-14155", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/urls/resolvers.py\n+++ b/django/urls/resolvers.py\n@@ -31,12 +31,19 @@\n \n class ResolverMatch:\n     def __init__(self, func, args, kwargs, url_name=None, app_names=None, namespaces=None, route=None, tried=None):\n-        self.func = func\n-        self.args = args\n-        self.kwargs = kwargs\n+        # Unwrap functools.partial() objects.\n+        if isinstance(func, functools.partial):\n+            self.func = func.func\n+            self.args = func.args + args\n+            self.kwargs = {**func.keywords, **kwargs}\n+        else:\n+            self.func = func\n+            self.args = args\n+            self.kwargs = kwargs\n+\n         self.url_name = url_name\n         self.route = route\n-        self.tried = tried\n+        self.tried = tried  # Contains patterns which were tried but didn't match.\n \n         # If a URLRegexResolver doesn't have a namespace or app_name, it passes\n         # in an empty value.\n@@ -46,13 +53,21 @@\n         self.namespaces = [x for x in namespaces if x] if namespaces else []\n         self.namespace = ':'.join(self.namespaces)\n \n-        if not hasattr(func, '__name__'):\n+        # Calculate the representation of the function object. This is used\n+        # several times, so compute it once.\n+        func = self.func  # Use the potentially unwrapped func.\n+        if not hasattr(func, '__name__'):\n             # A class-based view\n             self._func_path = func.__class__.__module__ + '.' + func.__class__.__name__\n+        elif inspect.isfunction(func):\n+            # A function-based view\n+            self._func_path = func.__module__ + '.' + func.__qualname__\n         else:\n-            # A function-based view\n-            self._func_path = func.__module__ + '.' + func.__qualname__\n+            # A callable object\n+            self._func_path = func.__module__ + '.' + func.__class__.__name__ + '.__call__'\n \n+    def __repr__(self):\n+        return (\n+            f\"<{self.__class__.__name__} {self._func_path} args={self.args!r} kwargs={self.kwargs!r} \"\n+            f\"url_name={self.url_name!r} app_name={self.app_name!r} namespace={self.namespace!r}>\"\n+        )\n\n class Resolver404(Exception):\n     pass\n```\n"}
{"instance_id": "django__django-14238", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -43,7 +43,9 @@\n         return isinstance(instance, self._subclasses) or super().__instancecheck__(instance)\n\n     def __subclasscheck__(self, subclass):\n-        return subclass in self._subclasses or super().__subclasscheck__(subclass)\n+        return (\n+            any(issubclass(subclass, cls) for cls in self._subclasses) or\n+            super().__subclasscheck__(subclass)\n+        )\n\n\n class AutoField(AutoFieldMixin, IntegerField, metaclass=AutoFieldMeta):\n```\n"}
{"instance_id": "django__django-14382", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/management/templates.py\n+++ b/django/core/management/templates.py\n@@ -33,7 +33,7 @@\n                  raise CommandError(e)\n          else:\n              if app_or_project == 'app':\n-                 self.validate_name(os.path.basename(target), 'directory')\n+                 self.validate_name(os.path.basename(target.rstrip(os.sep)), 'directory')\n              top_dir = os.path.abspath(os.path.expanduser(target))\n              if not os.path.exists(top_dir):\n                  raise CommandError(\"Destination directory '%s' does not \"\n```\n"}
{"instance_id": "django__django-14411", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/auth/widgets.py\n+++ b/django/contrib/auth/widgets.py\n@@ -23,3 +23,10 @@\n                 summary.append({'label': gettext(key), 'value': value_})\n         context['summary'] = summary\n         return context\n+\n+    def id_for_label(self, id_):\n+        \"\"\"\n+        Don't bind the 'for' attribute to the <label> tag, because this widget\n+        doesn't render a labelable HTML element.\n+        \"\"\"\n+        return None\n--- a/django/forms/boundfield.py\n+++ b/django/forms/boundfield.py\n@@ -109,7 +109,7 @@\n             doesn't render it as part of its output.\n             \"\"\"\n             id_ = self.id_for_label\n-            if id_:\n+            if id_:  # Add the 'for' attribute only if the widget specifies an ID.\n                 attrs = {**(attrs or {}), 'for': id_}\n             if self.field.required and hasattr(self.form, 'required_css_class'):\n                 attrs = {**(attrs or {}), 'class': self.form.required_css_class}\n\n```\n"}
{"instance_id": "django__django-14534", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/boundfield.py\n+++ b/django/forms/boundfield.py\n@@ -186,7 +186,12 @@\n \n     @property\n     def id_for_label(self):\n-        return 'id_%s_%s' % (self.data['name'], self.data['index'])\n+        \"\"\"\n+        Return the ID for the label. It relies on the ID calculation performed\n+        in the corresponding widget's `get_context()` method (which is passed\n+        to this BoundWidget) to ensure consistency.\n+        \"\"\"\n+        return self.data['attrs'].get('id')\n\n     @property\n     def choice_label(self):\n\n```\n"}
{"instance_id": "django__django-14580", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/writer.py\n+++ b/django/db/migrations/writer.py\n@@ -1,5 +1,6 @@\n \n import os\n+import re\n from importlib import import_module\n \n from django import get_version\n@@ -175,6 +176,14 @@\n         # Add migration base class import\n         imports.add('from django.db import migrations')\n \n+        # Ensure 'models' is imported if 'models.Model' appears directly in\n+        # the operations code. This can happen for example in `bases` lists\n+        # for CreateModel, where the serializer might not add the import.\n+        operations_string = \"\".join(operations_buff)\n+        if re.search(r'\\bmodels\\.Model\\b', operations_string):\n+            # The set automatically handles duplicates if serializer added it.\n+            imports.add('from django.db import models')\n+\n         # If there's a settings import, put it first\n         if hasattr(settings, 'AUTH_USER_MODEL') and settings.AUTH_USER_MODEL:\n             for i, imp in enumerate(sorted(imports)):\n```\n"}
{"instance_id": "django__django-14608", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/formsets.py\n+++ b/django/forms/formsets.py\n@@ -10,14 +10,15 @@\n \n     def non_form_errors(self):\n         \"\"\"\n-        Return an ErrorList of errors that aren't associated with a particular\n-        form -- i.e., from formset.clean(). Return an empty ErrorList if there\n-        are none.\n+        Return an ErrorList of errors that aren't associated with a\n+        particular form -- i.e., from formset.clean(). Return an empty\n+        ErrorList if there are none. The list has a CSS class of \"nonform\".\n         \"\"\"\n         if self._non_form_errors is None:\n             self.full_clean()\n         return self._non_form_errors\n \n+    _non_form_errors = None\n     @property\n     def errors(self):\n         \"\"\"Return a list of form.errors for every form in self.forms.\"\"\"\n@@ -219,7 +220,7 @@\n             # Give self.clean() a chance to do cross-form validation.\n             self.clean()\n         except ValidationError as e:\n-            self._non_form_errors = self.error_class(e.error_list)\n+            self._non_form_errors = self.error_class(e.error_list, css_class='nonform')\n \n     def clean(self):\n         \"\"\"\n```\n"}
{"instance_id": "django__django-14667", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -371,13 +371,11 @@\n         opts = self.get_meta()\n         loaded_fields = set()\n         deferred_set, defer = self.get_deferred_load_set()\n-\n         if not defer:\n-            # If we are loading specific fields, add the PK automatically.\n+            # Always load the PK in 'only' mode.\n+            loaded_fields.add(opts.pk.attname)\n             if deferred_set:\n-                loaded_fields.add(opts.pk.attname)\n                 loaded_fields.update(deferred_set)\n-            # Otherwise, take all fields except deferred ones.\n         else:\n             # If we are deferring fields, add all fields not explicitly deferred.\n             # PK is included by default.\n```\n"}
{"instance_id": "django__django-14672", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/reverse_related.py\n+++ b/django/db/models/fields/reverse_related.py\n@@ -16,7 +16,7 @@\n from django.db.models.query_utils import FilteredRelation, Q\n from django.db.models.utils import make_model_tuple\n from django.utils.deprecation import RemovedInDjango41Warning\n-from django.utils.functional import cached_property\n+from django.utils.functional import cached_property, partition\n from django.utils.hashable import make_hashable\n from django.utils.translation import gettext_lazy as _\n \n@@ -210,7 +210,7 @@\n             self.field.model._meta.label_lower,\n             self.field.name,\n             self.through._meta.label_lower,\n-            self.through_fields,\n+            make_hashable(self.through_fields),\n             self.related_name,\n             self.related_query_name,\n             make_hashable(self.limit_choices_to),\n```\n"}
{"instance_id": "django__django-14730", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/fields/related.py\n+++ b/django/db/models/fields/related.py\n@@ -21,6 +21,12 @@\n 20             symmetrical = (to == RECURSIVE_RELATIONSHIP_CONSTANT)\n 21 \n 22         if through is not None and db_table is not None:\n+23             raise ValueError(\n+24                 'Cannot specify a db_table if an intermediary model is used.'\n+25             )\n+26 \n+27         if symmetrical and related_name:\n+28             raise ValueError('Cannot specify a related_name argument for a symmetrical ManyToManyField.')\n 23             raise ValueError(\n 24                 'Cannot specify a db_table if an intermediary model is used.'\n 25             )\n\n```\n"}
{"instance_id": "django__django-14752", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/admin/views/autocomplete.py\n+++ b/django/contrib/admin/views/autocomplete.py\n@@ -25,14 +25,17 @@\n \n         self.object_list = self.get_queryset()\n         context = self.get_context_data()\n+        results = [\n+            self.serialize_result(obj, to_field_name)\n+            for obj in context['object_list']\n+        ]\n         return JsonResponse({\n-            'results': [\n-                {'id': str(getattr(obj, to_field_name)), 'text': str(obj)}\n-                for obj in context['object_list']\n-            ],\n+            'results': results,\n             'pagination': {'more': context['page_obj'].has_next()},\n         })\n \n+    def serialize_result(self, obj, to_field_name):\n+        return {'id': str(getattr(obj, to_field_name)), 'text': str(obj)}\n+\n     def get_paginator(self, *args, **kwargs):\n         \"\"\"Use the ModelAdmin's paginator.\"\"\"\n         return self.model_admin.get_paginator(self.request, *args, **kwargs)\n```\n"}
{"instance_id": "django__django-14787", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/utils/decorators.py\n+++ b/django/utils/decorators.py\n@@ -37,6 +37,8 @@\n         # 'func'. Also, wrap method.__get__() in a function because new\n         # attributes can't be set on bound method objects, only on functions.\n         bound_method = partial(method.__get__(self, type(self)))\n+        # Copy attributes (like __name__, __module__, __doc__, __dict__) from\n+        # the original method to the partial object\n+        update_wrapper(bound_method, method)\n         for dec in decorators:\n             bound_method = dec(bound_method)\n         return bound_method(*args, **kwargs)\n```\n"}
{"instance_id": "django__django-14855", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/admin/helpers.py\n+++ b/django/contrib/admin/helpers.py\n@@ -61,7 +61,11 @@\n             remote_field.model._meta.model_name,\n         )\n         try:\n-            url = reverse(url_name, args=[quote(remote_obj.pk)])\n+            # Use the admin site name from the model admin to determine the\n+            # correct admin site prefix, if needed.\n+            url = reverse(\n+                url_name, args=[quote(remote_obj.pk)], current_app=self.model_admin.admin_site.name\n+            )\n             return format_html('<a href=\"{}\">{}</a>', url, remote_obj)\n         except NoReverseMatch:\n             return str(remote_obj)\n```\n"}
{"instance_id": "django__django-14915", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -10,6 +10,9 @@\n             other = other.value\n         return self.value == other\n \n+    def __hash__(self):\n+        return hash(self.value)\n+\n \n class ModelChoiceIterator:\n     def __init__(self, field):\n```\n"}
{"instance_id": "django__django-14997", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "django__django-14999", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/operations/models.py\n+++ b/django/db/migrations/operations/models.py\n@@ -41,12 +41,13 @@\n         new_model = to_state.apps.get_model(app_label, self.new_name)\n         if self.allow_migrate_model(schema_editor.connection.alias, new_model):\n             old_model = from_state.apps.get_model(app_label, self.old_name)\n-            # Move the main table\n-            schema_editor.alter_db_table(\n-                new_model,\n-                old_model._meta.db_table,\n-                new_model._meta.db_table,\n-            )\n+            # Rename the main table if the db_table name has changed.\n+            if old_model._meta.db_table != new_model._meta.db_table:\n+                schema_editor.alter_db_table(\n+                    new_model,\n+                    old_model._meta.db_table,\n+                    new_model._meta.db_table,\n+                )\n             # Alter the fields pointing to us\n             for related_object in old_model._meta.related_objects:\n                 if related_object.many_to_many:\n```\n"}
{"instance_id": "django__django-15061", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/widgets.py\n+++ b/django/forms/widgets.py\n@@ -293,6 +293,12 @@\n             subwidgets.append(widget.get_context(widget_name, widget_value, widget_attrs)['widget'])\n         context['widget']['subwidgets'] = subwidgets\n         return context\n+\n+    def id_for_label(self, id_):\n+        \"\"\"\n+        MultiWidget has no single input element, so don't add the 'for' attribute.\n+        \"\"\"\n+        return ''\n \n     def value_from_datadict(self, data, files, name):\n         \"\"\"\n```\n"}
{"instance_id": "django__django-15202", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/validators.py\n+++ b/django/core/validators.py\n@@ -129,13 +129,16 @@\n                     self.schemes = schemes\n \n             def __call__(self, value):\n+                # Ensure the value is a string.\n                 value = str(value)\n-                # Check first if the scheme is valid\n-                scheme = value.split('://')[0].lower()\n-                if scheme not in self.schemes:\n-                    raise ValidationError(self.message, code='invalid_scheme', params={'value': value})\n+                try:\n+                    # Check first if the scheme is valid\n+                    scheme = value.split('://')[0].lower()\n+                    if scheme not in self.schemes:\n+                        raise ValidationError(self.message, code='invalid_scheme', params={'value': value})\n\n-                # Then check if the URL is valid using the regex.\n-                super().__call__(value)\n+                    # Then check if the URL is valid using the regex.\n+                    super().__call__(value)\n+                except ValueError:\n+                    # Catch ValueErrors from urlsplit() raised from regex validation\n+                    # or scheme splitting.\n+                    raise ValidationError(self.message, code='invalid', params={'value': value})\n```\n"}
{"instance_id": "django__django-15213", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/expressions.py\n+++ b/django/db/models/expressions.py\n@@ -1,5 +1,8 @@\n import copy\n from functools import partialmethod, wraps\n+\n+from django.db.models.fields import BooleanField\n+from django.db.models.query_utils import Q\n from django.db.models.sql.constants import (\n     CURRVAL, LCONN, NEXTVAL, RCONN, SHARED_LOCK, SKIP_LOCKED,\n )\n@@ -7,6 +10,7 @@\n     AND, OR, ORDER_DIR, ORDER_NULLS,\n )\n from django.db.models.sql.query import Query\n+from django.db.models.sql.where import EverythingNode, NothingNode\n from django.utils.deconstruct import deconstructible\n from django.utils.functional import cached_property\n from django.utils.hashable import make_hashable\n@@ -517,8 +521,25 @@\n     def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):\n         c = self.copy()\n         c.is_summary = summarize\n-        c.expression = c.expression.resolve_expression(query, allow_joins, reuse, summarize, for_save)\n+        expression_resolution = getattr(c.expression, 'resolve_expression', None)\n+        if expression_resolution:\n+            c.expression = expression_resolution(query, allow_joins, reuse, summarize, for_save)\n+\n+        # Convert Q() / ~Q() to Value(False) / Value(True) if possible.\n+        if isinstance(c.expression, Q) and query is not None:\n+            where = query.build_where(c.expression)\n+            if isinstance(where, NothingNode):\n+                c.expression = Value(False)\n+                # Re-resolve the new Value expression.\n+                c.expression = c.expression.resolve_expression(\n+                    query, allow_joins, reuse, summarize, for_save\n+                )\n+            elif isinstance(where, EverythingNode):\n+                c.expression = Value(True)\n+                # Re-resolve the new Value expression.\n+                c.expression = c.expression.resolve_expression(\n+                    query, allow_joins, reuse, summarize, for_save\n+                )\n         return c\n \n     def as_sql(self, compiler, connection):\n```\n"}
{"instance_id": "django__django-15252", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/recorder.py\n+++ b/django/db/migrations/recorder.py\n@@ -1,11 +1,29 @@\n-from django.db import DatabaseError\n+from django.db import DatabaseError, router\n from django.db.migrations.exceptions import MigrationSchemaMissing\n+from .models import Migration as MigrationModel\n+\n+\n+class MigrationRecorder:\n+    \"\"\"\n+    Deal with storing migration records in the database.\n+\n+    Because this table is actually itself used for dealing with model\n+    creation, it's tricky. We need to worry about the following scenarios:\n+\n+     - The table doesn't exist, and has to be created.\n+     - The table does exist, but has incorrect columns.\n+     - The table exists and is correct.\n+\n+    We map it to a Django model, but we can't rely on the ORM's normal behaviour\n+    as it might be outdated.\n+    \"\"\"\n+    _migration_class = None\n \n     @classmethod\n     def Migration(cls):\n         \"\"\"\n         Lazy load Migration model, as it may not be ready when this module is\n         imported.\n         \"\"\"\n-        # Dynamic import to prevent circular dependency\n         if not hasattr(cls, \"_migration_class\") or cls._migration_class is None:\n-            from .models import Migration\n-            cls._migration_class = Migration\n+            cls._migration_class = MigrationModel\n         return cls._migration_class\n \n     def __init__(self, connection):\n@@ -16,6 +34,10 @@\n     def migration_qs(self):\n         return self.Migration.objects.using(self.connection.alias)\n \n+    def _allowed(self):\n+        \"\"\"Check if the router allows migrations for the Migration model.\"\"\"\n+        return router.allow_migrate_model(self.connection.alias, self.Migration)\n+\n     def has_table(self):\n         \"\"\"Return True if the django_migrations table exists.\"\"\"\n         with self.connection.cursor() as cursor:\n@@ -23,16 +45,18 @@\n         return self.Migration._meta.db_table in tables\n \n     def ensure_schema(self):\n-        \"\"\"Ensure the table exists and has the correct schema.\"\"\"\n+        \"\"\"Ensure the table exists and has the correct schema, if allowed.\"\"\"\n+        # Check router rules first. If not allowed, exit immediately.\n+        if not self._allowed():\n+            return\n         # If the table's there, that's fine - we've never changed its schema\n         # in the codebase.\n         if self.has_table():\n             return\n         # Make the table\n         try:\n+            # We only get here if migrations are allowed and the table doesn't exist.\n             with self.connection.schema_editor() as editor:\n-                # Check if the router allows creating the table\n-                if not self._allowed():\n-                    return\n                 editor.create_model(self.Migration)\n         except DatabaseError as exc:\n             raise MigrationSchemaMissing(\"Unable to create the django_migrations table (%s)\" % exc)\n@@ -43,8 +67,13 @@\n         Return a dict mapping (app_name, migration_name) to Migration instances\n         for all applied migrations.\n         \"\"\"\n-        if self.has_table():\n-            return {(migration.app, migration.name): migration for migration in self.migration_qs}\n+        # Check router rules first. If not allowed, return empty dict.\n+        if not self._allowed():\n+            return {}\n+        # If allowed, proceed, checking for table existence.\n+        if self.has_table():\n+            # Ensure we use .all() to execute the query.\n+            return {(migration.app, migration.name): migration for migration in self.migration_qs.all()}\n         else:\n             # If the django_migrations table doesn't exist, then no migrations\n             # are applied.\n@@ -54,10 +83,14 @@\n     def record_applied(self, app, name):\n         \"\"\"Record that a migration was applied.\"\"\"\n         self.ensure_schema()\n-        self.migration_qs.create(app=app, name=name)\n+        # Only record if allowed. ensure_schema checks router rules.\n+        if self._allowed():\n+            self.migration_qs.create(app=app, name=name)\n \n     def record_unapplied(self, app, name):\n         \"\"\"Record that a migration was unapplied.\"\"\"\n+        # Only record if allowed.\n+        if self._allowed():\n+            self.migration_qs.filter(app=app, name=name).delete()\n \n     def flush(self):\n         \"\"\"Delete all migration records. Useful for testing migrations.\"\"\"\n```\n"}
{"instance_id": "django__django-15320", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/expressions.py\n+++ b/django/db/models/expressions.py\n@@ -713,6 +713,7 @@\n         # Deepcopy the query goal is to create a new query object which is\n         # decoupled from the original queryset. See RefsOuterRefs test case.\n         self.query = queryset.query.chain(Query)\n+        self.query.subquery = True\n         if not self.output_field:\n             # If output_field is not specified, guess it from the queryset.\n             if self.query.values_select:\n```\n"}
{"instance_id": "django__django-15347", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/messages/storage/cookie.py\n+++ b/django/contrib/messages/storage/cookie.py\n@@ -19,7 +19,7 @@\n             # Using 0/1 here instead of False/True to produce more compact json\n             is_safedata = 1 if isinstance(obj.message, SafeData) else 0\n             message = [self.message_key, is_safedata, obj.level, obj.message]\n-            if obj.extra_tags:\n+            if obj.extra_tags is not None:\n                 message.append(obj.extra_tags)\n             return message\n         return super().default(obj)\n\n```\n"}
{"instance_id": "django__django-15388", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/utils/autoreload.py\n+++ b/django/utils/autoreload.py\n@@ -257,11 +257,17 @@\n         self.watched_paths.clear()\n\n         # Watch template directories.\n+        # Avoid watching BASE_DIR directly, as watching the entire project root\n+        # for template changes is inefficient and can cause issues. Python files\n+        # in BASE_DIR are already watched separately.\n+        base_dir = Path(settings.BASE_DIR) if hasattr(settings, \"BASE_DIR\") else None\n         for directory in get_template_directories():\n-            self.watch_dir(directory, \"**/*.html\")\n+            if not (base_dir and Path(directory) == base_dir):\n+                self.watch_dir(directory, \"**/*.html\")\n         # Watch locale directories.\n         for directory in get_locale_directories():\n             self.watch_dir(directory, \"**/*.mo\")\n+\n         # Watch Python modules.\n         for file in chain(\n             iter_all_python_module_files(), iter_django_module_files()\n```\n"}
{"instance_id": "django__django-15400", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/utils/functional.py\n+++ b/django/utils/functional.py\n@@ -118,6 +118,12 @@\n     __ne__ = new_method_proxy(operator.ne)\n     __hash__ = new_method_proxy(hash)\n\n+    # Reflected operations\n+    def __radd__(self, other):\n+        if self._wrapped is empty:\n+            self._setup()\n+        return other + self._wrapped\n+\n     # List/Tuple/Dictionary methods support\n     __getitem__ = new_method_proxy(operator.getitem)\n     __setitem__ = new_method_proxy(operator.setitem)\n```\n"}
{"instance_id": "django__django-15498", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -304,6 +304,8 @@\n     Sunday, 06-Nov-94 08:49:37 GMT ; RFC 850, obsoleted by RFC 1036\n     Sun Nov  6 08:49:37 1994       ; ANSI C's asctime() format\n     \"\"\"\n+    if not date:\n+        return None\n     dt = _parse_http_date_safe(date)\n     if dt is None:\n         raise ValueError(\"%r is not in a valid HTTP date format\" % date)\n```\n"}
{"instance_id": "django__django-15695", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "django__django-15738", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/operations/models.py\n+++ b/django/db/migrations/operations/models.py\n@@ -1,5 +1,6 @@\n from django.db.migrations.operations.base import Operation\n from django.db.models.options import normalize_together\n+from django.db.models.fields import FieldDoesNotExist\n \n from .fields import FieldOperation, RemoveField, RenameField\n from .utils import field_references, get_references, resolve_relation\n@@ -44,15 +45,82 @@\n         )\n \n     def database_forwards(self, app_label, schema_editor, from_state, to_state):\n-        new_model = to_state.apps.get_model(app_label, self.name)\n-        if self.allow_migrate_model(schema_editor.connection.alias, new_model):\n-            old_model = from_state.apps.get_model(app_label, self.name)\n-            alter_together = getattr(schema_editor, \"alter_%s\" % self.option_name)\n-            alter_together(\n-                new_model,\n-                getattr(old_model._meta, self.option_name, set()),\n-                getattr(new_model._meta, self.option_name, set()),\n-            )\n+        new_model = to_state.apps.get_model(app_label, self.name)\n+        if self.allow_migrate_model(schema_editor.connection.alias, new_model):\n+            old_model = from_state.apps.get_model(app_label, self.name)\n+\n+            # Get old and new sets of constraints/indexes\n+            old_options = getattr(old_model._meta, self.option_name, set())\n+            new_options = getattr(new_model._meta, self.option_name, set())\n+            old_option_tuples = {tuple(fields) for fields in old_options}\n+            new_option_tuples = {tuple(fields) for fields in new_options}\n+\n+            # Determine constraint type and SQL command\n+            if self.option_name == \"unique_together\":\n+                constraint_kwargs = {\"unique\": True, \"primary_key\": False}\n+                sql_delete_constraint = schema_editor.sql_delete_unique\n+                sql_create_constraint = schema_editor._create_unique_sql\n+            elif self.option_name == \"index_together\":\n+                constraint_kwargs = {\"index\": True, \"unique\": False}\n+                sql_delete_constraint = schema_editor.sql_delete_index\n+                sql_create_constraint = schema_editor._create_index_sql\n+            else:\n+                # Should not happen for AlterUniqueTogether or AlterIndexTogether\n+                return\n+\n+            # Handle deletions using old_model state\n+            for field_names in old_option_tuples.difference(new_option_tuples):\n+                try:\n+                    # Validate fields exist in old_model before proceeding\n+                    for field_name in field_names:\n+                        old_model._meta.get_field(field_name)\n+\n+                    # Call _delete_composed_index using the old_model\n+                    schema_editor._delete_composed_index(\n+                        old_model,\n+                        field_names,\n+                        constraint_kwargs,\n+                        sql_delete_constraint,\n+                    )\n+                except FieldDoesNotExist:\n+                    # This might happen if a field involved in the constraint was\n+                    # also removed in the same migration. Deleting the constraint\n+                    # might be unnecessary or handled differently (e.g., cascade).\n+                    pass\n+\n+            # Handle creations using new_model state\n+            for field_names in new_option_tuples.difference(old_option_tuples):\n+                try:\n+                    fields = [new_model._meta.get_field(field) for field in field_names]\n+                    if self.option_name == \"index_together\":\n+                        # _create_index_sql needs a suffix for index_together\n+                        schema_editor.execute(\n+                            sql_create_constraint(\n+                                new_model, fields=fields, suffix=\"_idx\"\n+                            )\n+                        )\n+                    else:\n+                        # _create_unique_sql does not need suffix\n+                        schema_editor.execute(sql_create_constraint(new_model, fields))\n+                except FieldDoesNotExist:\n+                    # This indicates a problem with the migration or model state.\n+                    raise ValueError(\n+                        f\"Field missing in new model state while trying to create \"\n+                        f\"{self.option_name} {field_names}\"\n+                    )\n \n     def database_backwards(self, app_label, schema_editor, from_state, to_state):\n-        return self.database_forwards(app_label, schema_editor, from_state, to_state)\n+        # The state logic is reversed: from_state is the new state, to_state is the old state.\n+        current_model = from_state.apps.get_model(\n+            app_label, self.name\n+        )  # State *after* forward migration\n+        target_model = to_state.apps.get_model(\n+            app_label, self.name\n+        )  # State we are reverting *to*\n+\n+        if self.allow_migrate_model(\n+            schema_editor.connection.alias, current_model\n+        ):  # Check based on current model\n+            # Get options from the state *after* forward migration (current)\n+            # and the state we are reverting *to* (target)\n+            current_options = getattr(current_model._meta, self.option_name, set())\n+            target_options = getattr(target_model._meta, self.option_name, set())\n+            current_option_tuples = {tuple(fields) for fields in current_options}\n+            target_option_tuples = {tuple(fields) for fields in target_options}\n+\n+            # Determine constraint type and SQL command (same as forwards)\n+            if self.option_name == \"unique_together\":\n+                constraint_kwargs = {\"unique\": True, \"primary_key\": False}\n+                sql_delete_constraint = schema_editor.sql_delete_unique\n+                sql_create_constraint = schema_editor._create_unique_sql\n+            elif self.option_name == \"index_together\":\n+                constraint_kwargs = {\"index\": True, \"unique\": False}\n+                sql_delete_constraint = schema_editor.sql_delete_index\n+                sql_create_constraint = schema_editor._create_index_sql\n+            else:\n+                return\n+\n+            # Handle deletions (constraints/indexes that exist in 'current' but not in 'target')\n+            # Use 'current_model' state to find and delete them (reversing creation).\n+            for field_names in current_option_tuples.difference(target_option_tuples):\n+                try:\n+                    for field_name in field_names:\n+                        current_model._meta.get_field(field_name)\n+\n+                    schema_editor._delete_composed_index(\n+                        current_model,  # Use current_model for deletion in backwards\n+                        field_names,\n+                        constraint_kwargs,\n+                        sql_delete_constraint,\n+                    )\n+                except FieldDoesNotExist:\n+                    pass\n+\n+            # Handle creations (constraints/indexes that exist in 'target' but not in 'current')\n+            # Use 'target_model' state to create them (reversing deletion).\n+            for field_names in target_option_tuples.difference(current_option_tuples):\n+                try:\n+                    fields = [target_model._meta.get_field(field) for field in field_names]\n+                    if self.option_name == \"index_together\":\n+                        schema_editor.execute(\n+                            sql_create_constraint(\n+                                target_model, fields=fields, suffix=\"_idx\"\n+                            )\n+                        )\n+                    else:\n+                        schema_editor.execute(sql_create_constraint(target_model, fields))\n+                except FieldDoesNotExist:\n+                    raise ValueError(\n+                        f\"Field missing in target model state while trying to create \"\n+                        f\"{self.option_name} {field_names} during backwards migration\"\n+                    )\n \n     def references_field(self, model_name, name, app_label):\n         return self.references_model(model_name, app_label) and (\n```\n"}
{"instance_id": "django__django-15781", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/management/base.py\n+++ b/django/core/management/base.py\n@@ -4,7 +4,7 @@\n import argparse\n import os\n import sys\n-from argparse import ArgumentParser, HelpFormatter\n+from argparse import ArgumentParser, HelpFormatter, RawDescriptionHelpFormatter\n from io import TextIOBase\n \n import django\n@@ -183,6 +183,7 @@\n         parser = CommandParser(\n             prog=\"%s %s\" % (os.path.basename(prog_name), subcommand),\n             description=self.help or None,\n+            formatter_class=RawDescriptionHelpFormatter,\n             **kwargs,\n         )\n         # Add common options like --settings, --pythonpath, etc.\n```\n"}
{"instance_id": "django__django-15789", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/template/defaultfilters.py\n+++ b/django/template/defaultfilters.py\n@@ -1,5 +1,6 @@\n import random\n from decimal import ROUND_HALF_UP, Context, Decimal, DecimalException, InvalidOperation\n+from functools import partial\n \n from django.conf import settings\n from django.template import Variable, VariableDoesNotExist\n@@ -8,7 +9,7 @@\n from django.utils.dateformat import format, time_format\n from django.utils.encoding import iri_to_uri\n from django.utils.html import (\n-    _json_script, conditional_escape, escape, escapejs, format_html,\n+    _json_script as _json_script_escaped, conditional_escape, escape, escapejs, format_html,\n     html_safe, linebreaks, strip_tags, urlize,\n )\n from django.utils.http import urlquote\n@@ -24,6 +25,8 @@\n \n register = Library()\n \n+# This avoids shadowing the built-in function (https://bugs.python.org/issue21278)\n+_json_script = partial(_json_script_escaped, encoder=None)\n \n \n @register.filter(is_safe=True)\n--- a/django/utils/html.py\n+++ b/django/utils/html.py\n@@ -1,6 +1,7 @@\n \"\"\"HTML utilities suitable for global use.\"\"\"\n \n import html\n+import json\n import re\n from html.parser import HTMLParser\n from urllib.parse import parse_qsl, quote, unquote, urlencode, urlsplit, urlunsplit\n@@ -8,6 +9,7 @@\n from django.utils.encoding import punycode\n from django.utils.functional import Promise, keep_lazy, keep_lazy_text\n from django.utils.http import RFC3986_GENDELIMS, RFC3986_SUBDELIMS\n+from django.utils.module_loading import import_string\n from django.utils.regex_helper import _lazy_re_compile\n from django.utils.safestring import SafeData, SafeString, mark_safe\n from django.utils.text import normalize_newlines\n@@ -41,17 +43,20 @@\n }\n \n \n-def json_script(value, element_id=None):\n+def json_script(value, element_id=None, encoder=None):\n     \"\"\"\n     Escape all the HTML/XML special characters with their unicode escapes, so\n     value is safe to be output anywhere except for inside a tag attribute. Wrap\n     the escaped JSON in a script tag.\n+\n+    encoder is an optional JSONEncoder subclass to use. Default is\n+    django.core.serializers.json.DjangoJSONEncoder.\n     \"\"\"\n-    from django.core.serializers.json import DjangoJSONEncoder\n+    if encoder is None:\n+        from django.core.serializers.json import DjangoJSONEncoder\n+        encoder = DjangoJSONEncoder\n \n-    json_str = json.dumps(value, cls=DjangoJSONEncoder).translate(_json_script_escapes)\n+    json_str = json.dumps(value, cls=encoder).translate(_json_script_escapes)\n     if element_id:\n         template = '<script id=\"{}\" type=\"application/json\">{}</script>'\n         args = (element_id, mark_safe(json_str))\n```\n"}
{"instance_id": "django__django-15790", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/checks/templates.py\n+++ b/django/core/checks/templates.py\n@@ -49,19 +49,24 @@\n def check_for_template_tags_with_the_same_name(app_configs, **kwargs):\n     errors = []\n     libraries = defaultdict(list)\n-\n+\n     for conf in settings.TEMPLATES:\n+        # DjangoTemplates backend specific check\n+        if conf[\"BACKEND\"] != \"django.template.backends.django.DjangoTemplates\":\n+            continue\n         custom_libraries = conf.get(\"OPTIONS\", {}).get(\"libraries\", {})\n         for module_name, module_path in custom_libraries.items():\n             libraries[module_name].append(module_path)\n \n+    # Libraries are loaded from installed applications (\"templatetags\" package)\n+    # and builtins (\"django/templatetags\").\n     for module_name, module_path in get_template_tag_modules():\n         libraries[module_name].append(module_path)\n \n     for library_name, items in libraries.items():\n-        if len(items) > 1:\n+        # Check for duplicate paths for the same library name.\n+        unique_items = set(items)\n+        if len(unique_items) > 1:\n             errors.append(\n                 Error(\n                     E003.msg.format(\n                         repr(library_name),\n-                        \", \".join(repr(item) for item in items),\n+                        \", \".join(repr(item) for item in unique_items),\n                     ),\n                     id=E003.id,\n                 )\n\n```\n"}
{"instance_id": "django__django-15814", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -33,7 +33,7 @@\n             _, _, _, joins, _, _ = self.query.setup_joins([f.name], opts, root_alias)\n             alias = joins[-1]\n             columns = self.get_default_columns(\n-                start_alias=alias, opts=f.remote_field.model._meta\n+                start_alias=alias, opts=f.remote_field.model._meta.concrete_model._meta\n             )\n             for col in columns:\n                 select_fields.append(len(select))\n@@ -42,7 +42,7 @@\n             klass_info[\"select_fields\"] = select_fields\n             next_klass_infos = self.get_related_selections(\n                 select,\n-                f.remote_field.model._meta,\n+                f.remote_field.model._meta.concrete_model._meta,\n                 alias,\n                 cur_depth + 1,\n                 next,\n@@ -87,7 +87,9 @@\n                 related_klass_infos.append(klass_info)\n                 select_fields = []\n                 columns = self.get_default_columns(\n-                    start_alias=alias, opts=model._meta, from_parent=opts.model\n+                    start_alias=alias,\n+                    opts=model._meta.concrete_model._meta,\n+                    from_parent=opts.model,\n                 )\n                 for col in columns:\n                     select_fields.append(len(select))\n@@ -95,7 +97,7 @@\n                 klass_info[\"select_fields\"] = select_fields\n                 next = requested.get(f.related_query_name(), {})\n                 next_klass_infos = self.get_related_selections(\n-                    select, model._meta, alias, cur_depth + 1, next, restricted\n+                    select, model._meta.concrete_model._meta, alias, cur_depth + 1, next, restricted\n                 )\n                 get_related_klass_infos(klass_info, next_klass_infos)\n \n```\n"}
{"instance_id": "django__django-15819", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/management/commands/inspectdb.py\n+++ b/django/core/management/commands/inspectdb.py\n@@ -1,5 +1,6 @@\n import keyword\n import re\n+from collections import defaultdict\n \n from django.core.management.base import BaseCommand, CommandError\n from django.db import ConnectionHandler, connections\n@@ -126,6 +127,7 @@\n \n                     # Store field data before generating model output\n                     fields_data = []\n+                    relations_map = defaultdict(list)  # Map: related_model_repr -> [field_name]\n \n                     for i, row in enumerate(description):\n                         column_name = row.name\n@@ -172,6 +174,7 @@\n                                 field_params[\"to\"] = rel_to_repr\n                                 field_params[\"db_column\"] = repr(column_name)\n                                 att_name = self.normalize_col_name(att_name)\n+                                relations_map[rel_to_repr].append(att_name)\n                             else:\n                                 # Calling str() on relation object is not useful.\n                                 field_type = \"IntegerField\"\n@@ -209,6 +212,7 @@\n                             \"type\": field_type,\n                             \"params\": field_params,\n                             \"notes\": field_notes,\n+                            \"related_model_repr\": rel_to_repr if is_relation else None,\n                         }\n                         fields_data.append(current_field_data)\n \n@@ -220,10 +224,25 @@\n                         # {'name': 'm2m_field', 'type': 'ManyToManyField', 'params': {'to': \"'app.TargetModel'\", ...}, 'notes': []}\n                         rel_to_repr = field_info[\"params\"].get(\"to\")\n                         if rel_to_repr:\n+                            # Track M2M relation for clash detection\n                             relations_map[rel_to_repr].append(field_info[\"name\"])\n+                            field_info[\"related_model_repr\"] = rel_to_repr\n+                        else:\n+                            field_info[\"related_model_repr\"] = None\n                         fields_data.append(field_info)\n \n+                    # Add related_name where necessary to resolve clashes\n+                    clashed_fields = set()\n+                    for related_model_repr, field_names in relations_map.items():\n+                        if len(field_names) > 1:\n+                            clashed_fields.update(field_names)\n+\n+                    for field_data in fields_data:\n+                        if field_data[\"name\"] in clashed_fields and field_data[\"type\"] in (\"ForeignKey\", \"ManyToManyField\"):\n+                            if \"related_name\" not in field_data[\"params\"]:\n+                                field_data[\"params\"][\"related_name\"] = repr(field_data[\"name\"])\n+\n                     # Generate model output\n                     model_output = [\"\"]\n                     model_output.append(\"class %s(models.Model):\" % model_name)\n@@ -231,7 +250,7 @@\n                     field_output = []\n                     for field_data in fields_data:\n                         # Generate the field line string\n-                        field_desc = f\"{field_data['name']} = models.{field_data['type']}(\"\n+                        field_desc = f\"{field_data['name']} = models.{field_data['type']}(\" # noqa: B028\n                         params = field_data[\"params\"].copy()\n \n                         # Move 'to' argument first for relations\n@@ -244,7 +263,7 @@\n \n                         # Add keyword arguments sorted for consistency\n                         if params:\n-                            field_desc += \", \".join(f\"{k}={v}\" for k, v in params.items())\n+                            field_desc += \", \".join(f\"{k}={v}\" for k, v in sorted(params.items()))\n \n                         field_desc += \")\"\n \n```\n"}
{"instance_id": "django__django-15851", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/backends/postgresql/client.py\n+++ b/django/db/backends/postgresql/client.py\n@@ -31,9 +31,9 @@\n             args += [\"-h\", host]\n         if port:\n             args += [\"-p\", str(port)]\n+        args.extend(parameters)\n         if dbname:\n             args += [dbname]\n-        args.extend(parameters)\n \n         env = {}\n         if passwd:\n```\n"}
{"instance_id": "django__django-15902", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/formsets.py\n+++ b/django/forms/formsets.py\n@@ -1,12 +1,12 @@\n from django import forms\n from django.core.exceptions import ValidationError\n-from django.forms.renderers import get_default_renderer\n from django.forms.utils import ErrorList, RenderableFormMixin\n from django.forms.widgets import HiddenInput, NumberInput\n from django.utils.functional import cached_property\n from django.utils.inspect import func_supports_parameter\n from django.utils.safestring import mark_safe\n from django.utils.translation import gettext as _\n+from django.utils.translation import gettext_lazy\n from django.utils.translation import ngettext_lazy\n \n __all__ = (\"BaseFormSet\", \"formset_factory\", \"all_valid\")\n@@ -28,6 +28,19 @@\n         cleaned_data.setdefault(INITIAL_FORM_COUNT, 0)\n         return cleaned_data\n \n+    def render(self, template_name=None, context=None, renderer=None):\n+        \"\"\"\n+        Render the management form.\n+\n+        This method intentionally overrides RenderableMixin.render() to avoid\n+        issuing a deprecation warning when the default template is used,\n+        as the specific template is irrelevant for ManagementForm.\n+        \"\"\"\n+        renderer = renderer or self.renderer\n+        template = template_name or self.template_name\n+        context = context or self.get_context()\n+        # Skip the default.html deprecation warning check.\n+        return mark_safe(renderer.render(template, context))\n \n class BaseFormSet(RenderableFormMixin):\n     \"\"\"\n```\n"}
{"instance_id": "django__django-15996", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/serializer.py\n+++ b/django/db/migrations/serializer.py\n@@ -1,5 +1,6 @@\n import collections\n import datetime\n+import enum\n import functools\n import inspect\n import math\n@@ -106,11 +107,22 @@\n class EnumSerializer(BaseSerializer):\n     def serialize(self):\n         enum_class = self.value.__class__\n+        # If the enum is a Flag boundary, try to decompose it.\n+        if issubclass(enum_class, enum.Flag):\n+            members, uncovered = enum._decompose(enum_class, self.value)\n+            if not uncovered:\n+                # Sort members by value to ensure deterministic output.\n+                members.sort(key=lambda m: m.value)\n+                module = enum_class.__module__\n+                imports = {\"import %s\" % module}\n+                if not members:\n+                    # Handle the zero case.\n+                    return \"%s.%s(0)\" % (module, enum_class.__qualname__), imports\n+                return \" | \".join(\n+                    \"%s.%s.%s\" % (module, enum_class.__qualname__, member.name)\n+                    for member in members\n+                ), imports\n         module = enum_class.__module__\n-        return (\n-            \"%s.%s[%r]\" % (module, enum_class.__qualname__, self.value.name),\n-            {\"import %s\" % module},\n-        )\n+        imports = {\"import %s\" % module}\n+        return \"%s.%s[%r]\" % (module, enum_class.__qualname__, self.value.name), imports\n \n \n class FloatSerializer(BaseSimpleSerializer):\n@@ -229,7 +241,7 @@\n     _registry = {\n         # Builtins\n         dict: DictionarySerializer,\n-        models.Choices: ChoicesSerializer,\n+        enum.Enum: EnumSerializer,\n         enum.Enum: EnumSerializer,\n         datetime.datetime: DatetimeDatetimeSerializer,\n         (datetime.date, datetime.timedelta, datetime.time): DateTimeSerializer,\n@@ -248,6 +260,7 @@\n         uuid.UUID: UUIDSerializer,\n         pathlib.PurePath: PathSerializer,\n         os.PathLike: PathLikeSerializer,\n+        models.Choices: ChoicesSerializer,\n     }\n \n     @classmethod\n```\n"}
{"instance_id": "django__django-16041", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/forms/formsets.py\n+++ b/django/forms/formsets.py\n@@ -18,13 +18,16 @@\n \n     @property\n     def empty_form(self):\n+        form_kwargs = self.get_form_kwargs(None)\n+        # Prevent the form kwargs from overriding this value.\n+        form_kwargs.pop(\"empty_permitted\", None)\n         form = self.form(\n             auto_id=self.auto_id,\n             prefix=self.add_prefix(\"__prefix__\"),\n             empty_permitted=True,\n             use_required_attribute=False,\n-            **self.get_form_kwargs(None),\n             renderer=self.renderer,\n+            **form_kwargs,\n         )\n         self.add_fields(form, None)\n         return form\n\n```\n"}
{"instance_id": "django__django-16046", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/utils/numberformat.py\n+++ b/django/utils/numberformat.py\n@@ -88,7 +88,7 @@\n \n     # Sign\n     sign = ''\n-    if str_number[0] == '-':\n+    if str_number and str_number[0] == '-':\n         sign = '-'\n         str_number = str_number[1:]\n \n```\n"}
{"instance_id": "django__django-16139", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/auth/forms.py\n+++ b/django/contrib/auth/forms.py\n@@ -42,7 +42,10 @@\n         super().__init__(*args, **kwargs)\n         password = self.fields.get(\"password\")\n         if password:\n-            password.help_text = password.help_text.format(\"../password/\")\n+            # The password change link points to the change password page.\n+            # This is always relative to the admin root URL.\n+            change_password_url = f\"../../{self.instance.pk}/password/\"\n+            password.help_text = password.help_text.format(change_password_url)\n         user_permissions = self.fields.get(\"user_permissions\")\n         if user_permissions:\n             user_permissions.queryset = user_permissions.queryset.select_related(\n\n```\n"}
{"instance_id": "django__django-16229", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "django__django-16255", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/sitemaps/__init__.py\n+++ b/django/contrib/sitemaps/__init__.py\n@@ -167,7 +167,7 @@\n         if callable(self.lastmod):\n             try:\n                 return max([self.lastmod(item) for item in self.items()])\n-            except TypeError:\n+            except (TypeError, ValueError):\n                 return None\n         else:\n             return self.lastmod\n```\n"}
{"instance_id": "django__django-16379", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/core/cache/backends/filebased.py\n+++ b/django/core/cache/backends/filebased.py\n@@ -91,10 +91,10 @@\n\n     def has_key(self, key, version=None):\n         fname = self._key_to_file(key, version)\n-        if os.path.exists(fname):\n+        try:\n             with open(fname, \"rb\") as f:\n                 return not self._is_expired(f)\n-        return False\n+        except FileNotFoundError:\n+            return False\n\n     def _cull(self):\n         \"\"\"\n```\n"}
{"instance_id": "django__django-16400", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/auth/management/__init__.py\n+++ b/django/contrib/auth/management/__init__.py\n@@ -1,5 +1,6 @@\n \"\"\"\n-Creates permissions for all installed apps that need permissions.\n+Creates permissions for all installed apps that need permissions for the database\n+indicated by the ``using`` argument.\n \"\"\"\n import getpass\n import unicodedata\n@@ -10,7 +11,7 @@\n from django.contrib.auth.models import Permission\n from django.contrib.contenttypes.models import ContentType\n from django.core.management.base import CommandError\n-from django.db import DEFAULT_DB_ALIAS, router\n+from django.db import DEFAULT_DB_ALIAS, connections, router\n from django.dispatch import receiver\n from django.utils.encoding import smart_str\n from django.utils.translation import gettext, gettext_lazy as _\n@@ -27,7 +28,7 @@\n     if not router.allow_migrate_model(using, Permission):\n         return\n \n-    if verbosity >= 2:\n+    if verbosity >= 2 and kwargs.get(\"interactive\", True):\n         print(\"Running post-migrate handler for auth.permissions\")\n \n     searched_perms = []\n@@ -40,7 +41,7 @@\n         # This is needed to properly handle proxy models.\n         model = app_config.get_model(model_name, require_ready=False)\n         if not router.allow_migrate_model(using, model):\n-            continue\n+            continue\n \n         opts = model._meta\n         # The permissions we're looking for are stored in the ContentType map.\n@@ -48,7 +49,7 @@\n         # This avoids hitting the database for models that don't exist in the\n         # database yet. This can happen in case of clashes between models of the\n         # same name in different apps.\n-        try:\n+        try:\n             ctype = ContentType.objects.using(using).get_for_model(\n                 model, for_concrete_model=False\n             )\n@@ -60,7 +61,7 @@\n         all_perms = set(\n             Permission.objects.using(using)\n             .filter(\n-                content_type=ctype,\n+                content_type=ctype,\n             )\n             .values_list(\"codename\", flat=True)\n         )\n@@ -83,7 +84,7 @@\n             Permission(codename=codename, name=name, content_type=ctype)\n             for codename, name in perms_to_create\n         ]\n-        Permission.objects.using(using).bulk_create(permissions)\n+        Permission.objects.using(using).bulk_create(permissions)\n         if verbosity >= 2:\n             for perm in permissions:\n                 print(\"Adding permission '%s'\" % perm)\n@@ -96,7 +97,13 @@\n \n @receiver(post_migrate)\n def _create_permissions(**kwargs):\n-    # Ensure that permissions are created for the correct database.\n-    if kwargs.get(\"using\") == DEFAULT_DB_ALIAS:\n-        create_permissions(**kwargs)\n+    # The permissions need to be created in the database where the table is\n+    # actually created. It's possible that the ContentTypes table is created\n+    # as part of a migration but on a different database than the one specified\n+    # by the --database command line argument. Assume that the ContentTypes\n+    # table is part of the default database connection unless specified\n+    # otherwise.\n+    db = router.db_for_write(ContentType)\n+    if connections[db].settings_dict == connections[kwargs[\"using\"]].settings_dict:\n+        create_permissions(**kwargs)\n \n--- a/django/db/migrations/recorder.py\n+++ b/django/db/migrations/recorder.py\n@@ -62,7 +62,7 @@\n         # The database router is consulted when querying Migration.objects.\n         try:\n             return set(\n-                self.Migration.objects.values_list(\"app\", \"name\")\n+                self.Migration.objects.using(self.connection.alias).values_list(\"app\", \"name\")\n             )\n         except ProgrammingError:\n             # Table doesn't exist, so no migrations are applied.\n@@ -73,12 +73,12 @@\n         \"\"\"Record a migration as having been applied.\"\"\"\n         self.ensure_schema()\n         # Use the database specified by self.connection.alias.\n-        self.Migration.objects.create(app=app, name=name)\n+        self.Migration.objects.using(self.connection.alias).create(app=app, name=name)\n\n     def record_unapplied(self, app, name):\n         \"\"\"Record a migration as having been unapplied.\"\"\"\n         self.ensure_schema()\n         # Use the database specified by self.connection.alias.\n-        self.Migration.objects.filter(app=app, name=name).delete()\n+        self.Migration.objects.using(self.connection.alias).filter(app=app, name=name).delete()\n\n     def flush(self):\n```\n"}
{"instance_id": "django__django-16408", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -567,6 +567,10 @@\n             # Dynamically find the field instance if only the name is provided.\n             field = getattr(related_instance._meta.model, field_name).field\n             field.set_cached_value(related_instance, instance)\n+\n+        # Setter for FilteredRelation results.\n+        def filtered_relation_setter(instance, related_instance, alias_name):\n+            setattr(instance, alias_name, related_instance)\n \n         # Memoize the related klass infos that have been seen already.\n         related_klass_infos_seen = {}\n@@ -654,9 +658,10 @@\n                     klass_info = {\n                         \"model\": model,\n                         \"field\": final_field,\n-                        \"reverse\": True,\n-                        \"local_setter\": partial(local_setter, final_field),\n-                        \"remote_setter\": partial(remote_setter, name),\n+                        \"reverse\": False,  # Treat as forward relation for hydration.\n+                        # Use a setter that sets the attribute named by the alias.\n+                        \"local_setter\": partial(filtered_relation_setter, alias_name=name),\n+                        \"remote_setter\": lambda rel, inst, fn: None,  # No standard reverse cache.\n                         \"from_parent\": from_parent,\n                     }\n                     related_klass_infos.append(klass_info)\n```\n"}
{"instance_id": "django__django-16527", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/admin/templatetags/admin_modify.py\n+++ b/django/contrib/admin/templatetags/admin_modify.py\n@@ -18,9 +18,10 @@\n                 and change\n                 and context.get(\"show_delete\", True)\n             ),\n-            \"show_save_as_new\": not is_popup\n+            \"show_save_as_new\": (\n+                not is_popup\n+            and has_add_permission\n             and has_change_permission\n             and change\n-            and save_as,\n+            and save_as\n+            ),\n             \"show_save_and_add_another\": can_save_and_add_another,\n             \"show_save_and_continue\": can_save_and_continue,\n             \"show_save\": show_save and can_save,\n```\n"}
{"instance_id": "django__django-16595", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/operations/fields.py\n+++ b/django/db/migrations/operations/fields.py\n@@ -12,6 +12,10 @@\n             operation\n         ):\n             return [operation]\n+        elif isinstance(operation, AlterField) and self.is_same_field_operation(\n+            operation\n+        ):\n+            return [operation]\n         elif (\n             isinstance(operation, RenameField)\n             and self.is_same_field_operation(operation)\n\n```\n"}
{"instance_id": "django__django-16816", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/contrib/admin/checks.py\n+++ b/django/contrib/admin/checks.py\n@@ -1,3 +1,8 @@\n+from django.db.models.fields.related_descriptors import (\n+    ManyToManyDescriptor,\n+    ReverseManyToOneDescriptor,\n+    ReverseOneToOneDescriptor,\n+)\n from django.forms.models import BaseModelFormSet\n from django.template import TemplateDoesNotExist, engines\n from django.template.loader import find_template\n@@ -256,10 +261,10 @@\n         errors = []\n         list_display = obj.list_display\n         for index, item in enumerate(list_display):\n-            errors.extend(self._check_list_display_item(obj, model, item))\n+            errors.extend(self._check_list_display_item(obj, model, item, index))\n         return errors\n \n-    def _check_list_display_item(self, obj, model, item, field_name_prefix=\"\"):\n+    def _check_list_display_item(self, obj, model, item, index, field_name_prefix=\"\"):\n         if not isinstance(item, str):\n             # It is some sort of callable, so pass.\n             # TODO: Add options to check display functions.\n@@ -281,19 +286,37 @@\n                 )\n             ]\n         except FieldDoesNotExist:\n-            if not (\n-                hasattr(obj, item)\n-                or getattr(getattr(model, item, None), \"fget\", None)\n-                or getattr(model, item, None) is not None\n-            ):\n+            if hasattr(obj, item):\n+                # It's an attribute or method described on the ModelAdmin.\n+                attr = getattr(obj, item)\n+            elif getattr(getattr(model, item, None), \"fget\", None):\n+                # It's a property defined on the model.\n+                attr = getattr(model, item)\n+            else:\n+                attr = getattr(model, item, None)\n+\n+            # NEW: Check for reverse relations explicitly. They are attributes on the\n+            # model but are not fields and cannot be used in list_display.\n+            if isinstance(\n+                attr,\n+                (\n+                    ManyToManyDescriptor,\n+                    ReverseManyToOneDescriptor,\n+                    ReverseOneToOneDescriptor,\n+                ),\n+            ):\n+                return [\n+                    checks.Error(\n+                        f\"'{type(obj).__name__}.list_display[{index}]', '{item}' is a \"\n+                        f\"reverse relation and cannot be used in list_display.\",\n+                        hint=(\n+                            f\"Use a method on '{type(obj).__name__}' to display related \"\n+                            f\"objects.\"\n+                        ),\n+                        obj=obj.__class__,\n+                        id=\"admin.E108\",\n+                    )\n+                ]\n+            elif attr is None:\n                 # Neither field nor an attribute defined on ModelAdmin or model.\n                 return [\n                     checks.Error(\n-                        f\"'{field_name_prefix}{item}' refers to a field or attribute \"\n-                        f\"that cannot be found on {model._meta.object_name}.\",\n-                        obj=obj.__class__,\n-                        id=\"admin.E108\",\n-                    )\n-                ]\n-            else:\n-                attr = getattr(obj, item, None) or getattr(model, item, None)\n+                        f\"'{type(obj).__name__}.list_display[{index}]' refers to '{item}', \"\n+                        f\"which is not a callable, an attribute of '{type(obj).__name__}', \"\n+                        f\"or an attribute or field on '{model._meta.model_name}'.\",\n+                        obj=obj.__class__,\n+                        id=\"admin.E108\",\n+                    )\n+                ]\n+            # Otherwise, 'attr' is currently allowed.\n+            # attr is a callable or property defined on the ModelAdmin or the model.\n \n         # Check attributes defined on the model or ModelAdmin.\n         if attr:\n```\n"}
{"instance_id": "django__django-16820", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/autodetector.py\n+++ b/django/db/migrations/autodetector.py\n@@ -761,23 +761,43 @@\n             model_name,\n             dependencies,\n         ) in self._get_altered_foo_together_operations(option_name):\n-            removed_values = old_value.difference(new_value)\n+            # Calculate initial differences\n+            removed_values = old_value.difference(new_value)\n             added_values = new_value.difference(old_value)\n-            if removed_values == old_value and not new_value:\n+\n+            effective_removed_values = removed_values\n+            effective_added_values = added_values\n+\n+            # If dealing with index_together, account for values moved to Meta.indexes\n+            if option_name == \"index_together\":\n+                renamed = self.renamed_index_together_values.get(\n+                    (app_label, model_name), []\n+                )\n+                # Filter out removed values that were actually renamed to indexes\n+                effective_removed_values = {\n+                    val for val in removed_values if val not in renamed\n+                }\n+\n+            # If, after filtering, there are no effective changes left, skip generating\n+            # any AlterIndexTogether or RemoveIndexTogether operation.\n+            if not effective_removed_values and not effective_added_values:\n+                continue\n+\n+            # Check if this change represents a complete removal of all original values\n+            # and no new values were added.\n+            is_complete_removal = removed_values == old_value and not new_value\n+\n+            if is_complete_removal:\n+                # Generate RemoveFooTogether operation only if there were effective\n+                # removals (i.e., not everything was renamed).\n+                # Note: effective_added_values must be empty in this case.\n+                if effective_removed_values:\n+                    # RemovedInDjango51Warning.\n+                    self.add_operation(\n+                        app_label,\n+                        operation_class.removal_operation_class(model_name),\n+                        dependencies=dependencies,\n+                        beginning=True,\n+                    )\n+            elif effective_removed_values or effective_added_values:\n                 # RemovedInDjango51Warning.\n-                # Generate RemoveFooTogether operation.\n-                self.add_operation(\n-                    app_label,\n-                    operation_class.removal_operation_class(model_name),\n-                    dependencies=dependencies,\n-                    beginning=True,\n-                )\n-            elif removed_values or added_values:\n-                # RemovedInDjango51Warning.\n-                # Generate AlterFooTogether operation.\n                 self.add_operation(\n                     app_label,\n                     operation_class(model_name, new_value),\n```\n"}
{"instance_id": "django__django-16873", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/template/defaultfilters.py\n+++ b/django/template/defaultfilters.py\n@@ -413,16 +413,12 @@\n \n         Argument: The separator to use.\n         \"\"\"\n-        try:\n-            # Assume arg is a SafeData instance. If not, conditional_escape will be\n-            # called on the result below.\n-            data = mark_safe(str(arg))\n-        except (AttributeError, TypeError):\n-            # The mark_safe call is needed because the separator is used\n-            # before the later conditional_escape.\n-            data = mark_safe(conditional_escape(str(arg)))\n+        sep = str(arg)\n+        if autoescape:\n+            sep = conditional_escape(sep)\n+        # The separator is always safe after conditional escaping (if necessary),\n+        # because it will be used in the final join operation.\n+        data = mark_safe(sep)\n \n         try:\n             value = [force_str(v) for v in value]\n\n```\n"}
{"instance_id": "django__django-16910", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -116,7 +116,11 @@\n                 )\n                 continue\n \n-            related_field_name = f.field.related_query_name()\n+            # Use the field name on the reverse relation for OneToOne relations.\n+            if isinstance(f, OneToOneRel):\n+                related_field_name = f.name\n+            else:\n+                related_field_name = f.field.related_query_name()\n             if restricted:\n                 next = requested.get(related_field_name, {})\n                 if not next and related_field_name not in requested:\n@@ -144,7 +148,7 @@\n                 )\n \n             if load_fields:\n-                related_load_fields = {\n+                fields_to_load = {\n                     k[len(related_field_name) + len(LOOKUP_SEP)] : v\n                     for k, v in load_fields.items()\n                     if k.startswith(related_field_name + LOOKUP_SEP)\n@@ -152,7 +156,7 @@\n             else:\n                 fields_to_load = None\n \n-            if fields_to_load is not None and not fields_to_load:\n+            if fields_to_load is not None and not fields_to_load and restricted:\n                 # An empty dictionary means no fields from the related model\n                 # should be loaded. Skip recursion.\n                 continue\n```\n"}
{"instance_id": "django__django-17051", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -115,9 +115,11 @@\n      ):\n          \"\"\"\n          Insert each of the instances into the database. Do *not* call\n-         save() on each of the instances, do not send any pre/post_save\n-         signals, and do not set the primary key attribute if it is an\n-         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n+         save() on each instance, do not send any pre/post_save signals.\n+         The primary key attribute is assigned to the objects created on\n+         backends that support returning primary keys from insert\n+         (features.can_return_rows_from_bulk_insert=True), unless\n+         ignore_conflicts=True is used.\n          Multi-table models are not supported.\n          \"\"\"\n          # When you bulk insert you don't get the primary keys back (if it's an\n@@ -1445,7 +1447,8 @@\n             returning_fields = (\n                 opts.db_returning_fields\n                 if connection.features.can_return_rows_from_bulk_insert\n-                and on_conflict is None\n+                # Allow returning fields for regular inserts and update_conflicts.\n+                and (on_conflict is None or on_conflict == OnConflict.UPDATE)\n                 else ()\n             )\n             returned_columns = self._batched_insert(\n```\n"}
{"instance_id": "django__django-17087", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/django/db/migrations/serializer.py\n+++ b/django/db/migrations/serializer.py\n@@ -179,50 +179,8 @@\n             # It's a method on a class.\n             # Work out the name based on the parent.\n             try:\n-                klass = inspect.getmodule(self.value).__dict__[\n-                    self.value.__qualname__.split(\".\")[0]\n-                ]\n-                # Find the class the method is defined on.\n-                defining_class = klass\n-                for name in self.value.__qualname__.split(\".\")[1:]:\n-                    try:\n-                        attr = getattr(defining_class, name)\n-                        # If it is a class method, descriptor is on the defining class.\n-                        if isinstance(attr, classmethod):\n-                            attr = attr.__func__\n-                        # If it is a static method, descriptor is on the defining class.\n-                        elif isinstance(attr, staticmethod):\n-                            attr = attr.__func__\n-                    except AttributeError:\n-                        # Look on MRO.\n-                        defining_class_mro = list(defining_class.__mro__[1:])\n-                        while defining_class_mro:\n-                            base = defining_class_mro.pop(0)\n-                            try:\n-                                attr = getattr(base, name)\n-                                if isinstance(attr, classmethod):\n-                                    attr = attr.__func__\n-                                elif isinstance(attr, staticmethod):\n-                                    attr = attr.__func__\n-                            except AttributeError:\n-                                continue\n-                            # If it's defined on this class\n-                            if getattr(attr, \"__globals__\", {}) == base.__dict__:\n-                                klass = base\n-                                break\n-                        else:\n-                            # Not found on MRO. Break outer loop.\n-                            defining_class = klass\n-                            break\n-                        # Found on MRO. Break outer loop.\n-                        break\n-                    # If it is a method defined on this class (or a\n-                    # classmethod/staticmethod). Check by matching __globals__.\n-                    if getattr(attr, \"__globals__\", {}) == defining_class.__dict__ or (\n-                        isinstance(\n-                            getattr(defining_class, name, None), (classmethod, staticmethod)\n-                        )\n-                        and attr.__globals__ == defining_class.__dict__\n-                    ):\n-                        klass = defining_class\n-                        break\n-                    # If it's not defined on this class, maybe it's defined\n-                    # on the next level? e.g. Outer.Inner.method where Inner is the class.\n-                    elif inspect.isclass(attr):\n-                        defining_class = attr\n-                    # Otherwise, loop will continue. If it finishes, klass\n-                    # remains the outer class.\n-\n+                # Ensure the qualified name is usable from the module.\n+                inspect.getattr_static(module, self.value.__qualname__.split(\".\")[0])\n                 name = self.value.__qualname__\n             except (AttributeError, KeyError):\n                 # Just use the qualname. If it's not directly accessible through the\n@@ -230,18 +188,6 @@\n                 # e.g. when a class method is applied to a child model that\n                 # doesn't have the method defined on it.\n                 name = self.value.__qualname__\n-            # Check if the klass we found is the root one, or one found midway.\n-            if klass is not inspect.getmodule(self.value).__dict__[\n-                self.value.__qualname__.split(\".\")[0]\n-            ]:\n-                # If the module is different, we need to import it specially.\n-                if klass.__module__ != self.value.__module__:\n-                    module_name = klass.__module__\n-                    name = \"%s.%s\" % (klass.__name__, self.value.__name__)\n-                # Otherwise, reconstruct the name based on the new class's qualname.\n-                else:\n-                    name = \"%s.%s\" % (klass.__qualname__, self.value.__name__)\n-\n         else:\n             # It's a standalone function.\n             name = self.value.__name__\n```\n"}
{"instance_id": "matplotlib__matplotlib-18869", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/__init__.py\n+++ b/lib/matplotlib/__init__.py\n@@ -1,5 +1,7 @@\n \"\"\"\n An object-oriented plotting library.\n+\n+from pathlib import Path\n \n A procedural interface is provided by the companion package\n :mod:`matplotlib.pyplot`.\n@@ -11,28 +13,57 @@\n   year      = 2007\n }\"\"\"\n \n+# Need to allow users to override the backend based on environment variable.\n+# Must be imported before any other matplotlib imports.\n+from . import rcsetup\n+from packaging.version import parse as parse_version\n+\n+# Try to import _version.py, generated by setuptools_scm\n+try:\n+    from . import _version\n+except ImportError:\n+    # Could happen if package is not installed or setuptools_scm failed.\n+    # Set a default version string, or consider raising an error.\n+    class _DummyVersion:\n+        version = \"0.0.0+unknown\"\n+    _version = _DummyVersion()\n+\n+\n+_log = rcsetup._log\n+\n+__version__ = None  # Will be set by __getattr__\n+version_info = None # Will be set by __getattr__\n+\n+def _get_version_string():\n+    \"\"\"Get the version string, caching it in __version__.\"\"\"\n+    global __version__\n+    if __version__ is None:\n+        import setuptools_scm\n+        root = Path(__file__).resolve().parents[2]\n+        if (root / \".git\").exists() and not (root / \".git/shallow\").exists():\n+            try:\n+                __version__ = setuptools_scm.get_version(\n+                    root=root,\n+                    version_scheme=\"post-release\",\n+                    local_scheme=\"node-and-date\",\n+                    fallback_version=_version.version,\n+                )\n+            except Exception:\n+                __version__ = _version.version # Fallback\n+        else:  # Get the version from the _version.py setuptools_scm file.\n+            __version__ = _version.version\n+    return __version__\n+\n \n def __getattr__(name):\n+    global version_info\n     if name == \"__version__\":\n-        import setuptools_scm\n-        global __version__  # cache it.\n-        # Only shell out to a git subprocess if really needed, and not on a\n-        # shallow clone, such as those used by CI, as the latter would trigger\n-        # a warning from setuptools_scm.\n-        root = Path(__file__).resolve().parents[2]\n-        if (root / \".git\").exists() and not (root / \".git/shallow\").exists():\n-            __version__ = setuptools_scm.get_version(\n-                root=root,\n-                version_scheme=\"post-release\",\n-                local_scheme=\"node-and-date\",\n-                fallback_version=_version.version,\n-            )\n-        else:  # Get the version from the _version.py setuptools_scm file.\n-            __version__ = _version.version\n-        return __version__\n+        return _get_version_string()\n+    if name == \"version_info\":\n+        if version_info is None:\n+            version_str = _get_version_string()\n+            parsed_v = parse_version(version_str)\n+            version_info = (parsed_v.major, parsed_v.minor, parsed_v.micro)\n+        return version_info\n     raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n \n \n@@ -40,6 +71,7 @@\n \n     # Quickfix to ensure Microsoft Visual C++ redistributable\n     # DLLs are loaded before importing kiwisolver\n+    import importlib\n     from . import ft2font\n \n     for modname, minver in [\n```\n"}
{"instance_id": "matplotlib__matplotlib-22711", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/widgets.py\n+++ b/lib/matplotlib/widgets.py\n@@ -910,13 +910,13 @@\n             xy[1] = .25, val[1]\n             xy[2] = .75, val[1]\n             xy[3] = .75, val[0]\n-            xy[4] = .25, val[0]\n+            # xy[4] = .25, val[0]  # Polygon has only 4 vertices.\n         else:\n             xy[0] = val[0], .25\n             xy[1] = val[0], .75\n             xy[2] = val[1], .75\n             xy[3] = val[1], .25\n-            xy[4] = val[0], .25\n+            # xy[4] = val[0], .25  # Polygon has only 4 vertices.\n         self.poly.xy = xy\n         self.valtext.set_text(self._format(val))\n         if self.drawon:\n```\n"}
{"instance_id": "matplotlib__matplotlib-22835", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/artist.py\n+++ b/lib/matplotlib/artist.py\n@@ -23,11 +23,14 @@\n                 return \"[]\"\n             normed = self.norm(data)\n             if np.isfinite(normed):\n-                # Midpoints of neighboring color intervals.\n-                neighbors = self.norm.inverse(\n-                    (int(self.norm(data) * n) + np.array([0, 1])) / n)\n-                delta = abs(neighbors - data).max()\n-                g_sig_digits = cbook._g_sig_digits(data, delta)\n+                try:\n+                    # Midpoints of neighboring color intervals.\n+                    neighbors = self.norm.inverse(\n+                        (int(self.norm(data) * n) + np.array([0, 1])) / n)\n+                    delta = abs(neighbors - data).max()\n+                    g_sig_digits = cbook._g_sig_digits(data, delta)\n+                except ValueError:  # Handle non-invertible norms.\n+                    g_sig_digits = 3\n             else:\n                 g_sig_digits = 3  # Consistent with default below.\n             return \"[{:-#.{}g}]\".format(data, g_sig_digits)\n```\n"}
{"instance_id": "matplotlib__matplotlib-23299", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/__init__.py\n+++ b/lib/matplotlib/__init__.py\n@@ -618,12 +618,17 @@\n         return self\n \n     def __exit__(self, exc_type, exc_value, traceback):\n-        # Need to clear the figure manager created during context,\n-        # otherwise it will persist with the wrong backend. See #11141\n-        # Don't modify Gcf if an exception has occurred, as it may break\n-        # test cleanup.\n+        # Figures created within the context may be stale if the context\n+        # changed the backend. Destroy them unless the context only triggered\n+        # the initial backend resolution (i.e. the backend was unset/auto\n+        # before the context). See #11141, #23338.\n+        # Do not destroy figures if an exception occurred, as this may break\n+        # test cleanup logic.\n         if exc_type is None:\n-            _pylab_helpers.Gcf.destroy_all()\n+            backend_before = self._orig._get_backend_or_none()\n+            backend_after_ctx = rcParams._get_backend_or_none()\n+            if backend_after_ctx != backend_before and backend_before is not None:\n+                _pylab_helpers.Gcf.destroy_all()\n \n         # Restore the original rcparams values.\n         # This does not call the set() method, but directly updates the\n```\n"}
{"instance_id": "matplotlib__matplotlib-23314", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/mpl_toolkits/mplot3d/axes3d.py\n+++ b/lib/mpl_toolkits/mplot3d/axes3d.py\n@@ -261,6 +261,17 @@\n         self.mouse_init()\n         self.figure.canvas.callbacks.connect('button_press_event', self.button_press)\n \n+    def set_visible(self, vis):\n+        \"\"\"\n+        Set the visibility of the Axes3D.\n+\n+        Parameters\n+        ----------\n+        vis : bool\n+        \"\"\"\n+        super().set_visible(vis)\n+        for ax in [self.xaxis, self.yaxis, self.zaxis]:\n+            ax.set_visible(vis)\n+        self.patch.set_visible(vis)\n+        self.stale = True\n+\n     def _get_axis_list(self):\n         return self.xaxis, self.yaxis, self.zaxis\n \n```\n"}
{"instance_id": "matplotlib__matplotlib-23476", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/backends/backend_macosx.py\n+++ b/lib/matplotlib/backends/backend_macosx.py\n@@ -28,6 +28,10 @@\n class FigureCanvasMac(_macosx.FigureCanvas, FigureCanvasBase):\n     # docstring inherited\n\n+    # Remove the Mac-specific _set_device_pixel_ratio, inherit from base class.\n+    # The base class implementation correctly handles DPI scaling based on\n+    # the *change* in device pixel ratio.\n+\n     def __init__(self, figure):\n         # Must store figure settings before initializing the C++ side,\n         # as it may call back into Python.\n@@ -42,24 +46,16 @@\n         height = int(h * figure.dpi)\n         _macosx.FigureCanvas.__init__(self, width, height)\n\n+        # Set the initial dpi_ratio based on the current backend value.\n+        # This ensures that when the base class's _set_device_pixel_ratio\n+        # is called (e.g., by the C++ backend init), it compares the\n+        # actual ratio against the correct starting ratio, preventing\n+        # incorrect scaling of the figure's DPI (which may have been restored\n+        # from pickle).\n+        actual_ratio = super(FigureCanvasMac, self).get_device_pixel_ratio()\n+        self._dpi_ratio = actual_ratio\n+\n         figure.set_canvas(self)\n-        self._set_device_pixel_ratio(self._dpi_ratio)\n-\n-    def _set_device_pixel_ratio(self, ratio):\n-        # The ratio provided is the backing store scale factor, but the\n-        # FigureCanvasMac dpi ratio is defined relative to 72dpi.\n-        # (FIXME: Is this needed?)\n-        ratio = ratio * 72 / self.figure.dpi\n-        if self._dpi_ratio != ratio:\n-            # Pass physical dpi to figure.\n-            # Use the original ratio (backing store scale factor) here? No, the calculation above is used.\n-            # This results in figure.dpi = (ratio * 72 / figure.dpi) * figure.dpi = ratio * 72\n-            # where ratio is the backing store scale factor. This seems wrong.\n-            self.figure.set_dpi(ratio * self.figure.dpi, forward=False)\n-            self._dpi_ratio = ratio\n-\n-    # This method is called by the C++ code when the OS notifies that\n-    # the backing store scale factor (device pixel ratio) changed.\n     def _update_device_pixel_ratio(self):\n         # This must be called whenever the backing store scale factor changes.\n         # It will call the base class' _set_device_pixel_ratio with the\n\n```\n"}
{"instance_id": "matplotlib__matplotlib-23562", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/mpl_toolkits/mplot3d/art3d.py\n+++ b/lib/mpl_toolkits/mplot3d/art3d.py\n@@ -26,13 +26,15 @@\n         self.stale = True\n \n     def get_facecolor(self):\n-        return self._facecolors2d\n-\n+        # Return the facecolors defined by the collection.\n+        # This returns the same thing as PolyCollection.get_facecolor\n+        return self._facecolors\n     get_facecolors = get_facecolor\n \n     def get_edgecolor(self):\n-        return self._edgecolors2d\n-\n+        # Return the edgecolors defined by the collection.\n+        # This returns the same thing as PolyCollection.get_edgecolor\n+        return self._edgecolors\n     get_edgecolors = get_edgecolor\n \n \n```\n"}
{"instance_id": "matplotlib__matplotlib-23563", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/mpl_toolkits/mplot3d/art3d.py\n+++ b/lib/mpl_toolkits/mplot3d/art3d.py\n@@ -212,7 +212,13 @@\n         self.stale = True\n \n     def draw(self, renderer):\n-        xs3d, ys3d, zs3d = self._verts3d\n+        # Check if _verts3d exists. It might be missing if the object\n+        # was not fully initialized, e.g. due to an error during\n+        # line_2d_to_3d conversion.\n+        if not hasattr(self, '_verts3d'):\n+            # Don't draw the line if essential data is missing.\n+            return\n+        xs3d, ys3d, zs3d = self._verts3d\n         xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)\n         # Check for nan values.\n         # Save the NAN states flags, revert when done.\n```\n"}
{"instance_id": "matplotlib__matplotlib-23913", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/legend.py\n+++ b/lib/matplotlib/legend.py\n@@ -49,6 +49,7 @@\n from matplotlib.container import ErrorbarContainer, BarContainer, StemContainer\n from . import legend_handler\n \n+\n # Maps legend locations to the corresponding anchor point of the legend box.\n # The values are normalized coordinates in the legend box, where\n # (0, 0) is the lower-left corner and (1, 1) is the upper-right corner.\n@@ -275,7 +276,7 @@\n                  handler_map=None, title_fontproperties=None, alignment=\"center\",\n                  *,\n                  ncols=1,  # TODO: Deprecate? (present in kwargs below too)\n-                 **kwargs):\n+                 draggable=False, **kwargs):\n \n         \"\"\"\n         Parameters\n@@ -404,6 +405,10 @@\n             The alignment of the legend title and the box of entries.\n             The entries are aligned as a single block, so that *alignment*\n             value ``'left'`` means that the left edges of the title and the\n+            block of entries are aligned.\n+        draggable : bool, default: False\n+            Whether the legend can be dragged with the mouse. The legend\n+            position (``loc``) is updated upon release.\n         **kwargs\n             Keyword arguments forwarded to `.FancyBboxPatch`.\n         \"\"\"\n@@ -411,6 +416,7 @@\n         # `Axes.legend` and `Figure.legend`.\n         super().__init__()\n         self.set_figure(parent.figure)\n+        self._draggable = None  # Initialized by self.set_draggable.\n \n         _api.check_in_list(self.codes.keys(), loc=loc)\n         self._loc_used_default = loc == 'best'  # Allow detection later.\n@@ -563,6 +569,9 @@\n         # Draggable legends are created by the set_draggable method, but we need\n         # to keep track of the original location and parent.\n         self._original_loc = loc\n+\n+        if draggable:\n+            self.set_draggable(True)\n \n     def _parse_legend_args(self, parent, handles, labels, **kwargs):\n         \"\"\"Parse the legend arguments for the `Legend` class.\"\"\"\n@@ -703,6 +712,7 @@\n         self.set_picker(True)\n         self._update_prop = _api.deprecated(\"3.7\")(self._update_prop)\n \n+\n     def _set_artist_props(self, a):\n         \"\"\"Set figure, transform, and clip box for artist *a*.\"\"\"\n         a.set_figure(self.figure)\n@@ -716,6 +726,7 @@\n         # clip path, but it is going to be updated in the draw method.\n         a.set_clip_path(self.get_window_extent())\n \n+\n     @_api.deprecated(\"3.7\", alternative=\"set_alignment\")\n     def _set_align(self, align):\n         \"\"\"Set the alignment of the legend title and the handles+text.\"\"\"\n@@ -898,6 +909,7 @@\n         self.stale = True\n         return self._draggable\n \n+\n     def get_draggable(self):\n         \"\"\"Return ``True`` if the legend is draggable, ``False`` otherwise.\"\"\"\n         return self._draggable is not None\n```\n"}
{"instance_id": "matplotlib__matplotlib-23964", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/backends/backend_ps.py\n+++ b/lib/matplotlib/backends/backend_ps.py\n@@ -669,12 +669,13 @@\n                 continue\n \n             stream = self._get_text_ps(clean_line, ismath=ismath)\n-            for ps_name, xs_names in stream:\n-                if ps_name != self.fontname:\n-                    self.set_font(ps_name, self.fontsize)\n-                self._pswriter.write(\n-                    f\"<{xs_names}> {_nums_to_str(currx, curry)} \"\n-                    f\"{angle:1.1f} {dx:1.3f} {dy:1.3f} {w:1.3f} {h:1.3f} \"\n-                    f\"{d:1.3f} {rgb[0]:.3f} {rgb[1]:.3f} {rgb[2]:.3f} at\\n\")\n+            # stream can be None if the line is empty after stripping ligatures.\n+            if stream is not None:\n+                for ps_name, xs_names in stream:\n+                    if ps_name != self.fontname:\n+                        self.set_font(ps_name, self.fontsize)\n+                    self._pswriter.write(\n+                        f\"<{xs_names}> {_nums_to_str(currx, curry)} \"\n+                        f\"{angle:1.1f} {dx:1.3f} {dy:1.3f} {w:1.3f} {h:1.3f} \"\n+                        f\"{d:1.3f} {rgb[0]:.3f} {rgb[1]:.3f} {rgb[2]:.3f} at\\n\")\n \n             if debugPS:\n```\n"}
{"instance_id": "matplotlib__matplotlib-23987", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "matplotlib__matplotlib-24149", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/axes/_axes.py\n+++ b/lib/matplotlib/axes/_axes.py\n@@ -2179,11 +2179,12 @@\n             # removes the units from unit packages like `pint` that\n             # wrap numpy arrays.\n             try:\n-                x0 = cbook._safe_first_finite(x0)\n-            except (TypeError, IndexError, KeyError):\n+                _x0_finite = cbook._safe_first_finite(x0)\n+                # Check the finite element for units and apply to dx\n+                if hasattr(_x0_finite, 'unit'):\n+                    dx = dx * _x0_finite.unit\n+            except (TypeError, IndexError, KeyError, StopIteration):\n+                # If x0 is empty, has no finite values, or doesn't support\n+                # indexing/iteration, we cannot determine its units this way.\n                 pass\n-            if hasattr(x0, 'unit'):\n-                dx = dx * x0.unit\n             return dx\n         except (TypeError, ValueError, AttributeError, units.ConversionError):\n             # If not, then perhaps x0 is numeric and dx is timedelta?\n```\n"}
{"instance_id": "matplotlib__matplotlib-24265", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "matplotlib__matplotlib-24334", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/axis.py\n+++ b/lib/matplotlib/axis.py\n@@ -208,6 +208,8 @@\n \n     # Wrapper around set_ticklabels used to generate Axes.set_x/ytickabels; can\n     # go away once the API of Axes.set_x/yticklabels becomes consistent.\n+    # TODO: Make consistent with set_ticks signature.\n+    # TODO: fontdict is deprecated.\n     def _set_ticklabels(self, labels, *, fontdict=None, minor=False, **kwargs):\n         \"\"\"\n         Set this Axis' labels with list of string labels.\n@@ -329,6 +331,9 @@\n         return self.get_major_ticks(len(ticks))\n \n     def set_ticks(self, ticks, labels=None, *, minor=False, **kwargs):\n+        # Avoid side effects in downstream processing.\n+        kwargs = kwargs.copy()\n+\n         \"\"\"\n         Set this Axis' tick locations and optionally labels.\n \n@@ -357,6 +362,15 @@\n             pass *labels*. In other cases, please use `~.Axes.tick_params`.\n         \"\"\"\n         ticks = self._set_tick_locations(ticks, minor=minor)\n+        if kwargs:\n+            from matplotlib.artist import _valid_property\n+            from matplotlib.text import Text  # Avoid circular import.\n+            valid_keys = _valid_property(Text)\n+            for key in kwargs:\n+                if key not in valid_keys:\n+                    # Raise an error similar to Text.update or setp.\n+                    raise AttributeError(f'{type(Text).__name__!r} object '\n+                                         f'has no property {key!r}')\n         if labels is not None:\n             self.set_ticklabels(labels, minor=minor, **kwargs)\n         return ticks\n```\n"}
{"instance_id": "matplotlib__matplotlib-24970", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/colors.py\n+++ b/lib/matplotlib/colors.py\n@@ -726,13 +726,19 @@\n         # Clip values > N-1 and < 0\n         # Assign the bad-value index to masked elements.\n         # Set the over-value index to values > N-1.\n-        # Set the under-value index to values < 0.\n+        # Set the under-value index to values < 0. Use np.array(value).astype(\n+        # xa.dtype) to avoid NumPy deprecation warnings about assigning\n+        # out-of-range integers.\n         mask_bad = ma.getmask(xa)\n         if not cbook.is_scalar(mask_bad) and mask_bad.any():\n             if self._i_bad is not None:\n-                xa[mask_bad] = self._i_bad\n+                xa[mask_bad] = np.array(self._i_bad).astype(xa.dtype)\n         if self._i_over is not None:\n-            xa[xa > self.N - 1] = self._i_over\n+            # Note: \"> N - 1\" instead of \">= N\" to handle unsigned integers.\n+            xa[xa > self.N - 1] = np.array(self._i_over).astype(xa.dtype)\n         if self._i_under is not None:\n-            xa[xa < 0] = self._i_under\n+            # The check xa < 0 is False for unsigned integers like uint8, but\n+            # this line is harmless. The cast ensures the assignment works\n+            # correctly if _i_under is out of range for the dtype.\n+            xa[xa < 0] = np.array(self._i_under).astype(xa.dtype)\n\n         # Mask values that are already outside the valid range.\n```\n"}
{"instance_id": "matplotlib__matplotlib-25079", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "matplotlib__matplotlib-25311", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/legend.py\n+++ b/lib/matplotlib/legend.py\n@@ -10,6 +10,32 @@\n     def get_draggable(self):\n         \"\"\"Return ``True`` if the legend is draggable, ``False`` otherwise.\"\"\"\n         return self._draggable is not None\n+\n+    def __getstate__(self):\n+        state = self.__dict__.copy()\n+        # The DraggableLegend object is not picklable, partly because it holds\n+        # a reference to the canvas. We need to store its state and remove it.\n+        if self._draggable is not None:\n+            draggable_info = {\n+                'use_blit': self._draggable.use_blit,\n+                'update': self._draggable._update_loc,\n+            }\n+            state['_draggable_info'] = draggable_info\n+            # Remove the DraggableLegend instance itself from the state.\n+            state.pop('_draggable', None)\n+        return state\n+\n+    def __setstate__(self, state):\n+        draggable_info = state.pop('_draggable_info', None)\n+        self.__dict__.update(state)\n+        # Ensure _draggable is initialized to None before potentially recreating.\n+        self._draggable = None\n+        # Recreate the DraggableLegend object if it was originally present.\n+        # This relies on the Figure/Axes being fully reconstructed first so\n+        # that the canvas is available for event connections. Figure.__setstate__\n+        # handles canvas setup before restoring artists like Legend.\n+        if draggable_info is not None:\n+            self.set_draggable(True, **draggable_info)\n \n \n # Helper functions to parse legend arguments for both `figure.legend` and\n```\n"}
{"instance_id": "matplotlib__matplotlib-25332", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/figure.py\n+++ b/lib/matplotlib/figure.py\n@@ -1,5 +1,7 @@\n import logging\n import pathlib\n+from collections import defaultdict\n+import weakref\n \n import numpy as np\n \n@@ -10,7 +12,7 @@\n from matplotlib import axes, gridspec as mgridspec, legend as mlegend\n from matplotlib.artist import Artist, artist_inspector, allow_rasterization\n from matplotlib.cbook import _setattr_cm, _warn_external\n-from matplotlib.backend_bases import (\n+from matplotlib.cbook import Grouper\n     _Backend, FigureCanvasBase, FigureManagerBase, NavigationToolbar2,\n     RendererBase)\n from matplotlib.colors import Colormap, Normalize\n@@ -38,6 +40,22 @@\n         self.align_xlabels(axs=axs)\n         self.align_ylabels(axs=axs)\n \n+    def __getstate__(self):\n+        # Get the state from the FigureBase, which handles the canvas and observers\n+        state = super().__getstate__()\n+        # Remove the _align_label_groups entry, as it uses Grouper which holds\n+        # weak references, which are not pickleable. The alignment state will\n+        # be lost on unpickling.\n+        state.pop('_align_label_groups', None)\n+        return state\n+\n+    def __setstate__(self, state):\n+        # Restore the state using FigureBase's __setstate__\n+        super().__setstate__(state)\n+        # Re-initialize the _align_label_groups attribute to its default state,\n+        # as it was removed in __getstate__.\n+        self._align_label_groups = defaultdict(Grouper)\n+\n     def add_gridspec(self, nrows=1, ncols=1, **kwargs):\n         \"\"\"\n         Return a `.GridSpec` that has this figure as a parent.  This allows\n```\n"}
{"instance_id": "matplotlib__matplotlib-25433", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/widgets.py\n+++ b/lib/matplotlib/widgets.py\n@@ -1436,11 +1436,15 @@\n \n         self.valtext.set_text(self._format((vmin, vmax)))\n \n-        if self.drawon:\n-            self.ax.figure.canvas.draw_idle()\n-        self.val = (vmin, vmax)\n+        # Update internal state *before* observers and drawing, matching SliderBase.\n+        new_val = (vmin, vmax)\n+        self.val = new_val\n+\n         if self.eventson:\n-            self._observers.process(\"changed\", (vmin, vmax))\n+            self._observers.process(\"changed\", new_val)\n+\n+        if self.drawon:  # Draw after observers.\n+            # Check if axes is still part of a figure; callback may have deleted it.\n+            if self.ax.figure is not None and self.ax.figure.canvas is not None:\n+                self.ax.figure.canvas.draw_idle()\n \n     def on_changed(self, func):\n         \"\"\"\n```\n"}
{"instance_id": "matplotlib__matplotlib-25442", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/offsetbox.py\n+++ b/lib/matplotlib/offsetbox.py\n@@ -1565,8 +1565,11 @@\n \n     def disconnect(self):\n         \"\"\"Disconnect the callbacks.\"\"\"\n-        for cid in self._cids:\n-            self.canvas.mpl_disconnect(cid)\n+        # Check if ref_artist and its figure are valid *before* accessing canvas\n+        if self.ref_artist is not None and self.ref_artist.figure is not None:\n+            canvas = self.ref_artist.figure.canvas\n+            if canvas is not None:\n+                for cid in self._cids:\n+                    canvas.mpl_disconnect(cid)\n         self._cids = []\n         self.got_artist = False\n         if hasattr(self, 'patch') and self.patch:\n```\n"}
{"instance_id": "matplotlib__matplotlib-25498", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/colorbar.py\n+++ b/lib/matplotlib/colorbar.py\n@@ -28,6 +28,11 @@\n         self.cmap = mappable.cmap\n         if mappable.norm != self.norm:\n             self.norm = mappable.norm\n+            # If the norm has changed, ensure the new norm is updated\n+            # with data limits from the mappable, in case the user just\n+            # assigned a new norm instance to the mappable without scaling it.\n+            # We use autoscale_None to avoid overriding limits already set\n+            # on the norm instance.\n+            self.mappable.autoscale_None()\n             self._reset_locator_formatter_scale()\n \n         self._draw_all()\n```\n"}
{"instance_id": "matplotlib__matplotlib-26011", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/matplotlib/axes/_base.py\n+++ b/lib/matplotlib/axes/_base.py\n@@ -3267,9 +3267,10 @@\n         if auto:\n             self._request_autoscale_view(\"x\")\n \n+        # Always notify callbacks after limits are set.\n+        self.callbacks.process('xlim_changed', self)\n+\n         if emit:\n-            self.callbacks.process('xlim_changed', self)\n             # Call all of the other x-axes that are shared with this one\n             for other in self._shared_x_axes.get_siblings(self):\n                 if other is not self:\n@@ -3357,9 +3358,10 @@\n         if auto:\n             self._request_autoscale_view(\"y\")\n \n+        # Always notify callbacks after limits are set.\n+        self.callbacks.process('ylim_changed', self)\n+\n         if emit:\n-            self.callbacks.process('ylim_changed', self)\n             # Call all of the other y-axes that are shared with this one\n             for other in self._shared_y_axes.get_siblings(self):\n                 if other is not self:\n```\n"}
{"instance_id": "matplotlib__matplotlib-26020", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/lib/mpl_toolkits/axes_grid1/axes_grid.py\n+++ b/lib/mpl_toolkits/axes_grid1/axes_grid.py\n@@ -3,6 +3,7 @@\n from matplotlib import ticker\n\n from . import axes_divider\n+from . import mpl_axes  # For isinstance check in _tick_only\n from .mpl_axes import Axes  # Default axes class\n\n\n@@ -20,17 +21,34 @@\n     return ax\n\n\n-def _tick_only(ax, bottom_on, left_on):\n+def _tick_only(ax, bottom_on, left_on):\n     # A helper function to make axes only draw ticks and ticklabels without\n     # the axes spines.\n+    # This is tricky as the implementation depends on the axes class.\n+    # - For mpl_toolkits.axes_grid1.mpl_axes.Axes, ax.axis is a dict-like\n+    #   object mapping {\"left\", \"bottom\", \"right\", \"top\"} to AxisArtist\n+    #   instances.\n+    # - For standard matplotlib.axes.Axes (and subclasses like GeoAxes),\n+    #   ax.axis is a method, and control is via ax.xaxis, ax.yaxis, ax.spines.\n\n     bottom_off = not bottom_on\n     left_off = not left_on\n\n-    # Calling ax.axis('off') hides all spines, ticks, ticklabels and labels.\n-    # We need to turn spines off, and ticks, ticklabels, and labels on or off\n-    # selectively.\n-    # ax.axis[\"left\"] is the Y axis, ax.axis[\"bottom\"] is the X axis\n-    ax.axis[\"left\"].toggle(ticklabels=left_off, label=left_off)\n-    ax.axis[\"bottom\"].toggle(ticklabels=bottom_off, label=bottom_off)\n-    ax.axis[\"right\"].set_visible(False)\n-    ax.axis[\"top\"].set_visible(False)\n+    if isinstance(ax, mpl_axes.Axes):\n+        # Original behavior for axes_grid1 internal axes using AxisDict\n+        # ax.axis[\"left\"] is the Y axis, ax.axis[\"bottom\"] is the X axis\n+        ax.axis[\"left\"].toggle(ticklabels=left_off, label=left_off)\n+        ax.axis[\"bottom\"].toggle(ticklabels=bottom_off, label=bottom_off)\n+        ax.axis[\"right\"].set_visible(False)\n+        ax.axis[\"top\"].set_visible(False)\n+    else:\n+        # Fallback for standard matplotlib axes and subclasses (like GeoAxes)\n+        # Hide top and right axes elements completely\n+        ax.spines['right'].set_visible(False)\n+        ax.spines['top'].set_visible(False)\n+        ax.xaxis.set_ticks_position('bottom')  # Ensure ticks are only on bottom\n+        ax.yaxis.set_ticks_position('left')    # Ensure ticks are only on left\n+        ax.xaxis.set_tick_params(which='both', top=False, labeltop=False)\n+        ax.yaxis.set_tick_params(which='both', right=False, labelright=False)\n+        # Control bottom axis elements (spine, ticks, tick labels, axis label)\n+        ax.spines['bottom'].set_visible(bottom_on)\n+        ax.xaxis.set_tick_params(which='both', bottom=bottom_on, labelbottom=bottom_on)\n+        ax.xaxis.label.set_visible(bottom_on)\n+        # Control left axis elements (spine, ticks, tick labels, axis label)\n+        ax.spines['left'].set_visible(left_on)\n+        ax.yaxis.set_tick_params(which='both', left=left_on, labelleft=left_on)\n+        ax.yaxis.label.set_visible(left_on)\n\n\n class Grid:\n```\n"}
{"instance_id": "mwaskom__seaborn-2848", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n<patch>\n--- a/seaborn/axisgrid.py\n+++ b/seaborn/axisgrid.py\n@@ -1257,6 +1257,14 @@\n             # Get the names of the hue levels\n             hue_names = utils.categorical_order(data[hue], hue_order)\n\n+            # Filter the data frame to only include the hue levels\n+            # that will be plotted.\n+            if hue_order is not None:\n+                data = data[data[hue].isin(hue_order)]\n+                if data.empty:\n+                    warnings.warn(f\"Dataset is empty after filtering by `hue_order`.\")\n+                    # Note: plotting functions should handle empty data gracefully\n+\n             # --- Map hue levels to colors ---\n             n_colors = len(hue_names)\n             self.palette = palettes.color_palette(palette, n_colors)\n</patch>\n```\n"}
{"instance_id": "mwaskom__seaborn-3010", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/seaborn/_stats/regression.py\n+++ b/seaborn/_stats/regression.py\n@@ -21,13 +21,17 @@\n     gridsize: int = 100\n \n     def _fit_predict(self, data):\n+\n+        # Drop rows with missing data\n+        data = data.dropna(subset=[\"x\", \"y\"])\n \n         x = data[\"x\"]\n         y = data[\"y\"]\n-        if x.nunique() <= self.order:\n+\n+        # Check if sufficient data remains for the fit\n+        if len(data) <= self.order:\n             # TODO warn?\n             xx = yy = []\n         else:\n             p = np.polyfit(x, y, self.order)\n             xx = np.linspace(x.min(), x.max(), self.gridsize)\n\n```\n"}
{"instance_id": "mwaskom__seaborn-3190", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/seaborn/_core/properties.py\n+++ b/seaborn/_core/properties.py\n@@ -71,8 +71,16 @@\n \n     def default_scale(self, data: Series) -> Scale:\n         \"\"\"Given data, initialize appropriate scale class.\"\"\"\n-        # TODO check data type and return Nominal() when appropriate\n-        return Continuous()\n+        # Return Nominal scale for boolean, object, or categorical data\n+        if (\n+            data.dtype == bool\n+            or pd.api.types.is_categorical_dtype(data)\n+            or pd.api.types.is_object_dtype(data)\n+            or pd.api.types.is_string_dtype(data)\n+        ):\n+            return Nominal()\n+        # Default to Continuous for other types (presumably numeric)\n+        return Continuous()\n \n     def infer_scale(self, arg: Any, data: Series) -> Scale:\n         \"\"\"Given data and a scaling argument, initialize appropriate scale class.\"\"\"\n```\n"}
{"instance_id": "mwaskom__seaborn-3407", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "pallets__flask-4045", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/flask/blueprints.py\n+++ b/src/flask/blueprints.py\n@@ -29,6 +29,7 @@\n             root_path=root_path,\n         )\n         self.name = name\n+        assert \".\" not in name, \"Blueprint name should not contain dots\"\n         self.url_prefix = url_prefix\n         self.subdomain = subdomain\n         self.deferred_functions: t.List[DeferredSetupFunction] = []\n```\n"}
{"instance_id": "pallets__flask-4992", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/flask/config.py\n+++ b/src/flask/config.py\n@@ -11,12 +11,15 @@\n 10             absolute path or relative to the config root path.\n 11         :param load: A callable that takes a file handle and returns a\n 12             mapping of loaded data from the file.\n-13         :type load: ``Callable[[Reader], Mapping]`` where ``Reader``\n-14             implements a ``read`` method.\n+13         :type load: ``Callable[[IO], Mapping]`` where ``IO`` is the file\n+14             handle opened with ``mode``.\n 15         :param silent: Ignore the file if it doesn't exist.\n-16         :return: ``True`` if the file was loaded successfully.\n+16         :param mode: The mode to open the file with. Defaults to ``\"r\"``\n+17             (text mode). Use ``\"rb\"`` for binary mode.\n+18         :return: ``True`` if the file was loaded successfully.\n 19 \n-20         .. versionadded:: 2.0\n+21         .. versionadded:: 2.0\n+22         .. versionchanged:: 2.3\n+23            Added the ``mode`` parameter.\n 24         \"\"\"\n 25         filename = os.path.join(self.root_path, filename)\n 26 \n 27         try:\n-28             with open(filename) as f:\n+29             with open(filename, mode=mode) as f:\n 30                 obj = load(f)\n 31         except OSError as e:\n 32             if silent and e.errno in (errno.ENOENT, errno.EISDIR):\n@@ -45,12 +48,22 @@\n 41         .. code-block:: python\n 42 \n 43             import json\n-44             app.config.from_file(\"config.json\", load=json.load)\n+44             # load json file\n+45             app.config.from_file(\"config.json\", load=json.load)\n 46 \n 47             import toml\n+48             # load toml file requires pip install toml\n 49             app.config.from_file(\"config.toml\", load=toml.load)\n 50 \n+51             # load toml file with the stdlib requires python >= 3.11\n+52             try:\n+53                 import tomllib\n+54             except ImportError:\n+55                 # Help users install feedback\n+56                 raise RuntimeError(\"Please install tomllib or use Python 3.11+\")\n+57 \n+58             app.config.from_file(\"config.toml\", load=tomllib.load, mode=\"rb\")\n+59 \n 60         :param filename: The path to the data file. This can be an\n 61             absolute path or relative to the config root path.\n 62         :param load: A callable that takes a file handle and returns a\n@@ -65,7 +78,8 @@\n         self,\n         filename: str,\n         load: t.Callable[[t.IO[t.Any]], t.Mapping],\n-        silent: bool = False,\n+        silent: bool = False,\n+        mode: str = \"r\",\n     ) -> bool:\n         \"\"\"Update the values in the config from a file that is loaded\n         using the ``load`` parameter. The loaded data is passed to the\n@@ -74,12 +88,22 @@\n         .. code-block:: python\n \n             import json\n+            # load json file\n             app.config.from_file(\"config.json\", load=json.load)\n \n             import toml\n+            # load toml file requires pip install toml\n             app.config.from_file(\"config.toml\", load=toml.load)\n \n+            # load toml file with the stdlib requires python >= 3.11\n+            try:\n+                import tomllib\n+            except ImportError:\n+                # Help users install feedback\n+                raise RuntimeError(\"Please install tomllib or use Python 3.11+\")\n+\n+            app.config.from_file(\"config.toml\", load=tomllib.load, mode=\"rb\")\n+\n         :param filename: The path to the data file. This can be an\n```\n"}
{"instance_id": "pallets__flask-5063", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/flask/cli.py\n+++ b/src/flask/cli.py\n@@ -354,11 +354,13 @@\n     rules = list(app.url_map.iter_rules())\n     if not rules:\n         click.echo(\"No routes were registered.\")\n+        click.echo('Hint: Use \"flask run --host=0.0.0.0\" to expose the server.')\n         return\n \n     ignored_methods = set(() if all_methods else (\"HEAD\", \"OPTIONS\"))\n \n     if sort == \"rule\":\n+        # Sort by domain first, then rule\n         rules.sort(key=lambda rule: (rule.subdomain or \"\", rule.rule))\n     elif sort == \"endpoint\":\n         rules.sort(key=lambda rule: rule.endpoint)\n@@ -366,16 +368,29 @@\n     rows = []\n     column_widths = {\n         \"endpoint\": 0,\n+        \"domain\": 0,\n         \"methods\": 0,\n         \"rule\": 0,\n     }\n \n+    server_name = app.config.get(\"SERVER_NAME\")\n+    default_domain = server_name or \"<no server name>\"\n+\n     for rule in rules:\n         methods = \", \".join(sorted(rule.methods - ignored_methods))\n+\n+        domain_str = default_domain\n+        if rule.subdomain:\n+            if server_name:\n+                domain_str = f\"{rule.subdomain}.{server_name}\"\n+            else:\n+                # Indicate subdomain exists but server name is not set\n+                domain_str = f\"{rule.subdomain}.<no server name>\"\n+\n         rows.append(\n-            {\"endpoint\": rule.endpoint, \"methods\": methods, \"rule\": rule.rule}\n+            {\"domain\": domain_str, \"endpoint\": rule.endpoint, \"methods\": methods, \"rule\": rule.rule}\n         )\n-        column_widths[\"endpoint\"] = max(column_widths[\"endpoint\"], len(rule.endpoint))\n+        column_widths[\"domain\"] = max(column_widths[\"domain\"], len(domain_str))\n+        column_widths[\"endpoint\"] = max(column_widths[\"endpoint\"], len(rule.endpoint))\n         column_widths[\"methods\"] = max(column_widths[\"methods\"], len(methods))\n         column_widths[\"rule\"] = max(column_widths[\"rule\"], len(rule.rule))\n \n@@ -383,16 +398,18 @@\n \n     header = (\n         f\"{'Domain':<{column_widths['domain']}}  \"\n-        f\"{'Endpoint':<{column_widths['endpoint']}}  \"\n+        f\"{'Endpoint':<{column_widths['endpoint']}}  \" # Adjust spacing if needed\n         f\"{'Methods':<{column_widths['methods']}}  \"\n         f\"{'Rule':<{column_widths['rule']}}\"\n     )\n     click.echo(header)\n     click.echo(\n         f\"{'-' * column_widths['domain']}  \"\n-        f\"{'-' * column_widths['endpoint']}  \"\n+        f\"{'-' * column_widths['endpoint']}  \" # Adjust spacing if needed\n         f\"{'-' * column_widths['methods']}  \"\n         f\"{'-' * column_widths['rule']}\"\n     )\n \n     for row in rows:\n         click.echo(\n+            f\"{row['domain']:<{column_widths['domain']}}  \"\n             f\"{row['endpoint']:<{column_widths['endpoint']}}  \"\n             f\"{row['methods']:<{column_widths['methods']}}  \"\n             f\"{row['rule']:<{column_widths['rule']}}\"\n```\n"}
{"instance_id": "psf__requests-1963", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "psf__requests-2148", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -628,7 +628,7 @@\n                 try:\n                     for chunk in self.raw.stream(chunk_size, decode_content=True):\n                         yield chunk\n-                except ProtocolError as e:\n+                except ProtocolError as e: # This includes non-chunking related ProtocolErrors like Connection Reset\n                     raise ConnectionError(e)\n                 except DecodeError as e:\n                     raise ContentDecodingError(e)\n\n```\n"}
{"instance_id": "psf__requests-2317", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "psf__requests-2674", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -36,12 +36,19 @@\n from .compat import urlparse, basestring, urldefrag, unquote\n from ._internal_utils import to_native_string\n from .exceptions import (ConnectionError, ConnectTimeout, ReadTimeout, SSLError,\n-                         ProxyError, RetryError, InvalidSchema, InvalidURL)\n+                         ProxyError, RetryError, InvalidSchema, InvalidURL, Timeout,\n+                         ContentDecodingError, RequestException)\n from .structures import CaseInsensitiveDict\n from .utils import (get_encoding_from_headers, prepend_scheme_if_needed,\n                     get_auth_from_url, urldata, select_proxy)\n from .cookies import extract_cookies_to_jar\n from .packages.urllib3.exceptions import (\n+    ConnectTimeoutError,\n+    ReadTimeoutError,\n+    NewConnectionError,\n+    ProtocolError as _ProtocolError,\n+    DecodeError as _DecodeError,\n+    TimeoutError as _TimeoutError,\n     MaxRetryError,\n     SSLError as _SSLError,\n     ProxyError as _ProxyError,\n@@ -450,6 +457,10 @@\n             raise ConnectionError(err, request=request)\n \n         except SubjectAltNameWarning as err:\n+            # In Python 3, this warning is caught as SSLError. Read more:\n+            # https://github.com/shazow/urllib3/issues/1084\n+            pass\n+\n         except (BaseSSLError, CertificateError) as err:\n             # Handle specific SSL errors separately from other socket errors\n             if isinstance(err, CertificateError):\n@@ -464,6 +475,10 @@\n             # Catch read timeouts that occur during the initial connection\n             # attempt / request sending.\n             raise ReadTimeout(err, request=request)\n+\n+        # Catch specific urllib3 ReadTimeoutErrors first.\n+        except ReadTimeoutError as err:\n+            raise ReadTimeout(err, request=request)\n \n         except MaxRetryError as e:\n             if isinstance(e.reason, ConnectTimeoutError):\n@@ -487,13 +502,23 @@\n         except _ProxyError as e:\n             raise ProxyError(e)\n \n-        except (_SSLError, _HTTPError) as e:\n+        # Catch other urllib3 errors\n+        except (_SSLError,) as e:\n+            # Catch remaining SSLErrors\n             if isinstance(e, _SSLError):\n                 raise SSLError(e, request=request)\n-            elif isinstance(e, ReadTimeoutError):\n-                # This branch is hopefully not reachable after adding specific ReadTimeoutError catch\n-                raise ReadTimeout(e, request=request)\n             else:\n-                pass\n+                # This branch should not be hit, but as a safety measure raise generic RequestException.\n+                raise RequestException(e, request=request)\n+\n+        except _ProtocolError as e:\n+            raise ConnectionError(e, request=request)\n+\n+        except _DecodeError as e:\n+            raise ContentDecodingError(e, request=request)\n+\n+        except _TimeoutError as e: # Fallback for base TimeoutError\n+            raise Timeout(e, request=request)\n \n         except _HTTPError as e:\n+            # Catch any remaining urllib3 HTTPError types\n             raise RequestException(e, request=request)\n \n         return self.build_response(request, resp)\n--- a/requests/exceptions.py\n+++ b/requests/exceptions.py\n@@ -9,7 +9,7 @@\n This module contains the set of Requests' exceptions.\n \n \"\"\"\n-from .packages.urllib3.exceptions import HTTPError as BaseHTTPError\n+from .packages.urllib3.exceptions import DecodeError as _DecodeError, HTTPError as BaseHTTPError\n \n \n class RequestException(IOError):\n@@ -80,7 +80,7 @@\n     \"\"\"The server declared chunked encoding but sent an invalid chunk.\"\"\"\n \n \n-class ContentDecodingError(RequestException):\n+class ContentDecodingError(RequestException, BaseHTTPError):\n     \"\"\"Failed to decode response content\"\"\"\n \n \n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -1,6 +1,10 @@\n # -*- coding: utf-8 -*-\n \n from .packages.urllib3.exceptions import (\n+    DecodeError as Urllib3DecodeError,\n+    ReadTimeoutError as Urllib3ReadTimeoutError,\n+    ProtocolError as Urllib3ProtocolError,\n+)\n from .exceptions import (\n     RequestException, ConnectionError, HTTPError, StreamConsumedError,\n     ContentDecodingError, ChunkedEncodingError, RetryError, UnrewindableBodyError,\n@@ -751,10 +755,10 @@\n                 # Content-Length is defined. This check can be removed once\n                 # https://github.com/shazow/urllib3/issues/1084 is resolved.\n                 pass\n-            except (ProtocolError, DecodeError, ReadTimeoutError, AttributeError) as e:\n-                # Don't test for AttributeError because it's a bug in pyOpenSSL\n-                # See: https://github.com/shazow/urllib3/issues/1099\n+            except Urllib3DecodeError as e:\n+                raise ContentDecodingError(e)\n+            except (Urllib3ProtocolError, AttributeError) as e:\n                 raise ChunkedEncodingError(e)\n-            except (_ConnectionError, _SocketError) as e:\n+            except (_ConnectionError, _SocketError, Urllib3ReadTimeoutError) as e:\n                 # Catch ReadTimeoutError separately because it does not inherit from SocketError.\n                 raise ConnectionError(e)\n```\n"}
{"instance_id": "psf__requests-3362", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -38,18 +38,20 @@\n def stream_decode_response_unicode(iterator, r):\n     \"\"\"Stream decodes a iterator.\"\"\"\n \n-    if r.encoding is None:\n+    encoding = r.encoding\n+\n+    if encoding is None:\n+        # Fallback to apparent encoding if no encoding is specified.\n+        encoding = r.apparent_encoding\n+\n+    # If no encoding could be determined (explicitly or apparent), yield the original bytes.\n+    if encoding is None:\n         for item in iterator:\n             yield item\n         return\n \n-    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n+    decoder = codecs.getincrementaldecoder(encoding)(errors='replace')\n     for chunk in iterator:\n         rv = decoder.decode(chunk)\n         if rv:\n             yield rv\n     rv = decoder.decode(b'', final=True)\n     if rv:\n         yield rv\n```\n"}
{"instance_id": "psf__requests-863", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -1,5 +1,6 @@\n # -*- coding: utf-8 -*-\n \n+from collections import defaultdict\n import socket\n \n from .packages.urllib3.response import HTTPResponse\n@@ -77,7 +78,16 @@\n         self.proxies = proxies or {}\n \n         #: Dictionary of callback hooks, for internal usage.\n-        self.hooks = hooks\n+        self.hooks = defaultdict(list)\n+        hooks = dict(hooks or {}) # Ensure hooks is a dict\n+\n+        for event, hook in hooks.items():\n+            if hasattr(hook, '__call__'):\n+                # Single hook\n+                self.hooks[event].append(hook)\n+            else:\n+                # Iterable of hooks\n+                self.hooks[event].extend(hook)\n \n         #: Configuration dictionary, for internal usage.\n         self.config = dict(config or [])\n```\n"}
{"instance_id": "pydata__xarray-3364", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -50,6 +50,10 @@\n             already loaded.\n           * 'all': All data variables will be concatenated.\n           * list of str: The listed data variables will be concatenated, in\n+            addition to the 'minimal' data variables.\n+\n+        The default behaviour is 'minimal', but note that coordinates may also be\n+        selected using the `coords` argument.\n     coords : {'minimal', 'different', 'all' or list of str}, optional\n         These coordinate variables will be concatenated together:\n           * 'minimal': Only coordinates in which the dimension already appears\n@@ -64,10 +68,20 @@\n             addition to the 'minimal' coordinates.\n         The default behaviour is 'different'. 'minimal' is behaviour prior to\n         v0.15.0.\n+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional\n+        String indicating how to compare variables of the same name for\n+        potential conflicts when merging. This is applied to variables that\n+        are not concatenated along the dimension `dim`.\n+          - 'broadcast_equals': All values must be equal when variables are\n+            broadcast against each other to ensure common dimensions.\n+          - 'equals': All values and dimensions must be the same.\n+          - 'identical': All values, dimensions and attributes must be the\n+            same.\n+          - 'no_conflicts': Only dimensions must be the same. Variables with\n+            the same name are skipped unless they are coordinates (see `coords`).\n+          - 'override': Skip comparing and pick variable from first dataset.\n     positions : None or list of integer arrays, optional\n         List of integer arrays which specifies the integer positions to which\n         to assign each dataset along the concatenated dimension. If not\n-        supplied, datasets are concatenated in the provided order.\n     fill_value : scalar, optional\n         Value to use for newly missing values\n     join : {'outer', 'inner', 'left', 'right', 'exact'}, optional\n@@ -79,8 +93,12 @@\n         - 'left': use indexes from the first object with each dimension\n         - 'right': use indexes from the last object with each dimension\n         - 'exact': instead of aligning, raise `ValueError` when indexes to be\n-          aligned are not equal.\n+          aligned are not equal. Requires all objects to have the same indexes\n+          (excluding `dim`).\n \n+        This argument also controls how variables not present in all datasets are\n+        handled:\n+          - 'outer': Includes the union of variables from all datasets. Variables missing in some datasets will be filled with `fill_value`. (Default)\n+          - 'inner': Includes only variables present in all datasets.\n+          - 'exact': Requires all datasets to have the same variables.\n     Returns\n     -------\n     combined : xarray.Dataset or xarray.DataArray\n@@ -240,16 +258,13 @@\n \n     # concatenate variables\n     for k in concat_over:\n-        # Concatenate variables along the new dimension\n-        # N.B. ensure_common_dims cannot handle the case where the dimension is\n-        # present in some variables but not others. This is handled by the logic\n-        # above.\n-        try:\n-            vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n-        except KeyError:\n-            raise ValueError(\"%r is not present in all datasets.\" % k)\n+        # Ensure variables have matching dimensions and coordinate values,\n+        # except along the concatenated dimension. This is done by the align\n+        # call before _dataset_concat is called.\n+        # If a variable is missing in some datasets (join='outer'), align pads\n+        # the variable with NaN/fill_value, so it should always be present here.\n+        vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n         combined = concat_vars(vars, dim, positions)\n         assert isinstance(combined, Variable)\n         result_vars[k] = combined\n         if k in result_coord_names:\n```\n"}
{"instance_id": "pydata__xarray-4094", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/xarray/core/dataarray.py\n+++ b/xarray/core/dataarray.py\n@@ -1850,10 +1850,11 @@\n \n     def to_unstacked_dataset(\n         self, dim: Hashable, level: int = 0, name: Hashable = None\n-    ) -> \"Dataset\":\n+    ) -> \"Dataset\":\n         \"\"\"Unstack a DataArray along a dimension with a MultiIndex coordinate\n         to produce a Dataset.\n \n+        The specified dimension must have a MultiIndex coordinate. The specified\n         The specified dimension must have a MultiIndex coordinate. The specified\n         level of the MultiIndex provides the names of the DataArrays in the\n         new Dataset.\n@@ -1869,34 +1870,42 @@\n         Returns\n         -------\n         Dataset\n+\n+        See Also\n+        --------\n+        Dataset.to_stacked_array\n         \"\"\"\n-        if name is None:\n-            name = self.name\n-\n         index = self.get_index(dim)\n         if not isinstance(index, pd.MultiIndex):\n             raise ValueError(f\"coordinate {dim!r} is not a MultiIndex\")\n \n-        level_coord = index.get_level_values(level)\n-        unique_coord_values = pd.unique(level_coord)\n+        # Determine the name of the level to use for variable names\n+        if isinstance(level, int):\n+            if level >= index.nlevels:\n+                raise ValueError(f\"Level {level} exceeds the number of levels in MultiIndex {dim!r}\")\n+            level_name = index.names[level]\n+        else:\n+            level_name = level\n+\n+        if level_name not in index.names:\n+             raise ValueError(f\"Level {level_name!r} not found in MultiIndex {dim!r}\")\n+\n+        if level_name is None:\n+            # pandas MultiIndex level names can be None.\n+            # This conflicts with using it as a dimension name after unstacking.\n+            # Also, variable names in a Dataset cannot be None.\n+            raise ValueError(\n+                f\"The specified level {level} of the MultiIndex {dim!r} must have a name.\"\n+            )\n+\n+        # Unstack the specified dimension\n+        unstacked_array = self.unstack(dim)\n+\n+        # The dimension corresponding to the variable names is now `level_name`\n+        variable_dim = level_name\n \n         datasets = []\n-        for value in unique_coord_values:\n-            # Select data for the current variable name\n-            subset = self.sel({dim: level_coord == value})\n-\n-            # Create a new DataArray for this variable\n-            # Remove the unstacked dimension and its coordinate\n-            subset = subset.drop_vars(dim)\n-\n-            # Rename the DataArray if a name was provided\n-            if name is not None:\n-                subset.name = name\n-\n-            # Create a dataset from this single DataArray\n-            # The name of the DataArray becomes the variable name in the dataset\n-            ds = subset.to_dataset(name=value)\n+        for value in unstacked_array.coords[variable_dim].values:\n+            # Select the slice corresponding to the current variable name\n+            # drop=True drops the scalar coordinate variable_dim\n+            subset = unstacked_array.sel({variable_dim: value}, drop=True)\n+\n+            # Create a dataset for this variable\n+            ds = xr.Dataset({value: subset})\n             datasets.append(ds)\n \n         # Merge the datasets for each variable\n-        merged = merge(datasets, compat=\"override\")\n+        # Rely on automatic alignment based on shared coordinates.\n+        merged = merge(datasets)\n \n         return merged\n \n```\n"}
{"instance_id": "pydata__xarray-4248", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/xarray/core/formatting.py\n+++ b/xarray/core/formatting.py\n@@ -1,5 +1,6 @@\n import contextlib\n import functools\n+import io\n import itertools\n import math\n import textwrap\n@@ -7,13 +8,17 @@\n from typing import (\n     TYPE_CHECKING,\n     Hashable,\n+    Iterable,\n     Mapping,\n     Optional,\n     Sequence,\n+    TypeVar,\n     Union,\n )\n \n import numpy as np\n+\n+from . import formatting_html\n \n from .options import OPTIONS, set_options\n from .pycompat import dask_array_type\n@@ -21,6 +26,11 @@\n \n # Used for DataArray, Variable and Dataset methods; not for top-level functions\n T = TypeVar(\"T\")\n+\n+if TYPE_CHECKING:\n+    from .dataarray import DataArray\n+    from .dataset import Dataset\n+    from .variable import Variable\n \n \n @contextlib.contextmanager\n@@ -200,28 +210,34 @@\n     return summary\n \n \n-def summarize_coord(name: Hashable, var: Variable, col_width: int) -> list[str]:\n+def summarize_coord(\n+    name: Hashable, var: Union[\"Variable\", \"DataArray\"], col_width: int\n+) -> list[str]:\n     \"\"\"Summarize a single Coordinate for display.\"\"\"\n     is_index = name in var.dims\n     marker = \"*\" if is_index else \" \"\n+    # If var is a DataArray, operate on its underlying Variable\n+    real_var = var.variable if hasattr(var, \"variable\") else var\n \n-    first_col = f\"{marker} {name} ({', '.join(var.dims)})\"\n+    units = real_var.attrs.get(\"units\")\n+    attrs_to_summarize = real_var.attrs.copy()\n+    if units is not None:\n+        name_str = f\"{name}, in {units}\"\n+        attrs_to_summarize.pop(\"units\", None)  # Remove units if present\n+    else:\n+        name_str = f\"{name}\"\n+\n+    dims_str = f\"({', '.join(real_var.dims)})\" if real_var.dims else \"\"\n+    first_col = f\"{marker} {name_str} {dims_str}\"\n     if len(first_col) > col_width:\n         first_col = f\"{first_col[:col_width - 3]}...\"\n-    elif col_width is not None:\n-        first_col = first_col.ljust(col_width)\n-\n-    attrs_summary = summarize_attrs(var.attrs)\n+    first_col = first_col.ljust(col_width)\n \n-    dtype_str = dtype_repr(var.dtype)\n-    preview = summarize_array(var.variable if isinstance(var, DataArray) else var)\n+    dtype_str = dtype_repr(real_var.dtype)\n+    preview = summarize_array(real_var)\n \n     summary = [f\"{first_col} {dtype_str} {preview}\"]\n \n-    if attrs_summary:\n-        summary += [f\"    {attrs_summary}\"]\n+    if attrs_to_summarize:\n+        attrs_summary = summarize_attrs(attrs_to_summarize)\n+        if attrs_summary:  # Only add line if attributes string is non-empty\n+            summary += [f\"    {attrs_summary}\"]\n     return summary\n \n \n-def summarize_datavar(name: Hashable, var: Variable, col_width: int) -> list[str]:\n+def summarize_datavar(\n+    name: Hashable, var: Union[\"Variable\", \"DataArray\"], col_width: int\n+) -> list[str]:\n     \"\"\"Summarize a single DataArray for display.\"\"\"\n-    first_col = f\"  {name} ({', '.join(var.dims)})\"\n+    # If var is a DataArray, operate on its underlying Variable\n+    real_var = var.variable if hasattr(var, \"variable\") else var\n+\n+    units = real_var.attrs.get(\"units\")\n+    attrs_to_summarize = real_var.attrs.copy()\n+    if units is not None:\n+        name_str = f\"{name}, in {units}\"\n+        attrs_to_summarize.pop(\"units\", None)  # Remove units if present\n+    else:\n+        name_str = f\"{name}\"\n+\n+    dims_str = f\"({', '.join(real_var.dims)})\" if real_var.dims else \"\"\n+    first_col = f\"  {name_str} {dims_str}\"\n     if len(first_col) > col_width:\n         first_col = f\"{first_col[:col_width - 3]}...\"\n-    elif col_width is not None:\n-        first_col = first_col.ljust(col_width)\n-\n-    attrs_summary = summarize_attrs(var.attrs)\n+    first_col = first_col.ljust(col_width)\n \n-    dtype_str = dtype_repr(var.dtype)\n-    preview = summarize_array(var.variable if isinstance(var, DataArray) else var)\n+    dtype_str = dtype_repr(real_var.dtype)\n+    preview = summarize_array(real_var)\n \n     summary = [f\"{first_col} {dtype_str} {preview}\"]\n \n-    if attrs_summary:\n-        summary += [f\"    {attrs_summary}\"]\n+    if attrs_to_summarize:\n+        attrs_summary = summarize_attrs(attrs_to_summarize)\n+        if attrs_summary:  # Only add line if attributes string is non-empty\n+            summary += [f\"    {attrs_summary}\"]\n     return summary\n \n \n@@ -270,7 +286,7 @@\n     )\n \n     coord_lines = []\n-    for name, var in coords.items():\n+    for name, var in sorted(coords.items(), key=first_n_items(1)):\n         coord_lines.extend(summarize_coord(name, var, col_width))\n \n     var_lines = []\n```\n"}
{"instance_id": "pydata__xarray-4493", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/xarray/core/variable.py\n+++ b/xarray/core/variable.py\n@@ -257,14 +257,23 @@\n \n     if isinstance(obj, tuple):\n         try:\n-            data = obj[1]\n-            # If the data is already a DataArray, attempt to possibly take its name.\n+            # unpack tuple arguments\n+            dims = obj[0]\n+            data = obj[1]\n+            attrs = obj[2] if len(obj) > 2 else None\n+            encoding = obj[3] if len(obj) > 3 else None\n+\n+            if isinstance(data, DataArray):\n+                # If data is a DataArray, extract its Variable and update metadata\n             # This allows writing ``xr.Dataset({'a': ('x', da.a)})``\n-            if isinstance(data, DataArray):\n-                if name is None:\n-                    name = data.name\n-            obj = Variable(*obj)\n+                variable = data.variable.copy(deep=False)\n+                variable._dims = Variable._parse_dimensions(dims)\n+                if attrs is not None:\n+                    variable.attrs = attrs\n+                if encoding is not None:\n+                    variable.encoding = encoding\n+                variable._validate_dimensions()\n+                obj = variable\n+            else:\n+                # Otherwise, create a new Variable object\n+                obj = Variable(dims, data, attrs, encoding)\n         except (TypeError, IndexError):\n             raise ValueError(\n                 \"cannot convert tuple to variable unless it is of \"\n```\n"}
{"instance_id": "pydata__xarray-5131", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/xarray/core/groupby.py\n+++ b/xarray/core/groupby.py\n@@ -301,7 +301,7 @@\n \n     def __repr__(self) -> str:\n         return \"{}, grouped over {!r} \\n{}\".format(\n-            self.__class__.__name__, self._unique_coord.name, self.dims_str\n+            self.__class__.__name__, self._unique_coord.name, self.dims_str.strip()\n         )\n \n     def map(\n```\n"}
{"instance_id": "pylint-dev__pylint-5859", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/pylint/checkers/misc.py\n+++ b/pylint/checkers/misc.py\n@@ -41,9 +41,9 @@\n \n         notes = \"|\".join(re.escape(note) for note in self.config.notes)\n         if self.config.notes_rgx:\n-            regex_string = rf\"#\\s*({notes}|{self.config.notes_rgx})\\b\"\n+            regex_string = rf\"#\\s*({notes}|{self.config.notes_rgx})\"\n         else:\n-            regex_string = rf\"#\\s*({notes})\\b\"\n+            regex_string = rf\"#\\s*({notes})\"\n \n         self._fixme_pattern = re.compile(regex_string, re.I)\n \n```\n"}
{"instance_id": "pylint-dev__pylint-6506", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/pylint/config/config_initialization.py\n+++ b/pylint/config/config_initialization.py\n@@ -4,7 +4,7 @@\n import sys\n from typing import Any\n \n-from pylint.config.exceptions import _UnrecognizedOptionError\n+from pylint.config.exceptions import ConfigurationError, _UnrecognizedOptionError\n from pylint.config.find_default_config_files import find_default_config_files\n from pylint.config.toml_parser import parse_toml_file\n from pylint.constants import MAIN_CHECKER_NAME\n@@ -82,7 +82,7 @@\n     if unrecognized_options:\n         msg = \", \".join(unrecognized_options)\n         linter.add_message(\"unrecognized-option\", line=0, args=msg)\n-        raise _UnrecognizedOptionError(options=unrecognized_options)\n+        raise _UnrecognizedOptionError(options=unrecognized_options)  # Raise to exit early\n \n     # Set the current module to configuration as we don't know where\n     # the --load-plugins key is coming from\n--- a/pylint/lint/run.py\n+++ b/pylint/lint/run.py\n@@ -11,7 +11,10 @@\n from pylint import config, reporters\n from pylint.config.arguments_manager import _ArgumentsManager\n from pylint.config.config_initialization import _config_initialization\n-from pylint.config.find_default_config_files import find_pylintrc\n+from pylint.config.exceptions import _UnrecognizedOptionError\n+from pylint.config.find_default_config_files import (\n+    find_default_config_files,\n+)\n from pylint.constants import (\n     DEFAULT_PYLINT_HOME,\n     MAIN_CHECKER_NAME,\n@@ -129,13 +132,16 @@\n         # Load command line arguments\n         # pylint: disable=protected-access\n         try:\n-            args = _config_initialization(\n-                linter=self.linter,\n-                config_file=config_file,\n-                config_file_options=config_file_options,\n-                command_line_options=command_line_options,\n-                args_list=args_list,\n-                reporter=reporter,\n+            args = _config_initialization(  # type: ignore[assignment]\n+                linter=self.linter,  # type: ignore[arg-type]\n+                config_file=config_file,  # type: ignore[arg-type]\n+                config_file_options=config_file_options,  # type: ignore[arg-type]\n+                command_line_options=command_line_options,  # type: ignore[arg-type]\n+                args_list=args_list,  # type: ignore[arg-type]\n+                reporter=reporter,  # type: ignore[arg-type]\n+            )\n+        except _UnrecognizedOptionError:\n+            sys.exit(32)  # Stop Pylint execution and exit with code 32\n \n         # If the command line arguments are valid, then walk the arguments list\n         # to discover python modules and packages candidates to lint.\n```\n"}
{"instance_id": "pylint-dev__pylint-7080", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/pylint/utils/__init__.py\n+++ b/pylint/utils/__init__.py\n@@ -106,7 +106,9 @@\n         # Filter based on ignore list / patterns\n         for root, dirs, files in os.walk(directory, topdown=True):\n             # Use file_state object to check ignores\n-            if file_state._is_ignored(root, ignore_list, ignore_list_re):\n+            if file_state._is_ignored(  # Pass all ignore lists/patterns\n+                root, ignore_list, ignore_list_re, ignore_list_paths_re\n+            ):\n                 skip_subtrees[:] = dirs  # Modify skip_subtrees directly\n                 continue\n \n```\n"}
{"instance_id": "pylint-dev__pylint-7114", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/pylint/lint/expand_modules.py\n+++ b/pylint/lint/expand_modules.py\n@@ -9,12 +9,14 @@\n                 modname = \".\".join(\n                     modutils.modpath_from_file(something, path=additional_search_path)\n                 )\n-            except ImportError:\n+            except (ImportError, IndexError):  # modpath_from_file can raise IndexError\n                 modname = os.path.splitext(basename)[0]\n+            _is_arg_directory = os.path.isdir(something) # Store if the original arg was a directory\n             if os.path.isdir(something):\n                 filepath = os.path.join(something, \"__init__.py\")\n             else:\n                 filepath = something\n+\n         else:\n             # suppose it's a module or package\n             modname = something\n@@ -27,10 +29,13 @@\n                 # The SyntaxError is a Python bug and should be\n                 # removed once we move away from imp.find_module: https://bugs.python.org/issue10588\n                 errors.append({\"key\": \"fatal\", \"mod\": modname, \"ex\": ex})\n+                _is_arg_directory = False # Not a directory argument if lookup failed\n                 continue\n+            _is_arg_directory = os.path.isdir(filepath) # Check if resolved path is directory\n\n         filepath = os.path.normpath(filepath)\n-        modparts = (modname or something).split(\".\")\n+        effective_modname = modname or os.path.splitext(basename)[0]\n+        modparts = effective_modname.split(\".\")\n         try:\n             spec = modutils.file_info_from_modpath(\n                 modparts, path=additional_search_path\n@@ -43,15 +48,21 @@\n         else:\n             is_namespace = modutils.is_namespace(spec)\n             is_directory = modutils.is_directory(spec)\n-        if not is_namespace:\n+\n+        # If the original argument was a directory, but we ended up with a filepath\n+        # pointing to a non-existent __init__.py, we should skip adding it.\n+        # The directory content will be discovered later by other mechanisms.\n+        if _is_arg_directory and not os.path.exists(filepath):\n+            continue\n+\n+        # Add the module/file to the list, unless it's a namespace package\n+        # without a concrete __init__.py file path (though the check above handles this).\n+        if not is_namespace or os.path.isfile(filepath):\n             result.append(\n                 {\n                     \"path\": filepath,\n-                    \"name\": modname,\n+                    \"name\": effective_modname,\n                     \"isarg\": True,\n-                    \"basepath\": filepath,\n+                    \"basepath\": os.path.dirname(filepath), # Directory containing the file/package\n                     \"basename\": os.path.basename(filepath),\n                     \"modpath\": modparts,\n                     \"is_directory\": is_directory,\n```\n"}
{"instance_id": "pylint-dev__pylint-7228", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/pylint/config/utils.py\n+++ b/pylint/config/utils.py\n@@ -6,7 +6,6 @@\n import argparse\n import configparser\n import os\n-import re\n import shlex\n from collections.abc import Sequence\n from pathlib import Path\n@@ -102,7 +101,9 @@\n     elif opt_type == \"yn\":\n         kwargs[\"type\"] = _csv_validator(yn_validator)\n         kwargs[\"metavar\"] = \"<y or n>\"\n-    elif opt_type == \"regexp\":\n-        kwargs[\"type\"] = re.compile\n+    elif opt_type == \"regexp\":\n+        # Do not use re.compile here as it does not support all features\n+        # that the regex module supports, like \\p{...}\n+        kwargs[\"type\"] = str\n         kwargs[\"metavar\"] = \"<regexp>\"\n     elif opt_type == \"csv\":\n         kwargs[\"type\"] = _csv\n```\n"}
{"instance_id": "pylint-dev__pylint-7993", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/pylint/reporters/text.py\n+++ b/pylint/reporters/text.py\n@@ -1,10 +1,10 @@\n # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\n # For details: https://github.com/PyCQA/pylint/blob/main/LICENSE\n \n-import re\n import sys\n+import string\n import warnings\n-from dataclasses import asdict\n+from dataclasses import asdict, fields\n from io import TextIOWrapper\n from typing import TYPE_CHECKING, Optional, TextIO\n \n@@ -68,22 +68,37 @@\n         if template == self._template:\n             return\n \n-        # Set template to the currently selected template\n+        # Store the user-provided template\n         self._template = template\n \n-        # Check to see if all parameters in the template are attributes of the Message\n-        arguments = re.findall(r\"\\{(.+?)(:.*)?\\}\", template)\n-        for argument in arguments:\n-            if argument[0] not in MESSAGE_FIELDS:\n+        # Check to see if all parameters in the template are attributes of the Message.\n+        # Use string.Formatter().parse to correctly handle literal braces {{ }}\n+        formatter = string.Formatter()\n+        fixed_template_parts = []\n+        has_invalid_args = False\n+\n+        try:\n+            for literal_text, field_name, format_spec, conversion in formatter.parse(\n+                template\n+            ):\n+                fixed_template_parts.append(literal_text)\n+                if field_name is not None:\n+                    if field_name not in MESSAGE_FIELDS:\n+                        warnings.warn(\n+                            f\"Don't recognize the argument '{field_name}' in the --msg-template. \"\n+                            \"Are you sure it is supported on the current version of pylint?\"\n+                        )\n+                        has_invalid_args = True\n+                        # Skip adding this field to the fixed template\n+                    else:\n+                        # Reconstruct the valid field part\n+                        field_part = \"{\" + field_name\n+                        if conversion:\n+                            field_part += \"!\" + conversion\n+                        if format_spec:\n+                            field_part += \":\" + format_spec\n+                        field_part += \"}\"\n+                        fixed_template_parts.append(field_part)\n+        except ValueError as e:\n+            # Handle potential errors during parsing, e.g., unmatched braces\n+            warnings.warn(f\"Failed to parse --msg-template: '{template}'. Error: {e}\")\n+            # Fallback: keep the original template but issue a warning.\n+            self._fixed_template = template\n+            return\n+\n+        self._fixed_template = \"\".join(fixed_template_parts)\n-                warnings.warn(\n-                    f\"Don't recognize the argument '{argument[0]}' in the --msg-template. \"\n-                    \"Are you sure it is supported on the current version of pylint?\"\n-                )\n-                template = re.sub(r\"\\{\" + argument[0] + r\"(:.*?)?\\}\", \"\", template)\n-        self._fixed_template = template\n \n     def write_message(self, msg: Message) -> None:\n         \"\"\"Convenience method to write a formatted message with class default\n```\n"}
{"instance_id": "pytest-dev__pytest-11143", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -757,7 +757,8 @@\n         return False\n\n     def is_rewrite_disabled(self, docstring: Optional[str]) -> bool:\n-        return \"PYTEST_DONT_REWRITE\" in docstring\n+        # Check if docstring is a string, otherwise it cannot contain the marker.\n+        return isinstance(docstring, str) and \"PYTEST_DONT_REWRITE\" in docstring\n\n     # Implementation details.\n\n```\n"}
{"instance_id": "pytest-dev__pytest-11148", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/_py/path.py\n+++ b/src/_pytest/_py/path.py\n@@ -14,12 +14,16 @@\n          if modname is None:\n              modname = self.purebasename\n+         # Check sys.modules before trying to load module direct from path.\n+         # Primarily useful for importlib mode to avoid importing a module\n+         # twice due to sys.path modifications or namespace packages.\n+         if modname in sys.modules:\n+             return sys.modules[modname]\n          spec = importlib.util.spec_from_file_location(modname, str(self))\n          if spec is None or spec.loader is None:\n              raise ImportError(\n                  f\"Can't find module {modname} at location {str(self)}\"\n              )\n          mod = importlib.util.module_from_spec(spec)\n+         # Cache the module in sys.modules before executing it to prevent\n+         # recursive imports from creating a new instance.\n+         # See https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n+         sys.modules[modname] = mod\n          spec.loader.exec_module(mod)\n          return mod\n\n```\n"}
{"instance_id": "pytest-dev__pytest-5103", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "pytest-dev__pytest-5221", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/python.py\n+++ b/src/_pytest/python.py\n@@ -49,6 +49,7 @@\n                     curdir.bestrelpath(loc),\n                     fixturedef.argname,\n                     fixturedef,\n+                    fixturedef.scope,\n                 )\n             )\n \n@@ -61,7 +62,7 @@\n         tw.sep(\"-\", \"fixtures\")\n         if not available:\n             tw.line(\"no fixtures discovered\")\n-            return 0\n+            return\n \n         available.sort()\n         current_fixture_type = None\n@@ -70,7 +71,7 @@\n         fixture_doc = {}\n         fixture_loc = {}\n \n-        for baseid, module, bestrel, argname, fixturedef in available:\n+        for baseid, module, bestrel, argname, fixturedef, scope in available:\n             if current_fixture_type != bestrel:\n                 if not verbose:\n                     tw.line()\n@@ -82,7 +83,7 @@\n                 fixture_doc[argname] = fixturedef.func.__doc__\n                 fixture_loc[argname] = bestrel\n             else:\n-                msg = \"{} (defined in {})\".format(argname, fixture_loc[argname])\n+                msg = \"{} [{}] (defined in {})\".format(argname, scope, fixture_loc[argname])\n                 tw.line(msg)\n                 if verbose > 1:\n                     doc = fixturedef.func.__doc__\n@@ -91,7 +92,7 @@\n                         tw.line(\"    {}\".format(line))\n                     tw.line()\n             else:\n-                tw.write(argname)\n+                tw.write(\"{} [{}]\".format(argname, scope))\n                 doc = fixturedef.func.__doc__\n                 if doc:\n                     lines = doc.splitlines()\n```\n"}
{"instance_id": "pytest-dev__pytest-5227", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/logging.py\n+++ b/src/_pytest/logging.py\n@@ -17,7 +17,7 @@\n \n # Default formats\n DEFAULT_LOG_DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n-DEFAULT_LOG_FORMAT = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n+DEFAULT_LOG_FORMAT = \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"\n \n # Log levels\n PADDING = 21\n```\n"}
{"instance_id": "pytest-dev__pytest-5413", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/_code/__init__.py\n+++ b/src/_pytest/_code/__init__.py\n@@ -603,12 +603,18 @@\n             self._tblen = len(self.traceback)\n \n     def __str__(self) -> str:\n+        \"\"\"Return str(self.value)\"\"\"\n+        if self.value is None:\n+            # Handle the case from for_later() before __exit__ is called\n+            # or if __init__ fails early.\n+            # Use the placeholder string set in for_later() or a default.\n+            return getattr(self, \"_str\", \"<ExceptionInfo without value>\")\n         try:\n-            s = self._getreprcrash()\n+            return str(self.value)\n         except Exception:\n-            s = \"<[ExceptionInfo() failed for {!r}]>\".format(object.__repr__(self))\n-        return s\n-\n+            # Fallback if str(self.value) fails for some reason\n+            return \"<[ExceptionInfo(__str__) failed for {!r}]>\".format(\n+                object.__repr__(self.value)\n+            )\n \n     def _getreprcrash(self) -> str:\n         \"\"\"Return a representation of the exception info for printing when\n```\n"}
{"instance_id": "pytest-dev__pytest-5495", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/assertion/util.py\n+++ b/src/_pytest/assertion/util.py\n@@ -313,18 +313,26 @@\n \n     # Generate explanations for extra items.\n     if len_left > len_right:\n+        first_extra_item = left[len_min]\n+        if isinstance(left, bytes):\n+            formatted_item = f\"{repr(bytes([first_extra_item]))} ({first_extra_item})\"\n+        else:\n+            formatted_item = saferepr(first_extra_item)\n         explanation.extend(\n             [\n-                f\"Left contains more items, first extra item: {saferepr(left[len_min])}\",\n+                f\"Left contains more items, first extra item: {formatted_item}\",\n                 f\"Use -v to get the full diff\",\n             ]\n         )\n     elif len_right > len_left:\n+        first_extra_item = right[len_min]\n+        if isinstance(right, bytes):\n+            formatted_item = f\"{repr(bytes([first_extra_item]))} ({first_extra_item})\"\n+        else:\n+            formatted_item = saferepr(first_extra_item)\n         explanation.extend(\n             [\n-                f\"Right contains more items, first extra item: {saferepr(right[len_min])}\",\n+                f\"Right contains more items, first extra item: {formatted_item}\",\n                 f\"Use -v to get the full diff\",\n             ]\n         )\n```\n"}
{"instance_id": "pytest-dev__pytest-5692", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/junitxml.py\n+++ b/src/_pytest/junitxml.py\n@@ -1,8 +1,10 @@\n import functools\n import os\n import sys\n+import socket\n import time\n from io import StringIO\n+import datetime\n \n import pytest\n from _pytest.config import filename_arg\n@@ -108,6 +110,7 @@\n \n     def pytest_sessionstart(self):\n         self.suite_start_time = time.time()\n+        self.ts = datetime.datetime.fromtimestamp(self.suite_start_time).isoformat(timespec=\"seconds\")\n \n     def pytest_sessionfinish(self):\n         dirname = os.path.dirname(os.path.abspath(self.logfile))\n@@ -128,6 +131,8 @@\n         )\n         logfile.write('<?xml version=\"1.0\" encoding=\"utf-8\"?>')\n \n+        hostname = socket.gethostname()\n+\n         suite_node = Junit.testsuite(\n             self._get_global_properties_node(),\n             [x.to_xml() for x in self.node_reporters_ordered],\n@@ -137,6 +142,8 @@\n             skipped=self.stats[\"skipped\"],\n             tests=numtests,\n             time=\"%.3f\" % suite_time_delta,\n+            hostname=hostname,\n+            timestamp=self.ts,\n         )\n         logfile.write(Junit.testsuites([suite_node]).unicode(indent=0))\n         logfile.close()\n```\n"}
{"instance_id": "pytest-dev__pytest-6116", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/config/argparsing.py\n+++ b/src/_pytest/config/argparsing.py\n@@ -171,6 +171,7 @@\n         dest=\"collectonly\",\n         help=\"only collect tests, don't execute them.\",\n     )\n+    # TODO: Deprecate --co, it's too similar to --cov.\n     group.addoption(\n         \"--co\",\n         action=\"store_true\",\n@@ -178,6 +179,7 @@\n         dest=\"collectonly\",\n     )\n     group.addoption(\n+        \"-C\",  # Short alias for --collect-only.\n         \"--collect-only\",\n         action=\"store_true\",\n         dest=\"collectonly\",\n```\n"}
{"instance_id": "pytest-dev__pytest-7168", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/_io/saferepr.py\n+++ b/src/_pytest/_io/saferepr.py\n@@ -15,12 +15,21 @@\n\n def _format_repr_exception(exc: Exception, obj: object) -> str:\n     \"\"\"Format the information about exception raised in repr() call.\"\"\"\n+    # Use getattr with default values to guard against __getattr__ raising\n+    # an exception (although this is unlikely for built-in types).\n+    exc_name = getattr(type(exc), \"__name__\", \"unknown\")\n+    obj_name = getattr(type(obj), \"__name__\", \"unknown\")\n     try:\n-        exc_info = str(exc)\n+        # Try to get the exception message, guarding against str(exc) failing.\n+        exc_str = str(exc)\n     except Exception:\n-        exc_info = f\"<{type(exc).__name__} instance at {hex(id(exc))}>\"\n-    return \"<[reprexc] %s getting repr of %s instance at 0x%x>\" % (\n-        exc_info, obj.__class__.__name__, id(obj)\n+        # If str(exc) fails, provide a basic placeholder.\n+        exc_str = f\"<exception str() failed for {exc_name}>\"\n+\n+    # Format a useful message safely.\n+    # Based on original format string: <[reprexc] %s getting repr of %s instance at 0x%x>\n+    return \"<[reprexc] %\n"}
{"instance_id": "pytest-dev__pytest-7220", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/_code/code.py\n+++ b/src/_pytest/_code/code.py\n@@ -1016,20 +1016,10 @@\n \n     def _get_trimmed_path(self, path: str) -> str:\n         \"\"\"Try to shorten path\"\"\"\n-        # Paths are generally expected to be absolute here, except for\n-        # summary paths which are expected to be relative to CWD.\n-        # Let's try to normalize the path using the CWD.\n-        try:\n-            np = os.path.normpath(os.path.abspath(path))\n-        except OSError:\n-            np = path\n-        # Let's try to make it relative to CWD.\n-        try:\n-            rp = os.path.relpath(np)\n-            if len(rp) < len(np):\n-                return rp\n-            else:\n-                return path  # Return original if relpath is not shorter\n-        except ValueError:  # e.g. different drives\n-            return path  # Return original path\n+        # The path passed in should already be appropriately formatted,\n+        # usually relative to config.rootdir, by ExceptionInfo.getrepr().\n+        # Don't attempt to make it relative to the current working directory\n+        # here, as CWD might have been changed by a fixture/test, which\n+        # would break path resolution for editors and users. See #6476.\n+        return path\n```\n"}
{"instance_id": "pytest-dev__pytest-7373", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/mark/evaluate.py\n+++ b/src/_pytest/mark/evaluate.py\n@@ -5,12 +5,9 @@\n from typing import TYPE_CHECKING\n from typing import Type\n from typing import Union\n-from weakref import WeakKeyDictionary\n \n from _pytest.compat import TYPE_CHECKING\n from _pytest.config import Config\n-from _pytest.config import ExitCode\n-from _pytest.config.exceptions import UsageError\n from _pytest.outcomes import Skipped\n from _pytest.outcomes import XFailed\n \n@@ -22,50 +19,12 @@\n     from _pytest.nodes import Item\n \n \n-# Cache for evaluated skipif/xfail expressions. Config objects are stored by weak\n-# reference to ensure they don't stick around unnecessarily. A config object is\n-# used as a cache key to support evaluating expressions in the context of different\n-# pytest runs (which might have different values for variables like pytestconfig).\n-# We can't use Item objects as keys since they are recreated for each run.\n-_eval_cache: \"WeakKeyDictionary[Config, Dict[str, object]]\" = WeakKeyDictionary()\n-\n-\n class MarkEvalExc(Exception):\n-    \"\"\" exception used to wrap evaluation errors - for internal use only\"\"\"\n+    \"\"\"Exception used to wrap evaluation errors - for internal use only.\"\"\"\n \n     def __init__(self, expr: str, exc: Exception) -> None:\n-        Exception.__init__(self, expr, exc)\n+        super().__init__(expr, exc)\n         self.expr = expr\n         self.exc = exc\n-\n-\n-def cached_eval(\n-    config: Optional[Config], expr: str, d: Dict[str, object], funcargs: Dict[str, object]\n-) -> object:\n-    \"\"\"Evaluate expr in the context of d, funcargs, config using a cache.\"\"\"\n-    if config is not None and config.option.disable_warnings:\n-        # We assume that the expression will be evaluated multiple times,\n-        # so we can't rely on the warning registry. Just evaluate it\n-        # directly.\n-        return eval(expr, d, funcargs)\n-\n-    if config not in _eval_cache:\n-        _eval_cache[config] = {}\n-    cache = _eval_cache[config]\n-\n-    try:\n-        return cache[expr]\n-    except KeyError:\n-        import warnings\n-\n-        with warnings.catch_warnings():\n-            # Ignore warnings during eval. Otherwise warnings might provide\n-            # hints regarding the skip/xfail condition, which is confusing.\n-            warnings.simplefilter(\"ignore\")\n-            try:\n-                cache[expr] = res = eval(expr, d, funcargs)\n-            except Exception as exc:\n-                cache[expr] = res = MarkEvalExc(expr, exc)\n-        return res\n \n \n class MarkEvaluator:\n@@ -103,41 +62,32 @@\n             if isinstance(self.mark.args[0], str):\n                 expr = self.mark.args[0]\n                 if self.item is not None:\n-                    # \"funcargs\" are only available during test execution time.\n-                    d = {\"config\": self.item.config}\n-                    d.update(self.item.obj.__globals__)  # type: ignore[attr-defined]\n-                    result = cached_eval(\n-                        self.item.config, expr, d, self.item.funcargs\n-                    )\n+                    # Item is available, we are in runtest.\n+                    d = {\"config\": self.item.config}  # type: Dict[str, object]\n+                    d.update(self.item.obj.__globals__)\n+                    funcargs = self.item.funcargs\n                 else:\n+                    # Item is not available, we are in collection.\n                     d = self._get_globals()\n                     funcargs = {}\n \n-                if self.item is not None and self.item.config.option.disable_warnings:\n-                    # We assume that the expression will be evaluated multiple times,\n-                    # so we can't rely on the warning registry. Just evaluate it\n-                    # directly.\n-                    result = eval(expr, d, funcargs)\n-                else:\n-                    # TODO: This is internal, maybe move to MarkEvaluator?\n-                    from _pytest.warning_types import PytestWarning\n-\n-                    result = cached_eval(None, expr, d, funcargs)\n-\n-                if isinstance(result, MarkEvalExc):\n-                    raise result\n-\n-                # Re-evaluate for truthiness to handle numpy arrays, etc.\n-                return bool(result)\n+                # Eval the expression.\n+                try:\n+                    import warnings\n+                    with warnings.catch_warnings():\n+                        # Ignore warnings during eval. Otherwise warnings might provide\n+                        # hints regarding the skip/xfail condition, which is confusing.\n+                        warnings.simplefilter(\"ignore\")\n+                        result = eval(expr, d, funcargs)\n+                except Exception as exc:\n+                    # Wrap the exception and raise MarkEvalExc.\n+                    # The .istrue() method will handle it correctly.\n+                    markeval_exc = MarkEvalExc(self.getexplanation(), exc)\n+                    raise markeval_exc from exc\n+                else:\n+                    # Re-evaluate for truthiness to handle numpy arrays, etc.\n+                    return bool(result)\n \n             else:\n                 # Condition is a boolean.\n                 return bool(self.mark.args[0])\n         return True\n-\n-    def _get_globals(self) -> Dict[str, object]:\n-        if self.item is None:\n-            # Mark is module level, nothing we can do.\n-            return {\"config\": None}\n-        return self.item.obj.__globals__  # type: ignore[attr-defined]\n```\n"}
{"instance_id": "pytest-dev__pytest-7432", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -224,8 +224,19 @@\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n-    xfailed = item._store.get(xfailed_key, None)\n+    xfailed = item._store.get(xfailed_key, None)\n+\n+    # Fix location reporting for @pytest.mark.skip/skipif.\n+    # This needs to happen before the --runxfail check early-exits.\n+    if (\n+        item._store.get(skipped_by_mark_key, False)  # Check if skipped by mark\n+        and rep.skipped\n+        and type(rep.longrepr) is tuple\n+    ):\n+        _, _, reason = rep.longrepr\n+        filename, line = item.reportinfo()[:2]\n+        assert line is not None\n+        rep.longrepr = str(filename), line + 1, reason\n+\n     # unittest special case, see setting of unexpectedsuccess_key\n-    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n+    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n         if reason:\n             rep.longrepr = \"Unexpected success: {}\".format(reason)\n@@ -256,18 +267,6 @@\n             else:\n                 rep.outcome = \"passed\"\n                 rep.wasxfail = xfailed.reason\n-    elif (\n-        item._store.get(skipped_by_mark_key, True)\n-        and rep.skipped\n-        and type(rep.longrepr) is tuple\n-    ):\n-        # skipped by mark.skipif; change the location of the failure\n-        # to point to the item definition, otherwise it will display\n-        # the location of where the skip exception was raised within pytest\n-        _, _, reason = rep.longrepr\n-        filename, line = item.reportinfo()[:2]\n-        assert line is not None\n-        rep.longrepr = str(filename), line + 1, reason\n \n \n def pytest_report_teststatus(report: BaseReport) -> Optional[Tuple[str, str, str]]:\n```\n"}
{"instance_id": "pytest-dev__pytest-7490", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/reports.py\n+++ b/src/_pytest/reports.py\n@@ -16,6 +16,9 @@\n from _pytest.config import Config\n from _pytest.config.exceptions import PrintHelp\n from _pytest.nodes import Item\n+from _pytest.outcomes import skip, xfail\n+from _pytest.skipping import evaluate_xfail_marks\n+from _pytest.skipping import skipped_by_mark_key, xfailed_key\n from _pytest.store import StoreKey\n \n if TYPE_CHECKING:\n@@ -206,10 +209,35 @@\n                 rep.outcome = \"skipped\"\n                 rep.longrepr = (item.path, call.excinfo.value.lineno, call.excinfo.value.msg)\n             elif call.excinfo.errisinstance(xfail.Exception):\n-                rep.outcome = \"skipped\"  # TODO: this is confusing\n+                # pytest.xfail(...) was called\n+                rep.outcome = \"xfailed\"\n                 rep.wasxfail = call.excinfo.value.msg\n                 rep.longrepr = call.excinfo\n-            else:\n+            else:  # General failure\n+                # Check if the failure corresponds to an xfail marker.\n+                # This check is performed even if the marker was added dynamically during the call.\n+                xfail_marker_matches = False\n+                if not item.config.option.runxfail:\n+                    # Evaluate marks at the time of failure, including dynamic ones.\n+                    current_xfailed = evaluate_xfail_marks(item)\n+                    if current_xfailed:\n+                        rep.wasxfail = current_xfailed.reason  # Store reason\n+                        # Check if the raised exception matches the 'raises' condition\n+                        if current_xfailed.raises is None or call.excinfo.errisinstance(\n+                            current_xfailed.raises\n+                        ):\n+                            # Failure was expected (matches marker and 'raises')\n+                            if current_xfailed.strict:\n+                                # Strict xfail: report as failed\n+                                rep.outcome = \"failed\"\n+                            else:\n+                                # Non-strict xfail: report as xfailed\n+                                rep.outcome = \"xfailed\"\n+                            xfail_marker_matches = True\n+                        # else: raises condition not met, treat as normal failure below\n+\n+                # If not handled by an xfail marker, treat as a normal failure\n+                if not xfail_marker_matches:\n+                    if not item._store.get(skipped_by_mark_key, False):\n+                        rep.outcome = \"failed\"\n+                    else:\n+                        # This path indicates a skip marker was evaluated True\n+                        # but the test still ran and failed. This shouldn't\n+                        # happen with current skip logic. Default to failed.\n+                        rep.outcome = \"failed\"\n+\n+                # Populate longrepr for failures/xfailed outcomes, unless skipped by mark\n+                # (which shouldn't happen here, but check defensively).\n                 if not item._store.get(skipped_by_mark_key, False):\n-                    rep.outcome = \"failed\"\n+                    # Use item.repr_failure for failed, exception info for xfailed.\n+                    if rep.outcome == \"failed\":\n+                        rep.longrepr = item.repr_failure(call.excinfo)\n+                    elif rep.outcome == \"xfailed\":\n+                        rep.longrepr = call.excinfo\n+        else:  # Passed state\n+            if item._store.get(skipped_by_mark_key, False):\n+                # Skipped by mark but no exception? Should not happen.\n+                rep.outcome = \"passed\"\n+            else:\n+                # Check for unexpected pass (xpass)\n+                if not item.config.option.runxfail:\n+                    # Check xfail status evaluated during setup. Dynamic marks\n+                    # added during the test don't trigger xpass if the test passes.\n+                    xfailed_at_setup = item._store.get(xfailed_key, None)\n+                    if xfailed_at_setup:\n+                        rep.wasxfail = xfailed_at_setup.reason\n+                        if xfailed_at_setup.strict:\n+                            # Strict xfail: passing is an error\n+                            rep.outcome = \"failed\"\n+                            msg = \"strict xfail test passed unexpectedly\"\n+                            if rep.wasxfail:\n+                                msg += f\": {rep.wasxfail}\"\n+                            # Create a minimal longrepr for strict xpass failure\n+                            rep.longrepr = msg\n+                        else:\n+                            # Non-strict xfail: passing is xpass\n+                            rep.outcome = \"xpassed\"\n+                            rep.longrepr = None  # No longrepr for xpass\n+                    else:\n+                        # No xfail marker from setup, normal pass\n+                        rep.outcome = \"passed\"\n+                else:\n+                    # runxfail=True, treat as normal pass\n                     rep.longrepr = item.repr_failure(call.excinfo)\n-        else:\n-            if not item._store.get(skipped_by_mark_key, False):\n-                rep.outcome = \"passed\"\n+                    rep.outcome = \"passed\"\n+\n+        # Ensure outcome is set\n+        assert rep.outcome is not None\n+\n         for name, content in item.user_properties:\n             rep.user_properties.append((name, content))\n         return rep\n```\n"}
{"instance_id": "pytest-dev__pytest-8365", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/tmpdir.py\n+++ b/src/_pytest/tmpdir.py\n@@ -103,8 +103,10 @@\n             from_env = os.environ.get(\"PYTEST_DEBUG_TEMPROOT\")\n             temproot = Path(from_env or tempfile.gettempdir()).resolve()\n             user = get_user() or \"unknown\"\n+            # Replace characters invalid for directory names with an underscore.\n+            # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file#naming-conventions\n+            user = re.sub(r'[<>:\"/\\\\|?*]', \"_\", user)\n             # use a sub-directory in the temproot to speed-up\n             # make_numbered_dir() call\n             rootdir = temproot.joinpath(f\"pytest-of-{user}\")\n             rootdir.mkdir(exist_ok=True)\n             basetemp = make_numbered_dir_with_cleanup(\n```\n"}
{"instance_id": "pytest-dev__pytest-8906", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/src/_pytest/outcomes.py\n+++ b/src/_pytest/outcomes.py\n@@ -1,6 +1,10 @@\n \"\"\"Outcome exceptions.\n \n Pytest defines its own set of exceptions on top of Python exceptions, which are\n used to signal specific test outcomes. These are subclasses of the `OutcomeException`\n base class.\n+\n+.. autoexception:: pytest.skip.Exception\n+.. autoexception:: pytest.fail.Exception\n+.. autoexception:: pytest.xfail.Exception\n+.. autoexception:: pytest.exit.Exception\n \"\"\"\n import sys\n from typing import Any\n@@ -10,6 +14,7 @@\n from typing import Type\n from typing import TYPE_CHECKING\n \n+import attr\n from _pytest.compat import final\n from _pytest.deprecated import PENDING_PYTEST_8_API\n from _pytest.utils import _ExceptionChainRepr\n@@ -102,6 +107,28 @@\n     raise Skipped(msg=msg, allow_module_level=allow_module_level, _use_internal_repr=True)\n \n \n+@_with_exception(Skipped)\n+def skip_module(msg: str = \"\") -> \"NoReturn\":\n+    \"\"\"Skip the rest of the module being collected.\n+\n+    This function should be called only during module collection (e.g. at the\n+    module level). It provides a clearer way to skip the remainder of a module\n+    compared to calling :func:`pytest.skip` with ``allow_module_level=True``.\n+\n+    This is useful when a module requires dependencies or a specific environment\n+    that is not available, and importing the module or its dependencies would\n+    cause errors.\n+\n+    :param str msg: The message to report for the skip.\n+\n+    .. versionadded:: X.Y.Z\n+       Replace X.Y.Z with the pytest version this is released in.\n+    \"\"\"\n+    __tracebackhide__ = True\n+    # Raise Skipped directly with allow_module_level=True.\n+    raise Skipped(msg=msg, allow_module_level=True)\n+\n+\n @_with_exception(Failed)\n def fail(msg: str = \"\", pytrace: bool = True) -> \"NoReturn\":\n     \"\"\"Explicitly fail an executing test with the given message.\n--- a/src/_pytest/python.py\n+++ b/src/_pytest/python.py\n@@ -110,12 +110,13 @@\n             if e.allow_module_level:\n                 raise\n             # If allow_module_level is False (the default for pytest.skip), raise a CollectError.\n+            # Use e.msg to include the user's skip message in the error.\n+            msg_arg = f\"{e.msg!r}\" if e.msg else \"\"\n             raise self.CollectError(\n-                \"Using pytest.skip outside of a test is not allowed. \"\n-                \"To decorate a test function, use the @pytest.mark.skip \"\n-                \"or @pytest.mark.skipif decorators instead, and to skip a \"\n-                \"module use `pytestmark = pytest.mark.{skip,skipif}.\"\n+                f\"Using pytest.skip({msg_arg}) outside of a test function is not allowed.\\n\"\n+                \"To skip test functions, use the @pytest.mark.skip or @pytest.mark.skipif decorators.\\n\"\n+                \"To skip an entire module unconditionally, use `pytestmark = pytest.mark.skip(...)`.\\n\"\n+                \"To skip an entire module based on a runtime condition, use pytest.skip_module().\"\n             ) from e\n         self.config.pluginmanager.consider_module(mod)\n         return mod\n```\n"}
{"instance_id": "pytest-dev__pytest-9359", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "scikit-learn__scikit-learn-10297", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1,5 +1,6 @@\n \"\"\"Ridge regression\"\"\"\n \n+# Author: Mathieu Blondel <mathieu@mblondel.org>\n #         Reuben Fletcher-Costin <reuben.fletchercostin@gmail.com>\n #         Fabian Pedregosa <fabian.pedregosa@inria.fr>\n #         Michael Eickenberg <michael.eickenberg@nsi.edu>\n@@ -12,6 +13,7 @@\n import warnings\n \n import numpy as np\n+from numpy import linalg\n from scipy import linalg\n from scipy import sparse\n from scipy.sparse import linalg as sp_linalg\n@@ -24,6 +26,7 @@\n from ..utils.sparsefuncs import mean_variance_axis\n from ..utils import check_random_state\n from ..externals import six\n+from ..model_selection import GridSearchCV\n from ..multiclass import OneVsRestClassifier\n from ..preprocessing._data import _handle_zeros_in_scale, _scale_axis1\n from ..utils.multiclass import check_classification_targets\n@@ -1268,15 +1271,26 @@\n     def __init__(self, alphas=(0.1, 1.0, 10.0),\n                  fit_intercept=True, normalize=False, scoring=None,\n                  cv=None, class_weight=None):\n+                 store_cv_values=False):\n         super(RidgeClassifierCV, self).__init__(\n             alpha=0.0, fit_intercept=fit_intercept, normalize=normalize,\n             class_weight=class_weight)\n         self.alphas = np.asarray(alphas)\n         self.scoring = scoring\n         self.cv = cv\n+        self.store_cv_values = store_cv_values\n \n     # ... (rest of __init__ docstring) ...\n \n+    store_cv_values : boolean, default=False\n+        Flag indicating if the cross-validation values corresponding to\n+        each alpha should be stored in the `cv_values_` attribute (see\n+        below). This flag is only compatible with `cv=None` (i.e. using\n+        Leave-One-Out Cross-Validation).\n+\n     Attributes\n     ----------\n-    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n-                 shape = [n_samples, n_responses, n_alphas], optional\n-        Cross-validation values for each alpha (if store_cv_values=True and\n-        cv=None). After fit() has been called, this attribute will contain\n-        the mean squared errors (by default) or the values of the {loss,score}_func\n-        function (if provided in the constructor).\n+    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n+                 shape = [n_samples, n_classes, n_alphas], optional\n+        Cross-validation values for each alpha. This is only available\n+        if `cv=None` and `store_cv_values=True`. The shape is\n+        `(n_samples, n_alphas)` for binary classification (or when only\n+        one class is present), and `(n_samples, n_classes, n_alphas)`\n+        for multiclass classification. Each value corresponds to the output\n+        of the scoring function (or mean squared error if no score func is\n+        specified) applied to the internal `RidgeCV` model when `cv=None`.\n \n     coef_ : array, shape = [n_features] or [n_classes, n_features]\n         Weight vector(s).\n@@ -1286,11 +1300,14 @@\n         ``fit_intercept = False``.\n \n     alpha_ : float\n-        Estimated regularization parameter.\n+        Estimated regularization parameter. For multiclass classification, this\n+        is the average of the estimated `alpha_` per class.\n \n     n_iter_ : int\n         Number of iterations. Only available for sparse matrix and solver\n         'sparse_cg'.\n+\n+    estimators_ : list of n_classes estimators\n+        Estimators used for predictions.\n \n     Examples\n     --------\n@@ -1314,6 +1331,11 @@\n     def fit(self, X, y, sample_weight=None):\n         \"\"\"Fit Ridge classifier with built-in cross-validation.\n \n+        Parameters\n+        ----------\n+        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n+            Training vectors, where n_samples is the number of samples\n+            and n_features is the number of features.\n         Parameters\n         ----------\n         X : array-like, shape = [n_samples, n_features]\n@@ -1330,6 +1352,10 @@\n         self : object\n         \"\"\"\n         check_classification_targets(y)\n+        if self.cv is not None and self.store_cv_values:\n+            raise ValueError(\"cv!=None and store_cv_values=True \"\n+                             \" are incompatible\")\n+\n         self.classes_ = np.unique(y)\n         class_weight = self.class_weight\n \n@@ -1344,7 +1370,8 @@\n         if self.cv is None:\n             estimator = RidgeCV(alphas=self.alphas, fit_intercept=self.fit_intercept,\n                                 normalize=self.normalize, scoring=self.scoring,\n-                                cv=None)\n+                                cv=None,\n+                                store_cv_values=self.store_cv_values)\n             fit_params = {}\n         else:\n             estimator = Ridge(fit_intercept=self.fit_intercept,\n@@ -1364,6 +1391,16 @@\n         self.intercept_ = ovr.intercept_\n         if len(self.estimators_) == 1:\n             self.alpha_ = self.estimators_[0].alpha_\n+            if self.store_cv_values:\n+                # cv=None case, RidgeCV was used\n+                self.cv_values_ = self.estimators_[0].cv_values_\n+        elif self.store_cv_values:\n+            # cv=None case, RidgeCV was used, multiclass\n+            # Stack cv_values_ from each binary classifier\n+            cv_values_list = [est.cv_values_ for est in self.estimators_]\n+            self.cv_values_ = np.stack(cv_values_list, axis=1)\n+            # Store average alpha like RidgeCV does for multi-output\n+            self.alpha_ = np.mean([est.alpha_ for est in self.estimators_])\n+        else:\n+            # Store average alpha like RidgeCV does for multi-output\n+            self.alpha_ = np.mean([est.alpha_ for est in self.estimators_])\n \n         return self\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-10508", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/preprocessing/label.py\n+++ b/sklearn/preprocessing/label.py\n@@ -19,6 +19,7 @@\n from ..utils import column_or_1d\n from ..utils.validation import check_array\n from ..utils.validation import check_is_fitted\n+from ..utils.validation import _num_samples\n from ..utils.validation import _num_samples\n from ..utils.multiclass import unique_labels\n from ..utils.multiclass import type_of_target\n@@ -126,6 +127,10 @@\n         \"\"\"\n         check_is_fitted(self, 'classes_')\n\n+        # transform of empty array is empty array\n+        if _num_samples(y) == 0:\n+            return np.array([], dtype=np.int64)\n+\n         y = column_or_1d(y, warn=True)\n\n         classes = np.unique(y)\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-10949", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/utils/validation.py\n+++ b/sklearn/utils/validation.py\n@@ -218,6 +218,12 @@\n         # make sure we actually converted to numeric:\n         if dtype_numeric and array.dtype.kind == \"O\":\n             array = array.astype(np.float64)\n+        if warn_on_dtype and dtype_orig is not None and dtype_orig.kind == 'O' \\\n+                and np.issubdtype(array.dtype, np.floating):\n+            warnings.warn(\"Data with input dtype %s was converted to %s%s.\" %\n+                          (dtype_orig, array.dtype, context),\n+                          DataConversionWarning)\n+\n         if not allow_nd and array.ndim >= 3:\n             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n                              % (array.ndim, estimator_name))\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-11040", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/neighbors/base.py\n+++ b/sklearn/neighbors/base.py\n@@ -10,6 +10,7 @@\n # License: BSD 3 clause (C) INRIA, University of Amsterdam\n 10 8 import warnings\n 11 9 from abc import ABCMeta, abstractmethod\n+12 10 import numbers\n 13 11\n 14 12 import numpy as np\n 15 13 from scipy.sparse import csr_matrix, issparse\n@@ -107,6 +108,16 @@\n                  metric_params=None, n_jobs=1, **kwargs):\n \n         self.n_neighbors = n_neighbors\n+        # Validate n_neighbors\n+        if self.n_neighbors is not None:\n+            if not isinstance(self.n_neighbors, numbers.Integral):\n+                raise TypeError(\"n_neighbors must be an integer, got %s\" %\n+                                type(self.n_neighbors))\n+            elif self.n_neighbors <= 0:\n+                raise ValueError(\"Expected n_neighbors > 0. Got %d\" %\n+                                 self.n_neighbors)\n+\n+        # Rest of the parameters\n         self.radius = radius\n         self.algorithm = algorithm\n         self.leaf_size = leaf_size\n@@ -208,10 +219,19 @@\n         check_is_fitted(self, '_fit_method')\n \n         if n_neighbors is None:\n+            # If n_neighbors is not specified, use the value passed in init\n             n_neighbors = self.n_neighbors\n+        # Validate the number of neighbors\n+        elif not isinstance(n_neighbors, numbers.Integral):\n+            raise TypeError(\"n_neighbors must be an integer, got %s\" %\n+                            type(n_neighbors))\n+        elif n_neighbors <= 0:\n+            raise ValueError(\"Expected n_neighbors > 0. Got %d\" %\n+                             n_neighbors)\n \n         if X is not None:\n             query_is_train = False\n+            X = check_array(X, accept_sparse='csr')\n         else:\n             query_is_train = True\n             X = self._fit_X\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-11281", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/examples/mixture/plot_gmm_covariances.py\n+++ b/examples/mixture/plot_gmm_covariances.py\n@@ -102,7 +102,7 @@\n # Break up the dataset into non-overlapping training (75%) and testing\n # (25%) sets.\n skf = StratifiedKFold(n_splits=4)\n-# Only take the first fold.\n+# Only take the first fold. Requires the StratifiedKFold instance to be fitted as well.\n train_index, test_index = next(iter(skf.split(X, y)))\n \n \n@@ -115,7 +115,7 @@\n # Try GMMs using different types of covariances.\n estimators = {cov_type: GaussianMixture(n_components=n_classes,\n                                           covariance_type=cov_type, max_iter=20,\n-                                          random_state=0)\n+                                          random_state=0, n_clusters=n_classes)\n               for cov_type in ['spherical', 'diag', 'tied', 'full']}\n \n n_estimators = len(estimators)\n@@ -130,7 +130,7 @@\n     # Since we have class labels for the training data, we can\n     # initialize the GMM parameters in a supervised manner.\n     estimators[name].means_init = np.array([X_train[y_train == i].mean(axis=0)\n-                                            for i in range(n_classes)])\n+                                             for i in range(n_classes)])\n \n     # Train the other parameters using the EM algorithm.\n     estimators[name].fit(X_train)\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -1,12 +1,12 @@\n \"\"\"Bayesian Gaussian Mixture Models.\"\"\"\n \n-# Author: Wei Xue <xuewei4d@gmail.com>\n-#         Thierry Guillemot <thierry.guillemot.work@gmail.com>\n+# Author: Wei Xue <xuewei4d@gmail.com>, Thierry Guillemot <thierry.guillemot.work@gmail.com>\n # License: BSD 3 clause\n \n import numpy as np\n from scipy.special import digamma, gammaln\n \n+from ..utils.validation import check_is_fitted\n from ..utils import check_array\n from ._base import _BaseMixture, _check_shape\n from ._gaussian_mixture import _check_precision_positivity\n@@ -22,12 +22,12 @@\n \n     Parameters\n     ----------\n-    n_components : int, defaults to 1.\n+    n_clusters : int, defaults to 1.\n         The number of mixture components. Depending on the data and the value\n         of the `weight_concentration_prior` the model can decide to not use\n         all the components by setting some component `weights_` to values very\n         close to zero. The number of effective components is therefore smaller\n-        than n_components.\n+        than n_clusters.\n \n     covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'\n         String describing the type of covariance parameters to use.\n@@ -50,7 +50,7 @@\n         Allows to assure that the covariance matrices are all positive.\n \n     max_iter : int, defaults to 100.\n-        The number of EM iterations to perform.\n+        The number of variational inference iterations to perform.\n \n     n_init : int, defaults to 1.\n         The number of initializations to perform. The best results are kept.\n@@ -61,11 +61,11 @@\n             'kmeans' : responsibilities are initialized using kmeans.\n             'random' : responsibilities are initialized randomly.\n \n-    weights_init : array-like, shape (n_components, ), optional\n+    weights_init : array-like of shape (n_clusters,), default=None\n         The user-provided initial weights.\n         If it None, weights are initialized using the `init_params` method.\n \n-    means_init : array-like, shape (n_components, n_features), optional\n+    means_init : array-like of shape (n_clusters, n_features), default=None\n         The user-provided initial means.\n         If it None, means are initialized using the `init_params` method.\n \n@@ -73,19 +73,19 @@\n             optional\n         The user-provided initial precisions (inverse of the covariance matrices).\n         The shape depends on `covariance_type`::\n-            (n_components,)                        if 'spherical',\n-            (n_features, n_features)               if 'tied',\n-            (n_components, n_features)             if 'diag',\n-            (n_components, n_features, n_features) if 'full'\n+            (n_clusters,)                        if 'spherical',\n+            (n_features, n_features)             if 'tied',\n+            (n_clusters, n_features)             if 'diag',\n+            (n_clusters, n_features, n_features) if 'full'\n         If it None, precisions are initialized using the `init_params` method.\n \n     random_state : int, RandomState instance or None, optional (default=None)\n         Controls the random seed given to the method chosen to initialize the\n         parameters (see `init_params`).\n-        In addition, it controls the generation of random samples from the\n-        fitted distribution (see the method `sample`).\n-        Pass an int for reproducible output across multiple function calls.\n-        See :term:`Glossary <random_state>`.\n+        In addition, it controls the random draw of the weights and means at\n+        each initialization (if `init_params='random'`). Pass an int for\n+        reproducible results across multiple function calls.\n+        See :term:`Glossary <random_state>`.\n \n     warm_start : bool, default to False.\n         If 'warm_start' is True, the solution of the last fitting is used as\n@@ -110,10 +110,10 @@\n         The prior on the weight distribution (Dirichlet). This is a parameter\n         that influences the ability of the model to naturally adjust the number\n         of components used. If `weight_concentration_prior_type` is 'dirichlet_process',\n-        this is the concentration parameter for the Stick-breaking process. The higher\n-        concentration puts more mass on the single components. If\n+        this is the concentration parameter for the Stick-breaking process. The\n+        higher concentration puts more mass on the single components. If\n         `weight_concentration_prior_type` is 'dirichlet_distribution', this is\n-        the parameter of the Dirichlet distribution. The higher the value, the more\n+        the parameter of the Dirichlet distribution. The higher the value the more\n         strength is put on the weights being equal.\n         Defaults to ``1. / n_components``.\n \n@@ -121,7 +121,7 @@\n         The prior on the mean distribution (Gaussian).\n         The higher the value the more flexibility is allowed on the mean.\n         Defaults to 1.\n-    \n+\n     degrees_of_freedom_prior : float, optional.\n         The prior of the number of degrees of freedom on the covariance\n         distributions (Wishart). If it is not None, it must be greater\n@@ -139,10 +139,10 @@\n         The prior on the covariance distribution (Wishart). If it is not None,\n         it must be positive definite. If it is None, the prior is\n         ``reg_covar * eye(n_features)``. The shape depends on `covariance_type`::\n-            (n_features, n_features) if 'full',\n-            (n_features, n_features) if 'tied',\n-            (n_features)             if 'diag',\n-            (1,)                     if 'spherical'\n+            (n_features, n_features) if 'full'\n+            (n_features, n_features) if 'tied'\n+            (n_features,)            if 'diag'\n+            float                    if 'spherical'\n         Defaults to None.\n \n     Attributes\n@@ -150,13 +150,13 @@\n     weights_ : array-like, shape (n_components,)\n         The weights of each mixture components.\n \n-    means_ : array-like, shape (n_components, n_features)\n+    means_ : array-like, shape (n_clusters, n_features)\n         The mean of each mixture component.\n \n     covariances_ : array-like\n         The covariance of each mixture component.\n         The shape depends on `covariance_type`::\n-            (n_components,)                        if 'spherical',\n+            (n_clusters,)                        if 'spherical',\n             (n_features, n_features)               if 'tied',\n             (n_components, n_features)             if 'diag',\n             (n_components, n_features, n_features) if 'full'\n@@ -165,13 +165,13 @@\n     precisions_ : array-like\n         The precision matrices for each component in the mixture. A precision\n         matrix is the inverse of a covariance matrix. A covariance matrix is\n-        symmetric positive definite so the mixture of Gaussian can be\n+        symmetric positive definite so the mixture of Gaussian can be equivalently\n         parametrized by the precision matrices. Storing the precision matrices instead\n         of the covariance matrices makes it more efficient to compute the log-likelihood\n         of new samples at test time. The shape depends on `covariance_type`::\n-            (n_components,)                        if 'spherical',\n+            (n_clusters,)                        if 'spherical',\n             (n_features, n_features)               if 'tied',\n-            (n_components, n_features)             if 'diag',\n+            (n_clusters, n_features)             if 'diag',\n             (n_components, n_features, n_features) if 'full'\n \n     precisions_cholesky_ : array-like\n@@ -180,10 +180,10 @@\n         matrix is symmetric positive definite so the mixture of Gaussian can be\n         equivalently parametrized by the precision matrices. Storing the\n         Cholesky decomposition of the precision matrices makes it more efficient\n-        to compute the log-likelihood of new samples at test time. The shape\n+        to compute the log-likelihood of new samples at test time. The shape\n         depends on `covariance_type`::\n-            (n_components,)                        if 'spherical',\n-            (n_features, n_features)               if 'tied',\n+            (n_clusters,)                        if 'spherical',\n+            (n_features, n_features)             if 'tied',\n             (n_components, n_features)             if 'diag',\n             (n_components, n_features, n_features) if 'full'\n \n@@ -193,10 +193,10 @@\n     converged_ : bool\n         True when convergence was reached in fit(), False otherwise.\n \n-    n_iter_ : int\n+    n_iter_ : int\n         Number of step used by the best fit of inference to reach the convergence.\n \n-    lower_bound_ : float\n+    lower_bound_ : float\n         Lower bound value on the model evidence (of the training data) of the best fit of inference.\n \n     weight_concentration_prior_ : tuple or float\n@@ -204,19 +204,19 @@\n         concentration parameter of the Dirichlet distribution. If weight_concentration_prior_type\n         is 'dirichlet_process', the value corresponds to the concentration parameter of\n         the Stick-breaking process.\n-    \n+\n     weight_concentration_ : array-like, shape (n_components,)\n         The dirichlet concentration of each component on the weight distribution.\n \n-    mean_precision_prior_ : float\n+    mean_precision_prior_ : float\n         The precision on the mean distribution (Gaussian).\n \n-    mean_precision_ : array-like, shape (n_components,)\n+    mean_precision_ : array-like, shape (n_clusters,)\n         The precision of each components on the mean distribution (Gaussian).\n \n-    degrees_of_freedom_prior_ : float\n+    degrees_of_freedom_prior_ : float\n         The prior of the number of degrees of freedom on the covariance distributions (Wishart).\n-    \n+\n     degrees_of_freedom_ : array-like, shape (n_components,)\n         The number of degrees of freedom of each components in the model.\n \n@@ -224,13 +224,17 @@\n         The prior on the covariance distribution (Wishart). The shape depends on\n         `covariance_type`::\n             (n_features, n_features) if 'full',\n-            (n_features, n_features) if 'tied',\n-            (n_features)             if 'diag',\n-            (1,)                     if 'spherical'\n+            (n_features, n_features) if 'tied'\n+            (n_features,)            if 'diag'\n+            float                    if 'spherical'\n \n-    covariance_prior_ : array-like\n+    covariance_prior_ : array-like or float\n         The prior on the covariance distribution (Wishart).\n \n+    labels_ : array, shape (n_samples,)\n+        The labels for each data point in the data used to fit the model.\n+        Stored only if `fit` is called.\n+\n     See Also\n     --------\n     GaussianMixture : Finite Gaussian mixture model fitted with EM.\n@@ -244,12 +248,12 @@\n     >>> X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n     >>> bgm = BayesianGaussianMixture(n_components=2, random_state=42).fit(X)\n     >>> bgm.means_\n-    array([[ 4.0539...,  1.9918...],\n-           [ 0.9708...,  1.9991...]])\n+    array([[4.0539..., 1.9918...],\n+           [0.9708..., 1.9991...]])\n     >>> bgm.predict([[0, 0], [5, 5]])\n     array([1, 0])\n     \"\"\"\n-    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,\n+    def __init__(self, n_clusters=1, covariance_type='full', tol=1e-3,\n                  reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',\n                  weight_concentration_prior_type='dirichlet_process',\n                  weight_concentration_prior=None,\n@@ -258,7 +262,7 @@\n                  covariance_prior=None, random_state=None, warm_start=False,\n                  verbose=0, verbose_interval=10):\n \n-        super().__init__(n_components=n_components, tol=tol, reg_covar=reg_covar,\n+        super().__init__(n_clusters=n_clusters, tol=tol, reg_covar=reg_covar,\n                          max_iter=max_iter, n_init=n_init, init_params=init_params,\n                          random_state=random_state, warm_start=warm_start,\n                          verbose=verbose, verbose_interval=verbose_interval)\n@@ -275,7 +279,7 @@\n         self.covariance_prior = covariance_prior\n \n     def _check_parameters(self, X):\n-        \"\"\"Check the parameters of the model\n+        \"\"\"Check the parameters of the model.\n \n         Parameters\n         ----------\n@@ -283,7 +287,7 @@\n             Array with shape (n_samples, n_features)\n         \"\"\"\n         _check_shape(X, n_features=None)\n-        self._check_weights_init()\n+        self._check_initial_parameters(X)\n         self._check_means_init(X)\n         self._check_precisions_init(X)\n         self._check_covariance_type()\n@@ -291,11 +295,11 @@\n         self._check_weight_concentration_prior_parameter()\n         self._check_mean_prior_parameter()\n         self._check_degrees_of_freedom_prior_parameter()\n-        self._check_covariance_prior_parameter(X)\n+        self._check_covariance_prior_parameter(X) # Check covariance prior parameter\n \n     def _check_weights_init(self):\n         \"\"\"Check the user provided 'weights_init' parameter.\"\"\"\n-        if self.weights_init is not None:\n+        if self.weights_init is None:\n             pass\n         elif self.init_params == \"kmeans\":\n             check_is_fitted(self, \"weights_\")\n@@ -303,10 +307,10 @@\n             check_is_fitted(self, \"weights_\")\n         else:\n             raise ValueError(\"The provided weights_init parameter is inconsistent: \"\n-                             \"must be None or an array-like of shape \"\n-                             \"(n_components,).\")\n+                             \"must be None or an array-like of shape (n_clusters,).\")\n \n     def _check_means_init(self, X):\n+        \"\"\"Check the user provided 'means_init' parameter.\"\"\"\n         n_features = X.shape[1]\n         if self.means_init is None:\n             pass\n@@ -314,11 +318,11 @@\n             check_is_fitted(self, \"means_\")\n         else:\n             raise ValueError(\"The provided means_init parameter is inconsistent: \"\n-                             \"must be None or an array-like of shape \"\n-                             \"(n_components, n_features).\")\n+                             \"must be None or an array-like of shape (n_clusters, n_features).\")\n \n     def _check_precisions_init(self, X):\n         \"\"\"Check the user provided 'precisions_init' parameter.\"\"\"\n+        # Check precisions init parameters versus the n_features and covariance_type\n         n_features = X.shape[1]\n         if self.precisions_init is None:\n             pass\n@@ -326,10 +330,10 @@\n             check_is_fitted(self, \"precisions_cholesky_\")\n         else:\n             raise ValueError(\"The provided precisions_init parameter is inconsistent: \"\n-                             \"must be None or an array-like of shape depending on \"\n+                             \"must be None or an array-like of shape depending on \"\n                              \"covariance_type.\")\n \n-    def _check_covariance_type(self):\n+    def _check_covariance_type(self): # Check covariance type parameter\n         if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:\n             raise ValueError(\"Invalid value for 'covariance_type': %s \"\n                              \"'covariance_type' should be in \"\n@@ -340,13 +344,13 @@\n         \"\"\"Check the weight concentration prior parameters.\n \n         If weight_concentration_prior is None, it is set to 1. / n_components.\n-        \"\"\"\n+        \"\"\" # Check weight concentration prior type parameter\n         if self.weight_concentration_prior_type not in [\n                 'dirichlet_process', 'dirichlet_distribution']:\n             raise ValueError(\"Invalid value for \"\n                              \"'weight_concentration_prior_type': %s \"\n                              \"'weight_concentration_prior_type' should be in \"\n-                             \"['dirichlet_process', 'dirichlet_distribution']\"\n+                             \"['dirichlet_process', 'dirichlet_distribution']\",\n                              % self.weight_concentration_prior_type)\n \n         if self.weight_concentration_prior is None:\n@@ -354,7 +358,7 @@\n \n         if not isinstance(self.weight_concentration_prior, (float, tuple)) or \\\n            (isinstance(self.weight_concentration_prior, float) and\n-               self.weight_concentration_prior <= 0.):\n+            self.weight_concentration_prior <= 0.):\n             raise ValueError(\"The 'weight_concentration_prior' should be \"\n                              \"strictly positive (got %s).\"\n                              % self.weight_concentration_prior)\n@@ -362,7 +366,7 @@\n         # Check the shape of the weight concentration prior for the dirichlet distribution\n         if (self.weight_concentration_prior_type == 'dirichlet_distribution' and\n             isinstance(self.weight_concentration_prior, tuple)):\n-            if len(self.weight_concentration_prior) != self.n_components:\n+            if len(self.weight_concentration_prior) != self.n_clusters:\n                 raise ValueError(\"The 'weight_concentration_prior' should have \"\n                                  \"the same length as n_components (got %s).\"\n                                  % self.weight_concentration_prior)\n@@ -370,7 +374,7 @@\n                 raise ValueError(\"The 'weight_concentration_prior' should be \"\n                                  \"strictly positive (got %s).\"\n                                  % self.weight_concentration_prior)\n-    \n+\n     def _check_mean_prior_parameter(self):\n         \"\"\"Check the mean prior parameters.\"\"\"\n         if self.mean_precision_prior is None:\n@@ -380,7 +384,7 @@\n             raise ValueError(\"The 'mean_precision_prior' should be \"\n                              \"strictly positive (got %s).\"\n                              % self.mean_precision_prior)\n-    \n+\n     def _check_degrees_of_freedom_prior_parameter(self):\n         \"\"\"Check the prior parameters for the degrees of freedom parameter.\"\"\"\n         if self.degrees_of_freedom_prior is None:\n@@ -390,11 +394,11 @@\n             raise ValueError(\"The 'degrees_of_freedom_prior' should be \"\n                              \"greater than the number of features \"\n                              \"(including the constant term if present) \"\n-                             \"(got %s).\" % self.degrees_of_freedom_prior)\n+                             \"(got %s).\" % self.degrees_of_freedom_prior) # Check degrees of freedom prior parameter\n \n     def _check_covariance_prior_parameter(self, X):\n         \"\"\"Check the prior parameters for the covariance parameter.\"\"\"\n-        n_features = X.shape[1]\n+        _, n_features = X.shape\n \n         if self.covariance_prior is None:\n             self.covariance_prior_ = self.reg_covar * np.eye(n_features)\n@@ -402,7 +406,7 @@\n             self.covariance_prior_ = check_array(\n                 self.covariance_prior, (float, np.ndarray), ensure_2d=False)\n \n-        # Check the shape of the covariance prior\n+        # Check the shape of the covariance prior # Check covariance prior parameter\n         if self.covariance_type == 'full':\n             if (self.covariance_prior_.ndim != 2 or\n                     self.covariance_prior_.shape != (n_features, n_features)):\n@@ -410,7 +414,7 @@\n                                  \"(n_features, n_features) for covariance_type \"\n                                  \"'full' (got %s).\" % self.covariance_prior_.shape)\n             _check_precision_positivity(self.covariance_prior_,\n-                                        self.covariance_type)\n+                                        self.covariance_type) # Check covariance prior parameter\n         elif self.covariance_type == 'tied':\n             if (self.covariance_prior_.ndim != 2 or\n                     self.covariance_prior_.shape != (n_features, n_features)):\n@@ -418,7 +422,7 @@\n                                  \"(n_features, n_features) for covariance_type \"\n                                  \"'tied' (got %s).\" % self.covariance_prior_.shape)\n             _check_precision_positivity(self.covariance_prior_,\n-                                        self.covariance_type)\n+                                        self.covariance_type) # Check covariance prior parameter\n         elif self.covariance_type == 'diag':\n             if self.covariance_prior_.ndim != 1 or self.covariance_prior_.shape[0] != n_features:\n                 raise ValueError(\"The 'covariance_prior' should have shape \"\n@@ -426,7 +430,7 @@\n                                  \"'diag' (got %s).\" % self.covariance_prior_.shape)\n             if np.any(self.covariance_prior_ <= 0.):\n                 raise ValueError(\"The 'covariance_prior' should be positive.\")\n-        elif self.covariance_type == 'spherical':\n+        elif self.covariance_type == 'spherical': # Check covariance prior parameter\n             if self.covariance_prior_.ndim != 0:\n                 raise ValueError(\"The 'covariance_prior' should be a scalar \"\n                                  \"for covariance_type 'spherical' (got %s).\"\n@@ -434,11 +438,11 @@\n             if self.covariance_prior_ <= 0.:\n                 raise ValueError(\"The 'covariance_prior' should be positive.\")\n \n-    def _initialize_parameters(self, X, random_state):\n+    def _initialize_parameters(self, X, random_state): # Initialize parameters\n         \"\"\"Initialize the model parameters.\n \n         Parameters\n-        ----------\n+        ---------- # Initialize parameters\n         X : array-like, shape (n_samples, n_features)\n \n         random_state : RandomState\n@@ -446,14 +450,14 @@\n             The random state.\n         \"\"\"\n         n_samples, _ = X.shape\n-        \n+\n         # Init the weights, means and precisions parameters\n         # if the user didn't provide them\n         if self.weights_init is None:\n             self.weights_ = np.full(self.n_clusters, 1. / self.n_clusters)\n         else:\n             self.weights_ = check_array(self.weights_init, (int, float),\n-                                        ensure_2d=False, ensure_min_samples=self.n_components,\n+                                        ensure_2d=False, ensure_min_samples=self.n_clusters,\n                                         allow_nd=False)\n             if len(self.weights_) != self.n_clusters:\n                 raise ValueError(\"The 'weights_init' parameter should have shape \"\n@@ -462,10 +466,10 @@\n             if not np.all(self.weights_ >= 0.):\n                 raise ValueError(\"The 'weights_init' parameter should be positive.\")\n             if not np.isclose(np.sum(self.weights_), 1.):\n-                raise ValueError(\"The 'weights_init' parameter should sum to 1.\")\n+                raise ValueError(\"The 'weights_init' parameter should sum to 1.\") # Initialize parameters\n \n         if self.means_init is None:\n-            self.means_ = np.empty((self.n_components, X.shape[1]))\n+            self.means_ = np.empty((self.n_clusters, X.shape[1]))\n         else:\n             self.means_ = check_array(self.means_init, (int, float), ensure_2d=True,\n                                       allow_nd=False)\n@@ -473,11 +477,11 @@\n             if self.means_.shape[0] != self.n_clusters:\n                 raise ValueError(\"The 'means_init' parameter should have shape \"\n                                  \"(n_clusters, n_features).\")\n-            if self.means_.shape[1] != X.shape[1]:\n+            if self.means_.shape[1] != X.shape[1]: # Initialize parameters\n                 raise ValueError(\"The 'means_init' parameter should have shape \"\n                                  \"(n_clusters, n_features).\")\n \n-        if self.precisions_init is None:\n+        if self.precisions_init is None: # Initialize parameters\n             self.precisions_cholesky_ = _compute_precision_cholesky(\n                 np.array([np.cov(X.T) + self.reg_covar * np.eye(X.shape[1])\n                           for _ in range(self.n_clusters)]),\n@@ -486,7 +490,7 @@\n             self.precisions_cholesky_ = check_array(\n                 self.precisions_init, (int, float), ensure_2d=False, allow_nd=True)\n             _check_shape(self.precisions_cholesky_, self.covariance_type,\n-                         self.n_components)\n+                         self.n_clusters)\n \n         # Initialize the variational parameters\n         # We compute the initial estimate of the parameters from the initial\n@@ -494,11 +498,11 @@\n         # responsibilities.\n         resp = np.random.rand(n_samples, self.n_clusters)\n         resp /= resp.sum(axis=1)[:, np.newaxis]\n-        self._initialize(X, resp, random_state)\n+        self._initialize(X, resp, random_state) # Initialize parameters\n \n     def _initialize(self, X, resp, random_state):\n         \"\"\"Initialization of the mixture parameters.\n-\n+        # Initialize parameters\n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n@@ -510,11 +514,11 @@\n             The random state.\n         \"\"\"\n         n_samples, n_features = X.shape\n-        \n+\n         weights, means, covariances = _estimate_gaussian_parameters(\n             X, resp, self.reg_covar, self.covariance_type)\n         weights /= n_samples\n-        \n+\n         self.weights_ = weights if self.weights_init is None else self.weights_\n         self.means_ = means if self.means_init is None else self.means_\n         self.covariances_ = covariances if self.precisions_init is None else \\\n@@ -522,7 +526,7 @@\n                                            self.covariance_type)\n \n         # Update the parameters if the user provided values\n-        if self.weights_init is not None:\n+        if self.weights_init is not None: # Initialize parameters\n             self.weights_ = check_array(self.weights_init, (int, float),\n                                         ensure_2d=False, ensure_min_samples=self.n_clusters,\n                                         allow_nd=False)\n@@ -530,7 +534,7 @@\n             self.means_ = check_array(self.means_init, (int, float), ensure_2d=True,\n                                       allow_nd=False)\n \n-        if self.precisions_init is not None:\n+        if self.precisions_init is not None: # Initialize parameters\n             self.precisions_cholesky_ = check_array(\n                 self.precisions_init, (int, float), ensure_2d=False, allow_nd=True)\n             _check_shape(self.precisions_cholesky_, self.covariance_type,\n@@ -540,7 +544,7 @@\n         # degrees of freedom = n_features + 1\n         if self.degrees_of_freedom_prior is None:\n             self.degrees_of_freedom_prior_ = n_features\n-        else:\n+        else: # Initialize parameters\n             self.degrees_of_freedom_prior_ = self.degrees_of_freedom_prior\n \n         # covariance_prior = reg_covar * eye(n_features)\n@@ -550,7 +554,7 @@\n         # Initialize the variational parameters\n         self.weight_concentration_ = np.empty(self.n_clusters)\n         self.mean_precision_ = np.empty(self.n_clusters)\n-        self.means_ = np.empty((self.n_components, n_features))\n+        self.means_ = np.empty((self.n_clusters, n_features))\n         self.degrees_of_freedom_ = np.empty(self.n_clusters)\n         self.covariances_ = _estimate_gaussian_covariances(\n             resp, X, self.nk_, self.means_, self.reg_covar, self.covariance_type)\n@@ -560,7 +564,7 @@\n         # Update the parameters if the user provided values\n         if self.weights_init is not None:\n             self.weights_ = check_array(self.weights_init, (int, float),\n-                                        ensure_2d=False, ensure_min_samples=self.n_components,\n+                                        ensure_2d=False, ensure_min_samples=self.n_clusters,\n                                         allow_nd=False)\n         if self.means_init is not None:\n             self.means_ = check_array(self.means_init, (int, float), ensure_2d=True,\n@@ -568,7 +572,7 @@\n         if self.precisions_init is not None:\n             self.precisions_cholesky_ = check_array(\n                 self.precisions_init, (int, float), ensure_2d=False, allow_nd=True)\n-            _check_shape(self.precisions_cholesky_, self.covariance_type, self.n_components)\n+            _check_shape(self.precisions_cholesky_, self.covariance_type, self.n_clusters)\n \n         # Update the precisions parameters\n         self.precisions_ = _compute_precision_cholesky(\n@@ -576,7 +580,7 @@\n \n     def _m_step(self, X, log_resp):\n         \"\"\"M step.\n-\n+        # M step\n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n@@ -586,7 +590,7 @@\n             Logarithm of the posterior probabilities (or responsibilities) of\n             the point of each sample in X.\n         \"\"\"\n-        n_samples, n_features = X.shape\n+        n_samples, _ = X.shape\n         self.nk_, self.xk_, self.Sk_ = _estimate_gaussian_parameters(\n             X, np.exp(log_resp), self.reg_covar, self.covariance_type)\n \n@@ -602,7 +606,7 @@\n         self.degrees_of_freedom_ = self.degrees_of_freedom_prior_ + self.nk_\n         self.mean_precision_ = self.mean_precision_prior_ + self.nk_\n \n-        # Update the means and the covariances\n+        # Update the means and the covariances # M step\n         if self.covariance_type == 'full':\n             self.covariances_ = np.empty(\n                 (self.n_clusters, n_features, n_features))\n@@ -612,7 +616,7 @@\n         elif self.covariance_type == 'diag':\n             self.covariances_ = np.empty((self.n_clusters, n_features))\n         elif self.covariance_type == 'spherical':\n-            self.covariances_ = np.empty(self.n_components)\n+            self.covariances_ = np.empty(self.n_clusters)\n \n         for k in range(self.n_clusters):\n             diff = self.xk_[k] - self.mean_prior_\n@@ -620,7 +624,7 @@\n                 self.mean_precision_prior_ * self.nk_[k] /\n                 (self.mean_precision_prior_ + self.nk_[k])) * np.outer(diff, diff)\n             self.covariances_[k] = wishart_k + self.Sk_[k]\n-        \n+\n         # Update the means\n         self.means_ = (self.mean_precision_prior_ * self.mean_prior_ +\n                        self.nk_[:, np.newaxis] * self.xk_) / self.mean_precision_[:, np.newaxis]\n@@ -631,7 +635,7 @@\n         # Update the weights parameter\n         if self.weight_concentration_prior_type == 'dirichlet_process':\n             # Equation (37) in Bishop's PRML book\n-            digamma_sum = digamma(self.weight_concentration_[0] +\n+            digamma_sum = digamma(self.weight_concentration_[0] + # M step\n                                   self.weight_concentration_[1])\n             self.log_weights_ = (digamma(self.weight_concentration_[1]) - digamma_sum +\n                                  np.hstack((np.cumsum(digamma(self.weight_concentration_[0]) -\n@@ -642,7 +646,7 @@\n             self.log_weights_ = (digamma(self.weight_concentration_) -\n                                  digamma(np.sum(self.weight_concentration_)))\n \n-        # Update the cholesky decomposition of the precision matrices\n+        # Update the cholesky decomposition of the precision matrices # M step\n         if self.covariance_type == 'full':\n             self.precisions_cholesky_ = np.empty(\n                 (self.n_clusters, n_features, n_features))\n@@ -652,7 +656,7 @@\n         elif self.covariance_type == 'diag':\n             self.precisions_cholesky_ = np.empty((self.n_clusters, n_features))\n         elif self.covariance_type == 'spherical':\n-            self.precisions_cholesky_ = np.empty(self.n_components)\n+            self.precisions_cholesky_ = np.empty(self.n_clusters)\n \n         for k in range(self.n_clusters):\n             if self.covariance_type == 'full':\n@@ -667,7 +671,7 @@\n \n     def _e_step(self, X):\n         \"\"\"E step.\n-\n+        # E step\n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n@@ -677,7 +681,7 @@\n         -------\n         log_prob_norm : float\n             Mean of the logarithms of the probabilities of each sample in X\n-\n+        # E step\n         log_responsibility : array, shape (n_samples, n_components)\n             Logarithm of the posterior probabilities (or responsibilities) of\n             the point of each sample in X.\n@@ -685,7 +689,7 @@\n         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n         return np.mean(log_prob_norm), log_resp\n \n-    def _compute_lower_bound(self, log_resp, log_prob_norm):\n+    def _compute_lower_bound(self, log_resp, log_prob_norm): # Compute lower bound\n         \"\"\"Estimate the lower bound of the model evidence.\n \n         Parameters\n@@ -702,7 +706,7 @@\n             The lower\n"}
{"instance_id": "scikit-learn__scikit-learn-12471", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/preprocessing/_encoders.py\n+++ b/sklearn/preprocessing/_encoders.py\n@@ -113,6 +113,39 @@\n     def _transform(self, X, handle_unknown='error'):\n         X = self._check_X(X)\n \n+        X_original_dtype = X.dtype\n+        n_samples, n_features = X.shape\n+\n+        # If handle_unknown='ignore', we will replace unknown categories\n+        # with the first category. Check if the first category is string\n+        # and longer than the max length of the current dtype. If so,\n+        # cast to object dtype to avoid truncation errors / ValueError.\n+        needs_object_dtype = False\n+        if handle_unknown == 'ignore' and X.dtype.kind in ('S', 'U'):\n+            dtype_str = X.dtype.str\n+\n+            # Check if dtype string matches pattern like <U5 or <S10\n+            is_fixed_length = False\n+            if len(dtype_str) > 2 and dtype_str[1] in ('S', 'U'):\n+                try:\n+                    # Extract the number N from '<U<N>' or '<S<N>'\n+                    max_len = int(dtype_str[2:])\n+                    if max_len > 0: # Ensure N is positive, ignore '<U0' etc.\n+                        is_fixed_length = True\n+                except ValueError:\n+                    is_fixed_length = False # Not a fixed-length string dtype\n+\n+            if is_fixed_length:\n+                for i in range(n_features):\n+                    # Ensure categories exist and are not empty for the check\n+                    if i < len(self.categories_) and len(self.categories_[i]) > 0:\n+                        first_category = self.categories_[i][0]\n+                        # Check character/byte length against max allowed (N)\n+                        if isinstance(first_category, (str, bytes)) and \\\n+                           len(first_category) > max_len:\n+                            needs_object_dtype = True\n+                            break\n+        if needs_object_dtype:\n+            X = X.astype(object, copy=True)\n+\n         _, n_features = X.shape\n         X_int = np.zeros_like(X, dtype=np.int)\n         X_mask = np.ones_like(X, dtype=np.bool)\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-13142", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "scikit-learn__scikit-learn-13241", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -174,6 +174,16 @@\n             indices = self.lambdas_.argsort()[::-1]\n             self.lambdas_ = self.lambdas_[indices]\n             self.alphas_ = self.alphas_[:, indices]\n+\n+        # fix sign ambiguity of eigenvectors\n+        # The eigenvectors are unique up to a sign flip.\n+        # We can fix the sign by requiring the element with largest absolute value\n+        # to be positive. If this element is zero, the sign is arbitrary.\n+        max_abs_cols = np.argmax(np.abs(self.alphas_), axis=0)\n+        signs = np.sign(self.alphas_[max_abs_cols, range(self.alphas_.shape[1])])\n+        # Handle the case where the max absolute value element is zero\n+        signs[signs == 0] = 1\n+        self.alphas_ *= signs\n \n         # remove eigenvectors with a zero eigenvalue (or close to zero)\n         if self.remove_zero_eig:\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-13439", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/pipeline.py\n+++ b/sklearn/pipeline.py\n@@ -183,6 +183,14 @@\n             return self.steps[ind][1]\n         return Pipeline(self.steps[ind])\n \n+    def __len__(self):\n+        \"\"\"Returns the length of the pipeline\n+\n+        This enables slicing with ``len()``.\n+        \"\"\"\n+        # Subtract 1 because the last element is the final estimator\n+        return len(self.steps)\n+\n     @property\n     def named_steps(self):\n         \"\"\"Read-only attribute to access any step parameter by user given name.\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-13496", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/ensemble/_iforest.py\n+++ b/sklearn/ensemble/_iforest.py\n@@ -1,6 +1,7 @@\n \"\"\"Isolation Forest.\"\"\"\n \n # Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>\n+#          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n #          Gael Varoquaux\n # License: BSD 3 clause\n \n@@ -10,7 +11,7 @@\n from ..tree import ExtraTreeRegressor\n from ..utils import check_random_state, check_array, gen_batches\n from ..utils.validation import check_is_fitted, _num_samples\n-from ._bagging import BaseBagging\n+from ._bagging import BaseBagging, _get_n_samples_bootstrap\n \n \n __all__ = [\"IsolationForest\"]\n@@ -117,6 +118,11 @@\n     max_features : int or float, optional (default=1.0)\n         The number of features to draw from X to train each base estimator.\n \n+        - If int, then draw `max_features` features.\n+        - If float, then draw `max_features * X.shape[1]` features.\n+\n+        Note: using a float number less than 1.0 or integer less than number\n+        of features will enable feature subsampling and leads to a longer\n         - If int, then draw `max_features` features.\n         - If float, then draw `max_features * X.shape[1]` features.\n \n@@ -135,6 +141,11 @@\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n     Attributes\n     ----------\n     estimators_ : list of DecisionTreeClassifier\n@@ -171,7 +182,8 @@\n                  max_features=1.0,\n                  bootstrap=False,\n                  n_jobs=None,\n-                 random_state=None, verbose=0):\n+                 random_state=None, verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_depth=None,\n@@ -188,7 +200,8 @@\n             max_features=max_features,\n             bootstrap=bootstrap,\n             bootstrap_features=False,\n-            oob_score=False)\n+            oob_score=False,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n@@ -200,6 +213,7 @@\n         self.n_jobs = n_jobs\n         self.random_state = random_state\n         self.verbose = verbose\n+        self.warm_start = warm_start\n \n     def _set_oob_score(self, X, y):\n         raise NotImplementedError(\"OOB score not supported by iforest\")\n@@ -210,6 +224,10 @@\n         # behaviour parameter.\n         if self.behaviour != 'old':\n             self.contamination = contamination\n+\n+        if not isinstance(self.warm_start, bool):\n+            raise TypeError(\"warm_start must be bool\")\n+\n         return self\n \n     def fit(self, X, y=None, sample_weight=None):\n@@ -249,6 +267,10 @@\n         # ensure that max_sample is in [1, n_samples]:\n         n_samples = X.shape[0]\n \n+        if not (0. < self.max_samples <= 1. or\n+                1 <= self.max_samples <= n_samples):\n+            raise ValueError(\"max_samples must be in (0, 1] or [1, {}]. \"\n+                             \"Got {}\" .format(n_samples, self.max_samples))\n         if isinstance(self.max_samples, numbers.Integral):\n             max_samples = self.max_samples\n         else:  # float\n@@ -256,10 +278,6 @@\n \n         self.max_samples_ = max_samples\n \n-        if not (0. < self.max_samples_ <= n_samples):\n-            raise ValueError(\"max_samples must be in (0, 1], got %r\" %\n-                             self.max_samples)\n-\n         super().fit(X, y, sample_weight=sample_weight)\n         return self\n \n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -10,7 +10,7 @@\n from sklearn.utils.testing import assert_array_equal\n from sklearn.utils.testing import assert_array_almost_equal\n from sklearn.utils.testing import assert_equal\n-from sklearn.utils.testing import assert_raises\n+from sklearn.utils.testing import assert_raises, assert_true\n from sklearn.utils.testing import assert_greater\n from sklearn.utils.testing import assert_less\n from sklearn.utils.testing import assert_no_warnings\n@@ -20,7 +20,7 @@\n from sklearn.model_selection import ParameterGrid\n from sklearn.externals import six\n \n-from sklearn.ensemble import IsolationForest\n+from sklearn.ensemble import IsolationForest, RandomForestRegressor\n from sklearn.ensemble.iforest import _average_path_length\n from sklearn.metrics import roc_auc_score\n from sklearn.utils.random import sample_without_replacement\n@@ -297,3 +297,27 @@\n     assert_raises(ValueError, clf.fit, X)\n     clf = IsolationForest(max_samples=1000)\n     assert_raises(ValueError, clf.fit, X)\n+\n+\n+def test_iforest_warm_start():\n+    \"\"\"Test Isolation Forest warm start.\"\"\"\n+    X, y = make_blobs(n_samples=100, random_state=0)\n+    X = X.astype(np.float32)\n+\n+    # Test warm_start=True\n+    clf = IsolationForest(n_estimators=5, max_samples=10, random_state=1,\n+                          warm_start=True)\n+    clf.fit(X)\n+\n+    assert_equal(len(clf.estimators_), 5)\n+    assert_equal(clf.n_estimators, 5)\n+\n+    clf.n_estimators += 5\n+    clf.fit(X)\n+\n+    assert_equal(len(clf.estimators_), 10)\n+    assert_equal(clf.n_estimators, 10)\n+\n+    # Test warm_start=False\n+    clf = IsolationForest(n_estimators=5, random_state=1, warm_start=False)\n+    clf.fit(X)\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-13497", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/feature_selection/mutual_info_.py\n+++ b/sklearn/feature_selection/mutual_info_.py\n@@ -46,7 +46,7 @@\n     X, y = check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)\n     n_samples, n_features = X.shape\n \n-    if discrete_features == 'auto':\n+    if isinstance(discrete_features, str) and discrete_features == 'auto':\n         if issparse(X):\n             discrete_features = True\n         else:\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-13584", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/base.py\n+++ b/sklearn/base.py\n@@ -197,11 +197,19 @@\n             if name in params:\n                 v = params[name]\n                 default_v = param.default\n-                # FIXME: We should compare using np.array_equal for arrays, etc.\n-                # Here we just check that the value is not the default value\n-                # by using !=. The comparison logic is not robust yet.\n-                if v != default_v:\n+\n+                try:\n+                    # Use standard comparison first\n+                    is_different = v != default_v\n+\n+                    # For arrays, comparison results in a boolean array.\n+                    # Use np.any() to check if any element is different.\n+                    if isinstance(is_different, np.ndarray):\n+                        is_different = np.any(is_different)\n+\n+                    if is_different:\n+                        changed_params[name] = v\n+                except ValueError:\n+                    # Catch ValueError when comparing arrays with non-arrays\n+                    # or similar ambiguous comparisons. Treat as different.\n                     changed_params[name] = v\n         return changed_params\n \n```\n"}
{"instance_id": "scikit-learn__scikit-learn-13779", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/ensemble/_voting.py\n+++ b/sklearn/ensemble/_voting.py\n@@ -190,9 +190,9 @@\n         self._validate_estimators()\n \n         if sample_weight is not None:\n-            for name, est in self.estimators:\n-                if not has_fit_parameter(est, 'sample_weight'):\n-                    raise ValueError(f'Underlying estimator \\'{name}\\' does not'\n-                                     ' support sample weights.')\n+            for name, est in self.estimators:\n+                # Check sample_weight support only for non-None estimators\n+                if est is not None and not has_fit_parameter(est, 'sample_weight'):\n+                    raise ValueError(f\"Underlying estimator '{name}' does not\"\n+                                     \" support sample weights.\")\n \n         names, clfs = zip(*self.estimators)\n         self._validate_names(names)\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-14087", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/linear_model/logistic.py\n+++ b/sklearn/linear_model/logistic.py\n@@ -2188,11 +2188,11 @@\n \n             # Average the coefficients obtained for the best C parameter\n             if multi_class == 'multinomial':\n-                w = np.mean([coefs_paths[i][:, best_indices[i], :]\n+                w = np.mean([coefs_paths[i][best_indices[i]]\n                              for i in range(len(folds))], axis=0)\n             else:\n-                w = np.mean([coefs_paths[:, i, best_indices[i], :]\n-                             for i in range(len(folds))], axis=0)\n+                w = np.mean([coefs_paths[i][best_indices[i]]\n+                             for i in range(len(folds))], axis=0)\n \n             best_indices_C = best_indices % len(self.Cs_)\n             self.C_ = self.Cs_[best_indices_C]\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-14092", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/neighbors/nca.py\n+++ b/sklearn/neighbors/nca.py\n@@ -1,5 +1,6 @@\n \"\"\"Neighborhood Components Analysis\"\"\"\n \n+import numbers\n import warnings\n from time import time\n \n@@ -212,7 +213,8 @@\n \n         # Check the preferred dimensionality of the projected space\n         if self.n_components is not None:\n-            check_scalar(self.n_components, 'n_components', int, 1)\n+            check_scalar(self.n_components, 'n_components', numbers.Integral,\n+                         min_val=1)\n \n             if self.n_components > X.shape[1]:\n                 raise ValueError('The preferred dimensionality of the '\n@@ -232,9 +234,9 @@\n                                   .format(X.shape[1],\n                                           self.components_.shape[1]))\n \n-        check_scalar(self.max_iter, 'max_iter', int, 1)\n-        check_scalar(self.tol, 'tol', float, 0.)\n-        check_scalar(self.verbose, 'verbose', int, 0)\n+        check_scalar(self.max_iter, 'max_iter', numbers.Integral, min_val=1)\n+        check_scalar(self.tol, 'tol', numbers.Real, min_val=0.)\n+        check_scalar(self.verbose, 'verbose', numbers.Integral, min_val=0)\n \n         if self.callback is not None:\n             if not callable(self.callback):\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-14894", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/svm/base.py\n+++ b/sklearn/svm/base.py\n@@ -297,17 +297,25 @@\n         # with shape (n_support_vectors, n_features), where\n         # n_support_vectors = sum(self.n_support_). The support vectors\n         # are ordered by class.\n-        self.support_vectors_ = X[self.support_]\n \n         # dual_coef has shape (n_class-1, n_SV) for SVC\n         # dual_coef has shape (1, n_SV) for SVR and OneClass\n         # We need to convert it to a sparse matrix format\n-        dual_coef_indices = np.where(dual_coef)[1]\n-        dual_coef_data = dual_coef[dual_coef != 0]\n-        self.dual_coef_ = sp.csr_matrix(\n-            (dual_coef_data, dual_coef_indices, np.arange(n_class)),\n-            shape=(n_class - 1, dual_coef_indices.size // (n_class - 1)))\n+        n_sv = self.support_.shape[0]\n+\n+        if self._impl in ('epsilon_svr', 'nu_svr', 'one_class'):\n+            n_rows = 1\n+        else:  # c_svc, nu_svc\n+            n_rows = len(self.classes_) - 1\n+\n+        if n_sv == 0:\n+            # empty support vectors, dual_coef should be empty with correct shape\n+            self.support_vectors_ = sp.csr_matrix((0, X.shape[1]),\n+                                                  dtype=X.dtype)\n+            self.dual_coef_ = sp.csr_matrix((n_rows, 0),\n+                                            dtype=dual_coef.dtype)\n+        else:\n+            self.support_vectors_ = X[self.support_]\n+            # convert dense dual_coef to sparse matrix\n+            self.dual_coef_ = sp.csr_matrix(dual_coef, shape=(n_rows, n_sv))\n \n         if hasattr(self, 'probA_'):\n             self.probA_ = model['probA']\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-14983", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/model_selection/_split.py\n+++ b/sklearn/model_selection/_split.py\n@@ -1013,6 +1013,13 @@\n                          **self.cvargs)\n         return cv.get_n_splits(X, y, groups) * self.n_repeats\n \n+    def __repr__(self):\n+        # We need to define the __repr__ here since BaseEstimator's __repr__\n+        # does not work as we do not pass the parameters to the constructor.\n+        return ('{}(n_splits={}, n_repeats={}, random_state={})'\n+                .format(self.__class__.__name__,\n+                        self.cvargs['n_splits'],\n+                        self.n_repeats, self.random_state))\n \n class RepeatedKFold(_RepeatedSplits):\n     \"\"\"Repeated K-Fold cross validator.\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-15512", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/cluster/_affinity_propagation.py\n+++ b/sklearn/cluster/_affinity_propagation.py\n@@ -175,9 +175,10 @@\n     e = np.zeros((n_samples, convergence_iter))\n     ind = np.arange(n_samples)\n \n+    converged = False  # Flag to track convergence status\n     it = 0\n     while it < max_iter:\n-        # Compute responsibilities\n+        # Compute responsibilities (R)\n         Rold = R.copy()\n         AS = A + S\n         I = np.argmax(AS, axis=1)\n@@ -213,39 +214,53 @@\n             unconverged = (np.sum((se == convergence_iter) + (se == 0))\n                            != n_samples)\n             # Check convergence: stable exemplars and at least one cluster\n-            if (not\n"}
{"instance_id": "scikit-learn__scikit-learn-15535", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/metrics/cluster/_supervised.py\n+++ b/sklearn/metrics/cluster/_supervised.py\n@@ -42,10 +42,12 @@\n         The predicted labels.\n     \"\"\"\n     labels_true = check_array(\n-        labels_true, ensure_2d=False, ensure_min_samples=0\n+        labels_true, ensure_2d=False, ensure_min_samples=0,\n+        dtype=None  # Ensure labels_true is not converted to numeric\n     )\n     labels_pred = check_array(\n-        labels_pred, ensure_2d=False, ensure_min_samples=0\n+        labels_pred, ensure_2d=False, ensure_min_samples=0,\n+        dtype=None  # Ensure labels_pred is not converted to numeric\n     )\n \n     # Check that the arrays have the same length\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-25500", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -1,5 +1,6 @@\n \"\"\"Calibration of predicted probabilities.\"\"\"\n \n+# Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n #          Mathieu Blondel <mathieu@mblondel.org>\n #          Balazs Kegl <balazs.kegl@gmail.com>\n #          Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n@@ -17,6 +18,7 @@\n     _check_sample_weight,\n     _num_samples,\n     check_consistent_length,\n+    check_is_fitted,\n )\n from .utils import _safe_indexing\n from .isotonic import IsotonicRegression\n@@ -24,6 +26,7 @@\n from .model_selection import check_cv, cross_val_predict\n from .metrics._base import _check_pos_label_consistency\n from .metrics._plot.base import _get_response\n+from .utils import config_context\n \n \n class CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n@@ -748,7 +751,11 @@\n                 # When binary, `predictions` consists only of predictions for\n                 # clf.classes_[1] but `pos_class_indices` = 0\n                 class_idx += 1\n-            proba[:, class_idx] = calibrator.predict(this_pred)\n+            # Ensure calibrator predictions are numpy arrays, even if\n+            # transform_output is set to \"pandas\". Isotonic regression\n+            # can be impacted by this global configuration.\n+            with config_context(transform_output=\"default\"):\n+                proba[:, class_idx] = calibrator.predict(this_pred)\n \n         # Normalize the probabilities\n         if n_classes == 2:\n```\n"}
{"instance_id": "scikit-learn__scikit-learn-25570", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "scikit-learn__scikit-learn-25638", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "scikit-learn__scikit-learn-25747", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sklearn/utils/_set_output.py\n+++ b/sklearn/utils/_set_output.py\n@@ -1,5 +1,6 @@\n \"\"\"Utilities for the set_output API.\"\"\"\n import warnings\n+import numpy as np\n\n try:\n     import pandas as pd\n@@ -49,15 +50,28 @@\n         # TODO: Improve this check when feature_names_out supports callable\n         if callable(columns):\n             columns = columns()\n-        data_to_wrap.columns = columns\n+        try:\n+            data_to_wrap.columns = columns\n+        except ValueError as e:\n+            warnings.warn(\n+                f\"Could not set dataframe columns: {e}. Setting columns to default.\",\n+                UserWarning,\n+                stacklevel=2,\n+            )\n     if index is not None:\n-        data_to_wrap.index = index\n+        if len(index) != data_to_wrap.shape[0]:\n+            warnings.warn(\n+                f\"The length of the output {data_to_wrap.shape[0]} does not match the\"\n+                f\" length of the input {len(index)}. This can happen when dealing\"\n+                \" with aggregation transformers. Defaulting to the transformer's\"\n+                \" index.\",\n+                UserWarning,\n+                stacklevel=2,\n+            )\n+        else:\n+            data_to_wrap.index = index\n     return data_to_wrap\n\n-# If data_to_wrap is not pandas, create a DataFrame\n-# TODO: Improve this check when feature_names_out supports callable\n-if callable(columns):\n-    columns = columns()\n-return pd.DataFrame(data_to_wrap, index=index, columns=columns)\n+    # If data_to_wrap is not pandas, create a DataFrame\n+    # TODO: Improve this check when feature_names_out supports callable\n+    if callable(columns):\n+        columns = columns()\n+\n+    # Check for index length mismatch before creating DataFrame\n+    # Use np.asarray to get the shape of array-like objects\n+    output_shape = np.asarray(data_to_wrap).shape\n+    if index is not None and len(index) != output_shape[0]:\n+        warnings.warn(\n+            f\"The length of the output {output_shape[0]} does not match the length\"\n+            f\" of the input {len(index)}. This can happen when dealing with\"\n+            \" aggregation transformers. Setting index to range.\",\n+            UserWarning,\n+            stacklevel=2,\n+        )\n+        # Do not assign the original index if lengths mismatch\n+        index = None\n+\n+    return pd.DataFrame(data_to_wrap, index=index, columns=columns)\n\n\n def _wrap_data_with_container(method, data_to_wrap, original_input, estimator):\n```\n"}
{"instance_id": "sphinx-doc__sphinx-10325", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -26,8 +26,24 @@\n                          if m.directly_defined]\n\n     def filter_members(self, members: ObjectMembers, want_all: bool\n-                       ) -> List[Tuple[str, Any, bool]]:\n+                       ) -> List[Tuple[str, Any, bool]]:  # type: ignore\n         \"\"\"Filter the given member list.\n+\n+        Members are skipped based on the options:\n+\n+        * ``members``: If this option is set (and not empty), only members mentioned\n+          in this list are included.\n+        * ``exclude-members``: Members mentioned in this list are excluded.\n+        * ``private-members``: Members starting with ``_`` are included if this\n+          option is set.\n+        * ``special-members``: Members starting with ``__`` are included if this\n+          option is set.\n+        * ``inherited-members``:\n+            * If the option is not set, only members directly defined in the class are included.\n+            * If the option is set as a flag (`True`), all inherited members are included.\n+            * If the option is set to a comma-separated list of class names, inherited members are\n+              included, *except* for those inherited from any of the specified classes.\n+        * ``undoc-members``: Members without a docstring are included if this option is set.\n \n         Members are skipped if\n \n@@ -40,20 +56,23 @@\n         The user can override the skipping decision by connecting to the\n         ``autodoc-skip-member`` event.\n         \"\"\"\n-        def is_filtered_inherited_member(name: str, obj: Any) -> bool:\n-            if inspect.isclass(self.object):\n-                for cls in self.object.__mro__:\n-                    if cls.__name__ == self.options.inherited_members and cls != self.object:\n-                        # given member is a member of specified *super class*\n-                        return True\n-                    elif name in cls.__dict__:\n-                        return False\n-                    elif name in self.get_attr(cls, '__annotations__', {}):\n-                        return False\n-                    elif isinstance(obj, ObjectMember) and obj.class_ is cls:\n-                        return False\n-\n-            return False\n+        # Parse the :inherited-members: option\n+        # Note: This requires the option spec in directive.py to use something like\n+        #       'inherited-members': unchanged_required\n+        #       instead of bool_option or similar. This change is not included in this patch.\n+        inherited_members_option = self.options.get('inherited-members')\n+        consider_inherited = 'inherited-members' in self.options\n+        ignored_base_classes = set()\n+        is_ignore_list_mode = False\n+\n+        if isinstance(inherited_members_option, str) and inherited_members_option.strip():\n+            # Value provided, treat as comma-separated list of classes to ignore\n+            ignored_base_classes = {name.strip() for name in inherited_members_option.split(',')\n+                                    if name.strip()}\n+            is_ignore_list_mode = True\n+            consider_inherited = True  # Ignore list implies considering inherited members\n+        elif inherited_members_option is None and 'inherited-members' in self.options:\n+            # Flag is present without value (:inherited-members:)\n+            consider_inherited = True\n+        # else: option not present, consider_inherited remains False (default)\n \n         ret = []\n \n@@ -67,10 +86,22 @@\n         else:\n             attr_docs = {}\n \n+        # Process members\n+        membernames = list(members.keys())\n+\n+        # Reorder members\n+        member_order = self.options.get('member-order', self.config.autodoc_member_order)\n+        if member_order == 'alphabetical':\n+            membernames.sort()\n+        elif member_order == 'groupwise':\n+            # requires members values to have an 'order' attribute\n+            # which ObjectMember has.\n+            membernames.sort(key=lambda name: members[name].order)\n+        # Note: 'bysource' is handled by get_object_members()\n+\n         # process members and determine which to skip\n-        for obj in members:\n+        for membername in membernames:\n             try:\n-                membername, member = obj\n+                member_info = members[membername]\n+                member = member_info.value\n                 # if isattr is True, the member is documented as an attribute\n                 if member is INSTANCEATTR:\n                     isattr = True\n@@ -80,6 +111,20 @@\n                 else:\n                     isattr = False\n \n+                # Determine origin of member\n+                origin_cls = member_info.cls\n+                is_inherited = origin_cls is not self.object\n+\n+                # Skip based on :inherited-members: option\n+                if is_inherited:\n+                    if not consider_inherited:\n+                        logger.debug('[autodoc] skipping inherited member %s.%s',\n+                                     self.fullname, membername)\n+                        continue  # Skip inherited members if option not set\n+                    if is_ignore_list_mode and origin_cls.__name__ in ignored_base_classes:\n+                        logger.debug('[autodoc] skipping member %s.%s inherited from ignored class %s',\n+                                     self.fullname, membername, origin_cls.__name__)\n+                        continue  # Skip members inherited from specified ignored classes\n+\n                 doc = getdoc(member, self.get_attr, self.config.autodoc_inherit_docstrings,\n                              self.object, membername)\n                 if not isinstance(doc, str):\n@@ -93,6 +138,7 @@\n                 if cls:\n                     cls_doc = self.get_attr(cls, '__doc__', None)\n                     if cls_doc == doc:\n+                        # Member docstring is inherited from the class, ignore it\n                         doc = None\n \n                 # determine skip decision - may be overridden by user code\n```\n"}
{"instance_id": "sphinx-doc__sphinx-10451", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/autodoc/typehints.py\n+++ b/sphinx/ext/autodoc/typehints.py\n@@ -41,13 +41,21 @@\n         if param.annotation is Parameter.empty:\n             continue\n \n-        if param.name in field_locations:\n-            lineno = field_locations[param.name]\n-            field_marker = ':param %s:' % param.name\n+        # Determine the expected parameter name/key and marker in the docstring\n+        if param.kind == Parameter.VAR_POSITIONAL:\n+            field_key = '*' + param.name\n+            field_marker = ':param *%s:' % param.name\n+        elif param.kind == Parameter.VAR_KEYWORD:\n+            field_key = '**' + param.name\n+            field_marker = ':param **%s:' % param.name\n+        else:\n+            field_key = param.name\n+            field_marker = ':param %s:' % param.name\n+\n+        if field_key in field_locations:\n+            lineno = field_locations[field_key]\n \n             # check the field marker is valid\n-            # N.B. this is needed to avoid merging typehints into parameters\n-            # description\n             if find_field(lines, lineno, field_marker):\n                 # determine the type field marker\n                 field_type = ':type %s:' % param.name\n```\n"}
{"instance_id": "sphinx-doc__sphinx-11445", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sphinx-doc__sphinx-7686", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sphinx-doc__sphinx-7738", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/napoleon/docstring.py\n+++ b/sphinx/ext/napoleon/docstring.py\n@@ -1016,9 +1016,6 @@\n         return name\n \n     def _escape_args_and_kwargs(self, name: str) -> str:\n-        if name.endswith('_'):\n-            name = name[:-1] + r'\\_'\n-\n         if name[:2] == '**':\n             return r'\\*\\*' + name[2:]\n         elif name[:1] == '*':\n```\n"}
{"instance_id": "sphinx-doc__sphinx-7975", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/environment/adapters/indexentries.py\n+++ b/sphinx/environment/adapters/indexentries.py\n@@ -118,23 +118,35 @@\n             for subentry in indexentry[1].values():\n                 subentry[0].sort(key=keyfunc0)  # type: ignore\n\n-        # sort the index entries; put all symbols at the front, even those\n-        # following the letters in ASCII, this is where the chr(127) comes from\n-        def keyfunc(entry: Tuple[str, List]) -> Tuple[str, str]:\n+        # sort the index entries; symbols (group 0) come before letters/underscores/categories (group 1)\n+        def keyfunc(entry: Tuple[str, List]) -> Tuple[int, str, str]:\n             key, (void, void, category_key) = entry\n+            original_key = key  # Keep original key for secondary sort\n+\n             if category_key:\n                 # using specified category key to sort\n                 key = category_key\n+\n             lckey = unicodedata.normalize('NFD', key.lower())\n             if lckey.startswith('\\N{RIGHT-TO-LEFT MARK}'):\n                 lckey = lckey[1:]\n-            if lckey[0:1].isalpha() or lckey.startswith('_'):\n-                lckey = chr(127) + lckey\n-            # ensure a determinstic order *within* letters by also sorting on\n-            # the entry itself\n-            return (lckey, entry[0])\n+\n+            # Determine the sort group (0 for symbols, 1 for letters/etc.)\n+            sort_group = 1  # Default to letter/category group\n+            first_char = lckey[0] if lckey else ''\n+\n+            # Grouping logic based on the key used for grouping (entry key or category key)\n+            if not first_char:\n+                sort_group = 0 # Empty key is like a symbol\n+            elif not first_char.isalpha() and first_char != '_':\n+                sort_group = 0 # It's a symbol\n+\n+            # Primary sort key within the group is the normalized key used for grouping\n+            primary_sort_key = lckey\n+            # Secondary sort key is always the normalized original entry key\n+            secondary_sort_key = unicodedata.normalize('NFD', original_key.lower())\n+\n+            return (sort_group, primary_sort_key, secondary_sort_key)\n         newlist = sorted(new.items(), key=keyfunc)\n\n         if group_entries:\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8273", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/builders/manpage.py\n+++ b/sphinx/builders/manpage.py\n@@ -10,6 +10,7 @@\n     :license: BSD, see LICENSE for details.\n \"\"\"\n \n+import os\n from os import path\n from typing import Any, Dict, List, Set, Tuple, Union\n \n@@ -79,10 +80,14 @@\n             docsettings.authors = authors\n             docsettings.section = section\n \n+            # target filename like 'foo.1'\n             targetname = '%s.%s' % (name, section)\n+            # directory name like 'man1'\n+            dirname = 'man%s' % section\n+            ensuredir(path.join(self.outdir, dirname))\n+\n             logger.info(darkgreen(targetname) + ' { ', nonl=True)\n             destination = FileOutput(\n-                destination_path=path.join(self.outdir, targetname),\n+                destination_path=path.join(self.outdir, dirname, targetname),\n                 encoding='utf-8')\n \n             tree = self.env.get_doctree(docname)\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8282", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -995,7 +995,7 @@\n                             # remove first parameter (cls) from class method\n                             overload = overload.replace(parameters=parameters[1:])\n\n-                    sigs.append(stringify_signature(overload))\n+                    sigs.append(stringify_signature(overload, **kwargs))\n\n                 return '\\n'.join(sigs)\n             elif sig:\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8435", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -17,12 +17,13 @@\n from inspect import Parameter, Signature\n from types import ModuleType\n from typing import (Any, Callable, Dict, Iterator, List, Optional, Sequence, Set, Tuple, Type,\n-                    TypeVar, Union, get_type_hints)\n+                    TypeVar, Union, cast, get_type_hints)\n \n from docutils.statemachine import StringList\n \n import sphinx\n from sphinx.application import Sphinx\n+from sphinx.config import Config\n from sphinx.config import ENUM, Config\n from sphinx.deprecation import RemovedInSphinx40Warning, RemovedInSphinx50Warning\n from sphinx.environment import BuildEnvironment\n@@ -30,6 +31,7 @@\n from sphinx.ext.autodoc.mock import mock\n from sphinx.locale import _, __\n from sphinx.pycode import ModuleAnalyzer, PycodeError\n+from sphinx.ext.autodoc.typehints import apply_type_aliases\n from sphinx.util import inspect, logging\n from sphinx.util.docstrings import extract_metadata, prepare_docstring\n from sphinx.util.inspect import (evaluate_signature, getdoc, object_description, safe_getattr,\n@@ -605,8 +607,10 @@\n                 if self.options.annotation is not None and \\\n                    self.options.annotation is not SUPPRESS:\n                     # annotation is stored as string, not in restify form.\n-                    annotation = self.options.annotation\n-                    self.add_line('   :annotation: %s' % annotation, sourcename)\n+                    annotation_str = self.options.annotation\n+                    # Apply type aliases replacement here\n+                    annotation_str = apply_type_aliases(annotation_str, self.config)\n+                    self.add_line('   :annotation: %s' % annotation_str, sourcename)\n                 # add module/class options (if any)\n                 if self.options.members and self.options.show_inheritance:\n                     warnings.warn(__(\"both :members: and :show-inheritance: option are given \"\n--- a/sphinx/ext/autodoc/typehints.py\n+++ b/sphinx/ext/autodoc/typehints.py\n@@ -18,8 +18,9 @@\n \n from sphinx import addnodes\n from sphinx.application import Sphinx\n+from sphinx.config import Config\n from sphinx.util import inspect, typing\n-\n+from sphinx.util.typing import restify\n \n def record_typehints(app: Sphinx, objtype: str, name: str, obj: Any,\n                      options: Dict, args: str, retann: str) -> None:\n@@ -50,3 +51,14 @@\n         signature = cast(addnodes.desc_signature, contentnode.parent[0])\n         if signature['module']:\n             fullname = '.'.join([signature['module'], signature['fullname']])\n+\n+\n+def apply_type_aliases(annotation_str: str, config: Config) -> str:\n+    \"\"\"Apply autodoc_type_aliases to a type hint string.\"\"\"\n+    if not annotation_str:\n+        return annotation_str\n+\n+    aliases = config.autodoc_type_aliases\n+    if aliases and annotation_str in aliases:\n+        return restify(aliases[annotation_str])\n+    return annotation_str\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8474", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/writers/html.py\n+++ b/sphinx/writers/html.py\n@@ -1,10 +1,18 @@\n \"\"\"HTML writers.\"\"\"\n \n+from typing import cast\n+\n from docutils import nodes\n from docutils.nodes import Element, Node, Text\n from docutils.writers.html5_polyglot import HTMLTranslator as BaseTranslator\n \n from sphinx import addnodes\n+from sphinx.builders.html import StandaloneHTMLBuilder\n+from sphinx.builders.singlehtml import SingleHTMLBuilder\n from sphinx.locale import _, __\n from sphinx.util import logging\n from sphinx.util.docutils import SphinxTranslator\n@@ -31,7 +39,12 @@\n             prefix = self.config.numfig_format.get(figtype)\n \n             def append_fignumber(figtype: str, figure_id: str) -> None:\n-                key = figtype\n+                if isinstance(self.builder, SingleHTMLBuilder):\n+                    key = \"%s/%s\" % (self.builder.current_docname, figtype)\n+                elif isinstance(self.builder, StandaloneHTMLBuilder):\n+                    key = figtype\n+                else:\n+                    key = figtype  # XXX: other builders?\n \n                 if key in self.builder.fignumbers:\n                     if figure_id in self.builder.fignumbers[key]:\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8506", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/domains/std.py\n+++ b/sphinx/domains/std.py\n@@ -50,6 +50,7 @@\n \n \n class Cmdoption(ObjectDescription):\n+    \"\"\"Description of a command-line option (.. option).\"\"\"\n     has_content = True\n     required_arguments = 1\n     optional_arguments = 0\n@@ -68,10 +69,6 @@\n             potential_sig = potential_sig.strip()\n             m = option_desc_re.match(potential_sig)\n             if not m:\n-                logger.warning(__('Malformed option description %r, should look like '\n-                                  '\"opt\", \"-opt args\", \"--opt args\", \"/opt args\" '\n-                                  'or \"+opt args\"'), potential_sig,\n-                               location=signode)\n                 optname, args = potential_sig, ''\n             else:\n                 optname, args = m.groups()\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8595", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -1,10 +1,17 @@\n             else:\n-                for member in members.values():\n-                    if member.__name__ not in self.__all__:\n-                        member.skipped = True\n-\n-                return False, list(members.values())\n+                # If __all__ is empty, return an empty list\n+                if not self.__all__:\n+                    return False, []\n+\n+                # Filter members based on __all__\n+                ret = []\n+                for name in self.__all__:\n+                    if name in members:\n+                        ret.append(members[name])\n+                    else:\n+                        # Optional: issue warning for missing members in __all__?\n+                        # logger.warning(...)\n+                return False, ret\n         else:\n             memberlist = self.options.members or []\n             ret = []\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8627", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/util/inspect.py\n+++ b/sphinx/util/inspect.py\n@@ -16,6 +16,7 @@\n import re\n import sys\n import typing\n+from types import ModuleType\n from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\n \n from sphinx.errors import PycodeError\n@@ -29,6 +30,10 @@\n else:\n     from typing import _ForwardRef as ForwardRef  # type: ignore\n \n+#: Alias for ForwardRef that works across Python versions\n+ForwardRefType = Type[ForwardRef]  # type: ignore\n+\n+\n # cache for getdoc()\n # maps module name -> module object\n doc_cache: Dict[str, Dict[str, str]] = {}\n@@ -220,6 +225,67 @@\n     return None  # not found\n\n\n+def find_public_alias(obj: Any) -> Optional[str]:\n+    \"\"\"Search sys.modules for a public alias of the given object.\n+\n+    This is used to find the public name for objects defined in private\n+    modules (e.g., ``_struct.Struct`` is publicly available as ``struct.Struct``).\n+\n+    .. versionadded:: 4.X.Y # Adjust version when merged\n+    \"\"\"\n+    obj_module_name = getattr(obj, '__module__', None)\n+\n+    if not obj_module_name:\n+        return None  # Cannot determine origin\n+\n+    # Only search if the object's module is private\n+    if not obj_module_name.startswith('_'):\n+        return None\n+\n+    found_alias = None\n+    private_module_base = obj_module_name.lstrip('_')\n+\n+    # Sort modules to ensure deterministic results if multiple aliases exist\n+    # Prefer shorter module names first, then alphabetical\n+    module_names = sorted(sys.modules.keys(), key=lambda k: (len(k), k))\n+\n+    for modname in module_names:\n+        module = sys.modules.get(modname)\n+\n+        # Skip private/pseudo modules, non-modules, and the object's own module\n+        if (modname.startswith('_') or module is None or\n+                not isinstance(module, ModuleType) or modname == obj_module_name):\n+            continue\n+\n+        try:\n+            # Iterate through module attributes using safe_getmembers for safety\n+            for name, member in safe_getmembers(module):\n+                if name.startswith('_'):  # Skip private attributes in the public module\n+                    continue\n+                try:\n+                    if member is obj:\n+                        # Found a potential public alias\n+                        current_alias = f\"{modname}.{name}\"\n+\n+                        # Prioritize alias if module name matches the private module base name\n+                        if modname == private_module_base:\n+                            found_alias = current_alias\n+                            # Found the best possible match for this module, break inner loop\n+                            break\n+\n+                        # Otherwise, store the first one found as a fallback\n+                        if found_alias is None:\n+                            found_alias = current_alias\n+\n+                except Exception:\n+                    # Catch comparison errors, etc.\n+                    pass\n+\n+            # If we found the preferred alias (matching module name), stop searching modules\n+            if found_alias and modname == private_module_base:\n+                break\n+\n+        except Exception:\n+            # Ignore errors iterating module contents (e.g., getattr errors via safe_getmembers)\n+            pass\n+\n+    return found_alias\n+\n+\n def getdoc(obj: Any,\n            get_try_import: Callable[[str], Any] = safe_try_import, *,\n            allow_inherited: bool = False,\n--- a/sphinx/util/typing.py\n+++ b/sphinx/util/typing.py\n@@ -17,7 +17,7 @@\n from sphinx.util import logging\n from sphinx.util.inspect import (get_annotations, isclass, isenumclass, isforwardref,\n                                  safe_getattr)\n-from sphinx.util.inspect import get_fullname_for_module  # NOQA\n+from sphinx.util.inspect import find_public_alias, get_fullname_for_module  # NOQA\n\n\n logger = logging.getLogger(__name__)\n@@ -136,16 +136,20 @@\n             # typing module provides some special forms classes (ex. ForwardRef).\n             # It is safe to output the name directly.\n             # Note: this branch is not covered by tests. Maybe it is not used.\n-            return ':py:class:`~%s.%s`' % (hint.__module__, hint.__qualname__)\n+            qualname = getattr(hint, '__qualname__', hint.__name__)\n+            return ':py:class:`~%s.%s`' % (hint.__module__, qualname)\n+        elif hint.__module__.startswith('_'):\n+            # Check for public alias for types defined in private modules (like _struct.Struct)\n+            alias = find_public_alias(hint)\n+            if alias:\n+                return f':py:class:`~{alias}`'\n+            else:\n+                # Fallback to the private name if no public alias is found\n+                qualname = getattr(hint, '__qualname__', hint.__name__)\n+                return ':py:class:`~%s.%s`' % (hint.__module__, qualname)\n         else:\n             # Find module alias for the type hint if possible\n             module = sys.modules.get(hint.__module__)\n-            qualname = get_fullname_for_module(hint, module)\n-            if qualname:\n-                return ':py:class:`~%s.%s`' % (hint.__module__, qualname)\n-            else:\n-                # Failed to determine the canonical name.\n-                # Give up and use __name__.\n-                return ':py:class:`~%s.%s`' % (hint.__module__, hint.__qualname__)\n+            full_qualname = get_fullname_for_module(hint, module)\n+            if full_qualname:\n+                return ':py:class:`~%s.%s`' % (hint.__module__, full_qualname)\n+            else:\n+                # Failed to determine the canonical name. Give up and use __qualname__.\n+                qualname = getattr(hint, '__qualname__', hint.__name__)\n+                return ':py:class:`~%s.%s`' % (hint.__module__, qualname)\n     elif isinstance(hint, typing.TypeVar):\n         if hint.__module__ == 'typing':\n             return ':py:data:`~%s`' % hint.__name__\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8713", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/napoleon/docstring.py\n+++ b/sphinx/ext/napoleon/docstring.py\n@@ -1,8 +1,16 @@\n use_admonition = self._config.napoleon_use_admonition_for_notes\n return self._parse_generic_section(_('Notes'), use_admonition)\n \n-    def _parse_other_parameters_section(self, section: str) -> List[str]:\n-        return self._format_fields(_('Other Parameters'), self._consume_fields())\n+def _parse_other_parameters_section(self, section: str) -> List[str]:\n+    if self._config.napoleon_use_param:\n+        # Allow to declare multiple parameters at once (ex: x, y: int)\n+        fields = self._consume_fields(multiple=True)\n+        # _format_docutils_params doesn't take a header.\n+        # The result is a simple list of fields' params/types\n+        return self._format_docutils_params(fields)\n+    else:\n+        fields = self._consume_fields()\n+        return self._format_fields(_('Other Parameters'), fields)\n \n     def _parse_parameters_section(self, section: str) -> List[str]:\n         if self._config.napoleon_use_param:\n```\n"}
{"instance_id": "sphinx-doc__sphinx-8721", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/viewcode.py\n+++ b/sphinx/ext/viewcode.py\n@@ -223,6 +223,10 @@\n     env = app.builder.env\n     if not hasattr(env, '_viewcode_modules'):\n         return\n+    # Do not generate pages for epub builder if viewcode_enable_epub is False\n+    if app.builder.name == 'epub' and not app.config.viewcode_enable_epub:\n+        return\n+\n     highlighter = app.builder.highlighter  # type: ignore\n     urito = app.builder.get_relative_uri\n \n```\n"}
{"instance_id": "sphinx-doc__sphinx-8801", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sphinx/ext/autodoc/importer.py\n+++ b/sphinx/ext/autodoc/importer.py\n@@ -269,8 +269,9 @@\n         for name in cls_annotations:\n             if name not in members:\n                 docstring = None\n-                if clsname:\n-                    docstring = '\\n'.join(attr_docs.get((clsname, name), ''))\n+                # Use simple class name to match the key used in attr_docs.\n+                # refs: https://github.com/sphinx-doc/sphinx/issues/8735\n+                docstring = '\\n'.join(attr_docs.get((cls.__name__, name), ''))\n\n                 members[name] = ObjectMember(name, INSTANCEATTR, class_=cls,\n                                              docstring=docstring)\n```\n"}
{"instance_id": "sympy__sympy-11400", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/ccode.py\n+++ b/sympy/printing/ccode.py\n@@ -16,7 +16,7 @@\n from sympy.core import S\n from sympy.core.compatibility import string_types, range\n from sympy.codegen.ast import Assignment\n-from sympy.printing.codeprinter import CodePrinter\n+from sympy.printing.codeprinter import CodePrinter, requires_find_shared_expressions\n from sympy.printing.precedence import precedence\n from sympy.sets.fancysets import Range\n \n@@ -173,6 +173,11 @@\n         \"\"\"\n         Print Function calls appropriately.\n         \"\"\"\n+        # Handle specific functions first\n+        if expr.func == S.sinc:\n+            # sinc(x) -> Piecewise((sin(x)/x, Ne(x, 0)), (1, True))\n+            return self._print(expr.rewrite(Piecewise))\n+\n         if expr.func.__name__ in self.known_functions:\n             cond_cfunc = self.known_functions[expr.func.__name__]\n             if isinstance(cond_cfunc, str):\n@@ -211,6 +216,7 @@\n                 expr.func, self._print(expr.func(*expr.args)))\n \n     def _print_Pow(self, expr):\n+        # XXX: C specific, should this be in the base class?\n         if \"Pow\" in self.functions:\n             return self._print_Function(expr)\n         PREC = precedence(expr)\n@@ -241,6 +247,7 @@\n         else:\n             return CodePrinter._print_Pow(self, expr)\n \n+    @requires_find_shared_expressions\n     def _print_Piecewise(self, expr):\n         # This method requires find_shared_expressions() to be executed before\n         # code printing, e.g.:\n```\n"}
{"instance_id": "sympy__sympy-11870", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/simplify/trigsimp.py\n+++ b/sympy/simplify/trigsimp.py\n@@ -737,6 +737,11 @@\n         TR12,  # expand tan of sum\n         lambda x: _eapply(factor, x, trigs),\n         TR2,  # tan-cot -> sin-cos\n+        # --- NEW STEP ---\n+        # Rewrite exp(I*x) patterns into sin/cos using Euler's formula\n+        # This helps simplify expressions like exp(I*x) + exp(-I*x) -> 2*cos(x)\n+        # or -I/2*(exp(I*x) - exp(-I*x)) -> sin(x)\n+        lambda x: x.rewrite(sin).rewrite(cos),\n         [identity, lambda x: _eapply(_mexpand, x, trigs)],\n         TR2i,  # sin-cos ratio -> tan\n         lambda x: _eapply(lambda i: factor(i.normal()), x, trigs),\n```\n"}
{"instance_id": "sympy__sympy-11897", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -560,11 +560,11 @@\n                     # Check if num is Rational(1, q)\n                     elif num.is_Rational and num.p == 1:\n                         num_str = \"1\"\n-                        den = Mul(num.q, den)\n+                        den = Mul(num.q, den, evaluate=False)\n                     # Check if num is Rational(-1, q)\n                     elif num.is_Rational and num.p == -1:\n                         num_str = \"-1\"\n-                        den = Mul(num.q, den)\n+                        den = Mul(num.q, den, evaluate=False)\n                     else:\n                         num_str = self._print(num)\n\n@@ -708,7 +708,7 @@\n                 return tex % self._print(expr.base)\n\n             # Check for expr = 1/x**n\n-            elif expr.exp.is_negative and not expr.base.is_Add: # Denominator is not a sum\n+            elif expr.exp.is_negative and expr.base is not S.Exp1 and not expr.base.is_Add: # Denominator is not a sum, base is not E\n                 # Use PPRINT_PRECEDENCE so that terms are not parenthesized unnecessarily\n                 tex = r\"\\frac{1}{%s}\"\n                 # A few nice rules are needed for powers like x**(-3/2)\n```\n"}
{"instance_id": "sympy__sympy-12171", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/mathematica.py\n+++ b/sympy/printing/mathematica.py\n@@ -4,9 +4,11 @@\n \n from __future__ import print_function, division\n from sympy.printing.codeprinter import CodePrinter\n+from sympy.printing.precedence import precedence\n from sympy.printing.str import StrPrinter\n-from sympy.printing.precedence import precedence\n \n+from sympy.core import Float, S\n+from sympy.core.function import Derivative\n # Used in MCodePrinter._print_Function(self)\n known_functions = {\n     \"exp\": [(lambda x: True, \"Exp\")],\n@@ -50,9 +52,6 @@\n     _number_symbols = set()\n     _not_supported = set()\n \n-    # Needs to be imported here for StrPrinter._print_Float\n-    from sympy.core.numbers import Integer, Rational\n-\n     def __init__(self, settings={}):\n         \"\"\"Register function mappings supplied by user\"\"\"\n         CodePrinter.__init__(self, settings)\n@@ -91,3 +90,16 @@\n \n     def _print_list(self, expr):\n         return '{' + ', '.join(self.doprint(a) for a in expr) + '}'\n+\n+    def _print_Derivative(self, expr):\n+        \"\"\"Prints the derivative in Mathematica format D[f[x], x]\"\"\"\n+        return \"D[%s]\" % (self.stringify(expr.args, \", \"))\n+\n+    def _print_Float(self, expr):\n+        \"\"\"Prints a float in Mathematica format (e.g., 1.0*^-4)\"\"\"\n+        # Print float to string using parent class's method respecting precision settings\n+        float_str = StrPrinter._print_Float(self, expr)\n+        # Replace 'e' with Mathematica's '*^' for scientific notation\n+        return float_str.replace('e', '*^')\n+\n+    def _print_NaN(self, expr):\n+        return 'Indeterminate'\n```\n"}
{"instance_id": "sympy__sympy-12236", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-12419", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-12454", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/matrices/matrices.py\n+++ b/sympy/matrices/matrices.py\n@@ -1109,7 +1109,7 @@\n         \"\"\"\n         return all(self[i, j].is_zero\n                    for i in range(1, self.rows)\n-                   for j in range(i))\n+                   for j in range(min(i, self.cols)))\n\n     @property\n     def is_zero(self):\n```\n"}
{"instance_id": "sympy__sympy-12481", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/combinatorics/permutations.py\n+++ b/sympy/combinatorics/permutations.py\n@@ -258,25 +258,18 @@\n             # list of cycles e.g. [[1, 2], [3, 4]]\n             cycles = args[0]\n             if len(cycles) == 0:\n-                perm = _af_new([]) # identity\n+                # Identity permutation, potentially with a specified size\n+                perm = _af_new([], size=size) # Use _af_new helper which handles size\n             else:\n-                # process first cycle\n-                c = cycles[0]\n-                perm = _new_from_cycle(c)\n-                elements = list(c)\n-                # process remaining cycles\n-                for c in cycles[1:]:\n-                    perm = perm * _new_from_cycle(c)\n-                    elements.extend(c)\n-                # check for duplicates across cycles\n-                if has_dups(elements):\n-                    raise ValueError(filldedent('''\n-                        All elements must be unique. Try using Cycle().'''))\n+                # Start with identity and compose cycles from left to right\n+                perm = _af_new([]) # Start with identity of size 0\n+                for c in cycles:\n+                    # Compose with permutation from the current cycle\n+                    # Multiplication handles composition correctly, even with non-disjoint cycles.\n+                    # The Permutation(c) call ensures each individual cycle is valid.\n+                    perm = perm * Permutation(c) # This recursively calls __new__\n \n             if size is not None:\n-                perm = Permutation(perm.array_form, size=size)\n-            obj = Basic.__new__(cls, perm.array_form, size=perm.size)\n-            obj._array_form = perm.array_form\n-            obj._size = perm.size\n-            return obj\n+                # If size was specified, ensure the final permutation has that size.\n+                # Re-create the permutation object using the final array form and the specified size.\n+                # This handles padding or potential errors if size is incompatible.\n+                perm = Permutation(perm.array_form, size=size)\n \n         elif isinstance(args[0], Cycle):\n             # Convert a single Cycle object\n```\n"}
{"instance_id": "sympy__sympy-13031", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/matrices/common.py\n+++ b/sympy/matrices/common.py\n@@ -1,4 +1,7 @@\n-0         return self._eval_get_diag_blocks()\n+# Note: Ensure necessary imports are present, e.g.:\n+from functools import reduce\n+from .matrices import ShapeError # Adjust import path if needed for ShapeError\n+\n+class MatrixCommon(object): # Assuming this is the class structure\n+    def _eval_get_diag_blocks(self): # Placeholder for context\n+        # ... original implementation ...\n+        pass\n\n 1\n 2     @classmethod\n@@ -16,11 +19,23 @@\n 14         [0, 1, 0, 2]])\n 15         \"\"\"\n 16         if len(args) == 0:\n-17             return cls._new()\n+17             return cls._new() # Returns 0x0 matrix\n+\n+        # Check shapes and handle zero-row case\n+        rows = args[0].rows\n+        for M in args[1:]:\n+            if M.rows != rows:\n+                raise ShapeError(\"Matrices must have the same number of rows to hstack.\")\n+\n+        if rows == 0:\n+            # Explicitly handle the zero-row case by summing columns\n+            total_cols = sum(M.cols for M in args)\n+            # Create a new matrix of shape (0, total_cols)\n+            return cls._new(0, total_cols, [])\n+        else:\n+            # Non-zero rows: use reduce with row_join (previous logic)\n+            kls = type(args[0])\n+            return reduce(kls.row_join, args)\n\n-19         kls = type(args[0])\n-20         return reduce(kls.row_join, args)\n 21\n 22     def reshape(self, rows, cols):\n 23         \"\"\"Reshape the matrix. Total number of elements must remain the same.\n@@ -49,3 +64,35 @@\n     def row_del(self, row):\n         \"\"\"Delete the specified row.\"\"\"\n         if row < 0:\n+            row += self.rows\n+        if not 0 <= row < self.rows:\n+            raise IndexError(\"Row index out of bounds: %s\" % row)\n+        # Keep track of the new shape\n+        new_rows, new_cols = self.rows - 1, self.cols\n+        new_list = self._mat[:row*self.cols] + self._mat[(row+1)*self.cols:]\n+        return self._new(new_rows, new_cols, new_list)\n+\n+    # ... intervening methods like vec ...\n+\n+    @classmethod\n+    def vstack(cls, *args):\n+        \"\"\"Return a matrix formed by joining args vertically (i.e.\n+        by repeated application of col_join).\n+        # ... (Examples) ...\n+        \"\"\"\n+        if len(args) == 0:\n+            return cls._new() # Returns 0x0 matrix\n+\n+        # Check shapes and handle zero-column case\n+        cols = args[0].cols\n+        for M in args[1:]:\n+            if M.cols != cols:\n+                raise ShapeError(\"Matrices must have the same number of columns to vstack.\")\n+\n+        if cols == 0:\n+            # Explicitly handle the zero-column case by summing rows\n+            total_rows = sum(M.rows for M in args)\n+            return cls._new(total_rows, 0, [])\n+        else:\n+            # Non-zero columns: use reduce with col_join (previous logic)\n+            kls = type(args[0])\n+            return reduce(kls.col_join, args)\n```\n"}
{"instance_id": "sympy__sympy-13043", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/integrals/intpoly.py\n+++ b/sympy/integrals/intpoly.py\n@@ -1,5 +1,12 @@\n \"\"\"Algorithms for integrating polynomials over geometric regions\"\"\"\n \n+# Add necessary imports\n+from sympy.core.singleton import S\n+from sympy.core.expr import Expr\n+from sympy.core.add import Add\n+from sympy.core.symbol import Symbol\n+from sympy.polys.polyfuncs import total_degree # Import total_degree\n+from sympy.core.sorting import default_sort_key # Import default_sort_key\n from sympy.functions.elementary.miscellaneous import Max, Min\n from sympy.functions.elementary.piecewise import Piecewise\n from sympy.geometry.entity import GeometryEntity\n@@ -11,11 +18,7 @@\n from sympy.polys.polytools import poly_from_expr, Poly\n from sympy.polys.polyutils import _sort_gens\n from sympy.polys.domains import ZZ, QQ\n-from sympy.core.singleton import S\n-from sympy.core.expr import Expr\n-from sympy.core.add import Add\n-from sympy.core.symbol import Symbol\n-from sympy.polys.polyfuncs import degree_list\n+from collections import defaultdict # Import defaultdict\n \n \n def poly_degree(p, *gens):\n@@ -168,38 +171,51 @@\n \n def decompose(expr, separate=False):\n     \"\"\"Decomposes an input polynomial into homogeneous ones of\n-    smaller or equal degree.\n-    Returns a dictionary with keys as the degree of the smaller\n-    constituting polynomials. Values are the constituting polynomials.\n+    smaller or equal degree.\n+\n+    If `separate` is False (default), returns a dictionary where keys\n+    are the degrees and values are the corresponding homogeneous\n+    polynomials.\n+    If `separate` is True, returns a list of all constituent monomials\n+    sorted in a canonical order.\n+\n     Parameters\n     ==========\n-    expr : Polynomial(SymPy expression)\n-\n-    Optional Parameters :\n-\n-    separate : If True then simply return a list of the constituent monomials\n-               If not then break up the polynomial into constituent homogeneous\n-               polynomials.\n+    expr : Polynomial (SymPy expression)\n+    separate : bool, optional\n+        If True, return a sorted list of constituent monomials.\n+        If False (default), return a dictionary grouping homogeneous parts by degree.\n+\n     Examples\n     ========\n     >>> from sympy.abc import x, y\n     >>> from sympy.integrals.intpoly import decompose\n-    >>> decompose(x**2 + x*y + x + y + x**3*y**2 + y**5)\n+\n+    >>> p = x**2 + x*y + x + y + x**3*y**2 + y**5\n+\n+    >>> decompose(p)\n     {1: x + y, 2: x**2 + x*y, 5: x**3*y**2 + y**5}\n-    >>> decompose(x**2 + x*y + x + y + x**3*y**2 + y**5, True)\n-    [x, y, x**2, y**5, x*y, x**3*y**2]\n+\n+    >>> decompose(p, separate=True)\n+    [x, y, x*y, x**2, y**5, x**3*y**2]\n+\n     \"\"\"\n     expr = S(expr)\n-    poly_dict = {}\n+    poly_dict = defaultdict(lambda: S.Zero) # Use defaultdict for summing\n+    all_monomials = []\n \n     if isinstance(expr, Expr) and not expr.is_number:\n-        if expr.is_Symbol:\n-            poly_dict[1] = expr\n-        elif expr.is_Add:\n-            symbols = expr.atoms(Symbol)\n-            degrees = [(sum(degree_list(monom, *symbols)), monom)\n-                       for monom in expr.args]\n+        # Use free_symbols which is standard\n+        symbols = expr.free_symbols\n+        # Use as_ordered_terms to get monomials in a standard order initially\n+        monomials = expr.as_ordered_terms()\n \n-            # The code below needs to handle the grouping and separate logic\n-            # This part is missing in the provided snippet but is where the fix is needed.\n-            # Assuming the original code populated poly_dict and returned list(poly_dict.values()) for separate=True\n+        for monom in monomials:\n+            if monom.is_number: # Handle constant terms\n+                deg = 0\n+            else:\n+                # Use total_degree for robustness\n+                deg = total_degree(monom, *symbols)\n+\n+            if separate:\n+                all_monomials.append(monom)\n+            else:\n+                poly_dict[deg] += monom\n+\n+    elif expr.is_number: # Handle case where input is just a number\n+        if separate:\n+            if expr != 0:\n+                all_monomials.append(expr)\n+        else:\n+            poly_dict[0] = expr\n+\n+    # Return based on separate flag\n+    if separate:\n+        # Sort the collected monomials using the standard SymPy sort key\n+        return sorted(all_monomials, key=default_sort_key)\n+    else:\n+        # Convert defaultdict back to a regular dict for output\n+        return dict(poly_dict)\n```\n"}
{"instance_id": "sympy__sympy-13146", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/simplify/simplify.py\n+++ b/sympy/simplify/simplify.py\n@@ -32,6 +32,19 @@\n     except AttributeError:\n         pass\n \n+    # Try nsimplify() on the expression, if it simplifies or is no more complex.\n+    # This helps with cases like x**2.5 - x**(5/2) which are numerically\n+    # equal but were not simplifying due to different types in the exponent.\n+    from sympy.simplify.simplify import nsimplify\n+    try:\n+        expr_nsimp = nsimplify(expr, rational=True)\n+        # If nsimplify makes it 0, take it; otherwise, only take it if\n+        # the measure function determines it is no more complex.\n+        if expr_nsimp == 0 or measure(expr_nsimp) <= measure(expr):\n+            expr = expr_nsimp\n+    except Exception:\n+        pass # nsimplify decided not to process the expression or failed\n+\n     original_expr = expr = signsimp(expr)\n \n     from sympy.simplify.hyperexpand import hyperexpand\n```\n"}
{"instance_id": "sympy__sympy-13177", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/core/mod.py\n+++ b/sympy/core/mod.py\n@@ -1,7 +1,7 @@\n \n             # handle cases where p is definite multiple of q\n             if (p == q or p == -q or\n-                    p.is_Pow and p.exp.is_Integer and p.base == q or\n+                    (p.is_Pow and p.exp.is_Integer and p.base == q and q.is_integer) or\n                     p.is_integer and q == 1):\n                 return S.Zero\n \n```\n"}
{"instance_id": "sympy__sympy-13437", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/functions/combinatorial/numbers.py\n+++ b/sympy/functions/combinatorial/numbers.py\n@@ -1019,6 +1019,10 @@\n     def eval(cls, n, k_sym=None, symbols=None):\n         n = sympify(n)\n \n+        # Handle simple case bell(oo) -> oo\n+        if k_sym is None and symbols is None and n is S.Infinity:\n+            return S.Infinity\n+\n         # Handle bell(n, k, symbols) -- incomplete Bell polynomials\n         if (k_sym is not None) and (symbols is not None):\n             k_sym = sympify(k_sym)\n```\n"}
{"instance_id": "sympy__sympy-13471", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/core/numbers.py\n+++ b/sympy/core/numbers.py\n@@ -1041,7 +1041,12 @@\n                 # it's a hexadecimal (coming from a pickled object)\n                 # assume that it is in standard form\n                 num = list(num)\n-                num[1] = long(num[1], 16)\n+                # Python 2's hex() gives strings like '0x1L' which Python 3's\n+                # int() doesn't like.\n+                hex_mantissa = num[1]\n+                if hex_mantissa.endswith('L'):\n+                    hex_mantissa = hex_mantissa[:-1]\n+                num[1] = long(hex_mantissa, 16)\n                 _mpf_ = tuple(num)\n             else:\n                 if len(num) == 4:\n```\n"}
{"instance_id": "sympy__sympy-13480", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/functions/elementary/hyperbolic.py\n+++ b/sympy/functions/elementary/hyperbolic.py\n@@ -587,9 +587,9 @@\n             # Check if m is a multiple of I*pi\n             if m:\n                 # If k is integer, coth(m) = zoo. Then coth(x+m)=coth(x)\n                 # If k is half-integer, coth(m) = 0. Then coth(x+m)=tanh(x)\n-                if cotm is S.ComplexInfinity: # XXX cotm is not defined\n+                cothm = coth(m)\n+                if cothm is S.ComplexInfinity: # k is integer\n                     return coth(x)\n-                else: # cotm == 0\n+                elif cothm == 0: # k is half-integer\n                     return tanh(x)\n\n         if arg.func == atanh:\n```\n"}
{"instance_id": "sympy__sympy-13647", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/matrices/common.py\n+++ b/sympy/matrices/common.py\n@@ -5,7 +5,7 @@\n                 return self[i, j]\n             elif pos <= j < pos + other.cols:\n                 return other[i, j - pos]\n-            return self[i, j - pos - other.cols]\n+            return self[i, j - other.cols]\n\n         return self._new(self.rows, self.cols + other.cols,\n                          lambda i, j: entry(i, j))\n```\n"}
{"instance_id": "sympy__sympy-13773", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/matrices/expressions/matexpr.py\n+++ b/sympy/matrices/expressions/matexpr.py\n@@ -1,6 +1,7 @@\n from __future__ import print_function, division\n \n from sympy.core import Basic, S, Symbol, Integer, Tuple\n+from sympy.core.sympify import sympify\n from sympy.core.compatibility import range\n from sympy.core.decorators import call_highest_priority\n from sympy.functions import conjugate, adjoint\n@@ -10,6 +11,7 @@\n from sympy.matrices.expressions.transpose import transpose\n from sympy.matrices.expressions.trace import trace\n from sympy.matrices.expressions.determinant import det\n+from sympy.matrices.matrices import MatrixBase\n \n \n class MatrixExpr(Basic):\n@@ -142,9 +144,13 @@\n \n     @call_highest_priority('__matmul__')\n     def __matmul__(self, other):\n+        from .matmul import MatMul\n+        # sympify is needed because this method is called by Basic.__matmul__\n+        other = sympify(other)\n+        if not isinstance(other, (MatrixExpr, MatrixBase)):\n+            raise TypeError(\"Scalar operands are not allowed for matrix multiplication, use '*' instead\")\n         return MatMul(self, other)\n \n-    @call_highest_priority('__rmatmul__')\n     def __rmatmul__(self, other):\n         # As __rmatmul__ is not decorated with @call_highest_priority,\n         # this method will be called only if `other` does not know how\n@@ -152,6 +158,10 @@\n         # If `other` is a matrix, this method will not be called.\n         # See the implementation of Basic.__matmul__.\n         from .matmul import MatMul\n+        # sympify is needed because this method is called by Basic.__rmatmul__\n+        other = sympify(other)\n+        if not isinstance(other, (MatrixExpr, MatrixBase)):\n+            raise TypeError(\"Scalar operands are not allowed for matrix multiplication, use '*' instead\")\n         return MatMul(other, self)\n \n     def __pow__(self, exp):\n--- a/sympy/matrices/matrices.py\n+++ b/sympy/matrices/matrices.py\n@@ -39,6 +39,7 @@\n from sympy.utilities.iterables import is_sequence, NotIterable\n from sympy.utilities.misc import filldedent\n from sympy.matrices.expressions import MatrixExpr, Identity, ZeroMatrix\n+from sympy.matrices.expressions.matexpr import MatrixExpr\n \n \n class ShapeError(ValueError):\n@@ -530,10 +531,14 @@\n \n     @call_highest_priority('__matmul__')\n     def __matmul__(self, other):\n-        \"\"\"Return self @ other \"\"\"\n+        \"\"\"Return self @ other.\"\"\"\n+        # sympify is needed because this method is called by Basic.__matmul__\n+        other = _sympify(other)\n+        if not isinstance(other, (MatrixExpr, MatrixBase)):\n+            raise TypeError(\"Scalar operands are not allowed for matrix multiplication, use '*' instead\")\n         return self._eval_matrix_mul(other)\n \n-    @call_highest_priority('__rmatmul__')\n+    # No @call_highest_priority because we want this to be called only if\n+    # other.__matmul__(self) returns NotImplemented. See Basic.__rmatmul__.\n     def __rmatmul__(self, other):\n         \"\"\"Return other @ self \"\"\"\n         # Both classes that derive from MatrixExpr and MatrixBase implement\n@@ -541,21 +546,10 @@\n         # SymPy matrix type.\n         # If `other` is a NumPy array, we should let NumPy handle the\n         # multiplication. Otherwise, raise a TypeError.\n-        if hasattr(other, '__array_interface__'):\n-            # If `other` is a NumPy array, we should let NumPy handle the\n-            # multiplication.\n-            return NotImplemented\n-        elif isinstance(other, (list, tuple)):\n-            # If `other` is a list or tuple, convert it to a Matrix and retry.\n-            # This is necessary for compatibility with previous versions of\n-            # SymPy.\n-            # XXX: This should be deprecated and removed eventually.\n-            from .dense import Matrix\n-            return Matrix(other) @ self\n-        else:\n-            # Otherwise, raise a TypeError.\n-            # TODO: Add a check for scalars and raise a more specific error\n-            # message like NumPy does.\n-            raise TypeError(\"Matrix multiplication is not defined between \"\n-                            \"Matrix and type %s\" % type(other))\n+        # sympify is needed because this method is called by Basic.__rmatmul__\n+        other = _sympify(other)\n+        if not isinstance(other, (MatrixExpr, MatrixBase)):\n+            # We only end up here if `other` is not a SymPy matrix type.\n+            raise TypeError(\"Scalar operands are not allowed for matrix multiplication, use '*' instead\")\n+        return other._eval_matrix_mul(self)\n \n     def __neg__(self):\n         \"\"\"Return -self \"\"\"\n```\n"}
{"instance_id": "sympy__sympy-13895", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/simplify/radsimp.py\n+++ b/sympy/simplify/radsimp.py\n@@ -470,7 +470,7 @@\n     # We also need to ensure that all Pow are simplified, otherwise we might\n     # miss some possible simplifications.\n     expr = expr.replace(\n-        Pow, lambda b, e: powdenest(Pow(expand_mul(b), e), force=True))\n+        Pow, lambda b, e: powdenest(Pow(expand_mul(b), e)))\n\n     # Denest Bases\n     def _denest_term(term, positive_vars):\n```\n"}
{"instance_id": "sympy__sympy-13915", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/core/basic.py\n+++ b/sympy/core/basic.py\n@@ -14,6 +14,19 @@\n          >>> limit(x**3 - 3*x, x, oo)\n          oo\n \n+         Substitution into an expression that is undefined at the substitution\n+         point is possible, but the result is undefined (`nan`) or an infinite\n+         quantity (`oo`, `zoo`):\n+\n+         >>> from sympy import oo, zoo, nan, symbols\n+         >>> a, b = symbols('a b')\n+         >>> r = (1/(a+b) + 1/(a-b))/(1/(a+b) - 1/(a-b)) # = -a/b\n+         >>> r.subs(b, a) # -> (1/(2*a) + 1/0) / (1/(2*a) - 1/0) -> zoo / (-zoo)\n+         nan\n+         >>> r.limit(b, a)\n+         -1\n+         >>> (1/x).subs(x, 0)\n+         zoo\n+\n          If the substitution will be followed by numerical\n          evaluation, it is better to pass the substitution to\n          evalf as\n```\n"}
{"instance_id": "sympy__sympy-13971", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -14,9 +14,9 @@\n         else:\n             printset = tuple(s)\n \n-        return (r\"\\left\\[\"\n+        return (r\"[\"\n               + r\", \".join(self._print(el) for el in printset)\n-              + r\"\\right\\]\")\n+              + r\"]\")\n \n     _print_SeqPer = _print_SeqFormula\n     _print_SeqAdd = _print_SeqFormula\n```\n"}
{"instance_id": "sympy__sympy-14024", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-14308", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/physics/vector/vector.py\n+++ b/sympy/physics/vector/vector.py\n@@ -7,53 +7,3 @@\n         elif outstr.startswith(' '):\n             outstr = outstr[1:]\n         return outstr\n-\n-    def _pretty(self, printer=None):\n-        \"\"\"Pretty Printing method. \"\"\"\n-        from sympy.physics.vector.printing import VectorPrettyPrinter\n-        from sympy.printing.pretty.stringpict import prettyForm\n-        e = self\n-\n-        class Fake(object):\n-\n-            def render(self, *args, **kwargs):\n-                ar = e.args  # just to shorten things\n-                if len(ar) == 0:\n-                    return unicode(0)\n-                settings = printer._settings if printer else {}\n-                vp = printer if printer else VectorPrettyPrinter(settings)\n-                pforms = []  # output list, to be concatenated to a string\n-                for i, v in enumerate(ar):\n-                    for j in 0, 1, 2:\n-                        # if the coef of the basis vector is 1, we skip the 1\n-                        if ar[i][0][j] == 1:\n-                            pform = vp._print(ar[i][1].pretty_vecs[j])\n-                        # if the coef of the basis vector is -1, we skip the 1\n-                        elif ar[i][0][j] == -1:\n-                            pform = vp._print(ar[i][1].pretty_vecs[j])\n-                            pform= prettyForm(*pform.left(\" - \"))\n-                            bin = prettyForm.NEG\n-                            pform = prettyForm(binding=bin, *pform)\n-                        elif ar[i][0][j] != 0:\n-                            # If the basis vector coeff is not 1 or -1,\n-                            # we might wrap it in parentheses, for readability.\n-                            if isinstance(ar[i][0][j], Add):\n-                                pform = vp._print(\n-                                    ar[i][0][j]).parens()\n-                            else:\n-                                pform = vp._print(\n-                                    ar[i][0][j])\n-                            pform = prettyForm(*pform.right(\" \",\n-                                                ar[i][1].pretty_vecs[j]))\n-                        else:\n-                            continue\n-                        pforms.append(pform)\n-\n-                # TODO: This is copied from sympy.printing.pretty.pretty.py\n-                #       It should be unified.\n-                if len(pforms) == 0:\n-                    return unicode(0)\n-                pform = pforms[0]\n-                if pform.binding == prettyForm.NEG:\n-                    pform = prettyForm(*pform.left(\"-\"))\n-                else:\n-                    # XXX is this needed?\n-                    pform = prettyForm(*pform.left(\"+\"))\n-                for i in range(1, len(pforms)):\n-                    pform2 = pforms[i]\n-                    if pform2.binding == prettyForm.NEG:\n-                        pform = prettyForm(*pform.right(\" - \", pform2))\n-                    else:\n-                        pform = prettyForm(*pform.right(\" + \", pform2))\n-                if pform.picture[0].startswith('+'):\n-                    pform.picture[0] = pform.picture[0][1:]\n-                return pform\n-\n-        fake = Fake()\n-        return fake\n--- a/sympy/printing/pretty/pretty.py\n+++ b/sympy/printing/pretty/pretty.py\n@@ -7,43 +7,111 @@\n         return D\n \n     def _print_BasisDependent(self, expr):\n-        from sympy.vector import Vector\n+        from sympy.vector import Vector, Dyadic\n+        from sympy.printing.pretty.stringpict import prettyForm, stringPict\n+        from sympy.core.add import Add\n+        from sympy.core.numbers import Integer, Number\n+        from sympy.core.symbol import Symbol\n+        from sympy.functions.elementary.miscellaneous import Max\n \n-        if not self._use_unicode:\n-            raise NotImplementedError(\"ASCII pretty printing of BasisDependent is not implemented\")\n+        use_unicode = self._settings.get('use_unicode', True)\n+        if not use_unicode:\n+            # Fallback or error for ASCII\n+            return stringPict(str(expr)) # Or raise NotImplementedError\n \n         if expr == expr.zero:\n-            return prettyForm(expr.zero._pretty_form)\n-        o1 = []\n-        vectstrs = []\n+            # Print SymPy's Integer(0) using the printer\n+            return self._print(Integer(0))\n+\n+        # Dictionary to hold component information: {system: [(basis_vec, coeff), ...]}\n+        # or {system: dyadic_expr}\n+        components_dict = expr.separate()\n+\n+        # Check if components_dict is empty or contains only zero vectors/dyadics\n+        is_effectively_zero = True\n+        for system, item in components_dict.items():\n+            if hasattr(item, 'components'): # Check if it's vector-like\n+                if any(v != 0 for v in item.components.values()):\n+                    is_effectively_zero = False\n+                    break\n+            elif item != 0: # Handle non-vector components (e.g., Dyadic parts)\n+                 is_effectively_zero = False\n+                 break\n+        if is_effectively_zero:\n+            return self._print(Integer(0))\n+\n+        pforms = []  # List to hold {'form': abs_term_form, 'sign': sign} for each term\n+\n+        # Sort systems for consistent output (optional, but good practice)\n+        sorted_systems = sorted(components_dict.keys(), key=str)\n+\n+        for system in sorted_systems:\n+            bdexpr = components_dict[system] # BasisDependent expression for this system\n+\n+            # Assuming vect.components gives {basis_vector: coefficient}\n+            # This works for Vector. Need to adapt if Dyadic structure is different.\n+            if isinstance(bdexpr, Vector):\n+                inneritems = list(bdexpr.components.items())\n+                # Sort by basis vector string representation for consistent order\n+                inneritems.sort(key=lambda x: str(x[0]))\n+            elif isinstance(bdexpr, Dyadic):\n+                 # Dyadic printing needs specific logic, likely iterating through its terms\n+                 # For now, let's represent it as a single item to be printed by _print_Dyadic\n+                 # This might need refinement if _print_BasisDependent should handle Dyadic terms directly.\n+                 # Let's assume _print(bdexpr) handles Dyadics correctly for now.\n+                 # We treat the whole dyadic as one term.\n+                 inneritems = [(bdexpr, 1)] # Placeholder: treat dyadic as basis, coeff=1\n+            else:\n+                # Handle other BasisDependent types if necessary\n+                continue # Skip unknown types for now\n+\n+            for k, v in inneritems:  # k is basis vector or dyadic, v is coefficient\n+                # Skip zero terms\n+                if v == 0:\n+                    continue\n+\n+                # Determine sign\n"}
{"instance_id": "sympy__sympy-14317", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-14396", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/polys/polyoptions.py\n+++ b/sympy/polys/polyoptions.py\n@@ -6,8 +6,8 @@\n _re_realfield = re.compile(r\"^(R|RR)(_(\\d+))?$\")\n _re_complexfield = re.compile(r\"^(C|CC)(_(\\d+))?$\")\n _re_finitefield = re.compile(r\"^(FF|GF)\\((\\d+)\\)$\")\n-_re_polynomial = re.compile(r\"^(Z|ZZ|Q|QQ)\\[(.+)\\]$\")\n-_re_fraction = re.compile(r\"^(Z|ZZ|Q|QQ)\\((.+)\\)$\")\n+_re_polynomial = re.compile(r\"^(Z|ZZ|Q|QQ|R|RR|C|CC)\\[(.+)\\]$\")\n+_re_fraction = re.compile(r\"^(Z|ZZ|Q|QQ|R|RR|C|CC)\\((.+)\\)$\")\n _re_algebraic = re.compile(r\"^(Q|QQ)\\<(.+)\\>$\")\n \n @classmethod\n@@ -116,12 +116,18 @@\n \n             if r is not None:\n                 ground, gens = r.groups()\n-\n                 gens = list(map(sympify, gens.split(',')))\n \n                 if ground in ['Z', 'ZZ']:\n-                    return sympy.polys.domains.ZZ.poly_ring(*gens)\n+                    ground_domain = sympy.polys.domains.ZZ\n+                elif ground in ['Q', 'QQ']:\n+                    ground_domain = sympy.polys.domains.QQ\n+                elif ground in ['R', 'RR']:\n+                    ground_domain = sympy.polys.domains.RR\n+                elif ground in ['C', 'CC']:\n+                    ground_domain = sympy.polys.domains.CC\n                 else:\n-                    return sympy.polys.domains.QQ.poly_ring(*gens)\n+                    raise OptionError(\"Invalid ground domain: %s\" % ground) # Should not happen with regex\n+                return ground_domain.poly_ring(*gens)\n \n             r = cls._re_fraction.match(domain)\n \n@@ -132,10 +138,16 @@\n                 gens = list(map(sympify, gens.split(',')))\n \n                 if ground in ['Z', 'ZZ']:\n-                    return sympy.polys.domains.ZZ.frac_field(*gens)\n+                    ground_domain = sympy.polys.domains.ZZ\n+                elif ground in ['Q', 'QQ']:\n+                    ground_domain = sympy.polys.domains.QQ\n+                elif ground in ['R', 'RR']:\n+                    ground_domain = sympy.polys.domains.RR\n+                elif ground in ['C', 'CC']:\n+                    ground_domain = sympy.polys.domains.CC\n                 else:\n-                    return sympy.polys.domains.QQ.frac_field(*gens)\n-\n+                    raise OptionError(\"Invalid ground domain: %s\" % ground) # Should not happen with regex\n+                return ground_domain.frac_field(*gens)\n             r = cls._re_algebraic.match(domain)\n \n             if r is not None:\n```\n"}
{"instance_id": "sympy__sympy-14774", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/physics/vector/printing.py\n+++ b/sympy/physics/vector/printing.py\n@@ -5,7 +5,7 @@\n                 len(args) == 1 and \\\n                 not self._needs_function_brackets(expr.args[0])\n \n-            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acot\"]\n+            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acsc\", \"asec\", \"acot\"]\n \n             # If the function is an inverse trig function, handle the style\n             if func in inv_trig_table:\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -19,7 +19,7 @@\n                 len(args) == 1 and \\\n                 not self._needs_function_brackets(expr.args[0])\n \n-            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acot\"]\n+            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acsc\", \"asec\", \"acot\"]\n \n             # If the function is an inverse trig function, handle the style\n             if func in inv_trig_table:\n```\n"}
{"instance_id": "sympy__sympy-14817", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/pretty/pretty.py\n+++ b/sympy/printing/pretty/pretty.py\n@@ -817,21 +817,34 @@\n         return self._print(B.blocks)\n\n     def _print_MatAdd(self, expr):\n+        # Avoid importing S (sympify) here if possible, especially for parsing\n+        from sympy import MatMul\n+\n         s = None\n         for item in expr.args:\n-            pform = self._print(item)\n+            term_negative = False\n+            term_to_print = item\n+\n+            # Use as_coeff_mmul to safely extract coefficient and matrix part\n+            # item.as_coeff_mmul() -> (coeff, matrix_expr)\n+            coeff, M = item.as_coeff_mmul()\n+\n+            # Check if the coefficient is negative. coeff is a SymPy object.\n+            if coeff.is_negative:\n+                term_negative = True\n+                # Prepare the term to be printed without the negative sign\n+                if coeff == -1:\n+                    term_to_print = M\n+                else:\n+                    # Create a new MatMul with the positive coefficient\n+                    term_to_print = MatMul(-coeff, M, evaluate=False)\n+\n+            # Print the term (potentially without the negative sign)\n+            pform = self._print(term_to_print)\n+\n             if s is None:\n-                s = pform     # First element\n+                # First term\n+                if term_negative:\n+                    s = prettyForm(*stringPict.next(prettyForm('-'), pform))\n+                else:\n+                    s = pform\n             else:\n-                if S(item.args[0]).is_negative:\n-                    s = prettyForm(*stringPict.next(s, ' '))\n-                    pform = self._print(item)\n+                # Subsequent terms\n+                op = \" - \" if term_negative else \" + \"\n+                s = prettyForm(*stringPict.next(s, op, pform))\n-                else:\n-                    s = prettyForm(*stringPict.next(s, ' + '))\n-                s = prettyForm(*stringPict.next(s, pform))\n\n         return s\n\n```\n"}
{"instance_id": "sympy__sympy-15011", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-15308", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -30,6 +30,10 @@\n         else:\n             return r\"%s^\\dagger\" % self._print(mat)\n \n+    def _print_Trace(self, expr):\n+        # Using \\mathrm{Tr}, consistent with common practice.\n+        return r\"\\mathrm{Tr}\\left(%s\\right)\" % self._print(expr.arg)\n+\n     def _print_MatAdd(self, expr):\n         terms = [self._print(t) for t in expr.args]\n         l = []\n@@ -45,6 +49,12 @@\n             sign = \"\"\n         return sign + ' '.join(l)\n \n+    def _print_MatPow(self, expr):\n+        from sympy.printing.precedence import PRECEDENCE\n+        base = self.parenthesize(expr.base, PRECEDENCE['Pow'], strict=False)\n+        exp = self._print(expr.exp)\n+        return \"%s^{%s}\" % (base, exp)\n+\n     def _print_MatMul(self, expr):\n         from sympy import Add, MatAdd, HadamardProduct, MatMul, Mul\n \n```\n"}
{"instance_id": "sympy__sympy-15345", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/mathematica.py\n+++ b/sympy/printing/mathematica.py\n@@ -100,6 +100,14 @@\n         #        return self._print(expr.args[0])\n         return \"%s[%s]\" % (fname, self._print_seq(expr.args))\n \n+    def _print_Max(self, expr):\n+        # Mathematica Max uses square brackets.\n+        # The CodePrinter base class uses parentheses.\n+        return self._print_Function(expr)\n+\n+    def _print_Min(self, expr):\n+        # Mathematica Min uses square brackets.\n+        return self._print_Function(expr)\n \n     def _print_Pow(self, expr, rational=False):\n         if expr.exp == S.Half and not rational:\n```\n"}
{"instance_id": "sympy__sympy-15346", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-15609", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -997,8 +997,19 @@\n             return r\"\\left(%s\\right)\" % self._print(expr.args[0])\n         return r\"\\operatorname{erfc}%s\" % tex\n\n+    def _print_MatrixElement(self, expr):\n+        # Print indices i and j\n+        i_str = self._print(expr.i)\n+        j_str = self._print(expr.j)\n+\n+        # Add braces around indices starting with underscore\n+        # to prevent double subscript errors e.g. M_{i, _k}\n+        if i_str.startswith('_'):\n+            i_str = '{%s}' % i_str\n+        if j_str.startswith('_'):\n+            j_str = '{%s}' % j_str\n+\n+        return self._print(expr.parent) + '_{%s, %s}' % (i_str, j_str)\n\n-    def _print_MatrixElement(self, expr):\n-        return self._print(expr.parent) + '_{%s, %s}' % (self._print(expr.i), self._print(expr.j))\n     def _print_MatrixSlice(self, expr):\n         def latexslice(x):\n             x = list(x)\n```\n"}
{"instance_id": "sympy__sympy-15678", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-16106", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/mathml.py\n+++ b/sympy/printing/mathml.py\n@@ -283,6 +283,33 @@\n         x.appendChild(self.dom.createTextNode('i'))\n         return x\n \n+    def _print_Idx(self, expr):\n+        # Print the label of the Idx object. Range is ignored.\n+        # Idx('i', n) -> <mi>i</mi>\n+        return self._print(expr.label)\n+\n+    def _print_IndexedBase(self, expr):\n+        # Print the base label, potentially with symbol mapping\n+        # IndexedBase('A') -> <mi>A</mi>\n+        return self._print(expr.label)\n+\n+    def _print_Indexed(self, expr):\n+        # Use <msub> for base and indices\n+        # a[i] -> <msub><mi>a</mi><mi>i</mi></msub>\n+        # a[i, j] -> <msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>\n+        dom_element = self.dom.createElement('msub')\n+        dom_element.appendChild(self._print(expr.base))\n+\n+        if len(expr.indices) == 1:\n+            dom_element.appendChild(self._print(expr.indices[0]))\n+        else:\n+            mrow = self.dom.createElement('mrow')\n+            for i, index in enumerate(expr.indices):\n+                if i > 0:\n+                    mo = self.dom.createElement('mo')\n+                    mo.appendChild(self.dom.createTextNode(','))\n+                    mrow.appendChild(mo)\n+                mrow.appendChild(self._print(index))\n+            dom_element.appendChild(mrow)\n+        return dom_element\n+\n     def _print_Integer(self, expr):\n         dom_element = self.dom.createElement('mn')\n         dom_element.appendChild(self.dom.createTextNode(str(expr.p)))\n```\n"}
{"instance_id": "sympy__sympy-16281", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/pretty/pretty.py\n+++ b/sympy/printing/pretty/pretty.py\n@@ -34,41 +34,44 @@\n     def _print_Product(self, expr):\n         func = expr.term\n         pretty_func = self._print(func)\n+        func_height = pretty_func.height()\n+        func_width = pretty_func.width()\n\n-        horizontal_chr = xobj('_', 1)\n-        corner_chr = xobj('_', 1)\n-        vertical_chr = xobj('|', 1)\n-\n+        # Determine characters based on unicode setting\n         if self._use_unicode:\n-            # use unicode corners\n             horizontal_chr = xobj('-', 1)\n-            corner_chr = u'\\N{BOX DRAWINGS LIGHT DOWN AND HORIZONTAL}'\n+            corner_chr = u'\\N{BOX DRAWINGS LIGHT DOWN AND HORIZONTAL}' # '\u252c'\n+            vertical_chr = xobj('|', 1) # Using '|' for simplicity\n+        else:\n+            horizontal_chr = xobj('_', 1)\n+            corner_chr = xobj('_', 1) # ASCII uses '_' for corner too\n+            vertical_chr = xobj('|', 1)\n\n-        func_height = pretty_func.height()\n+        # Calculate the width of the space inside the product symbol\n+        # Make width depend on the function's width, min 1.\n+        width = max(1, func_width)\n+\n+        # Construct the product symbol lines\n+        sign_lines = []\n+        if self._use_unicode:\n+            # Use extended top bar for Unicode\n+            top_bar = horizontal_chr + corner_chr + horizontal_chr * width + corner_chr + horizontal_chr\n+            vert_segment = ' ' + vertical_chr + ' ' * width + vertical_chr + ' '\n+        else:\n+            # Original bar shape for ASCII\n+            top_bar = corner_chr + horizontal_chr * width + corner_chr\n+            vert_segment = vertical_chr + ' ' * width + vertical_chr\n+\n+        sign_lines.append(top_bar)\n+        # Loop func_height times (instead of func_height + 1) to remove extra bottom line\n+        for i in range(func_height):\n+            sign_lines.append(vert_segment)\n+\n+        # Stack the lines to form the core symbol\n+        pretty_sign = stringPict('')\n+        pretty_sign = prettyForm(*pretty_sign.stack(*sign_lines))\n+        sign_height = pretty_sign.height() # Store height of the core symbol\n\n         first = True\n         max_upper = 0\n-        sign_height = 0\n-\n-        for lim in expr.limits:\n-            width = (func_height + 2) * 5 // 3 - 2\n-            sign_lines = []\n-            sign_lines.append(corner_chr + (horizontal_chr*width) + corner_chr)\n-            for i in range(func_height + 1):\n-                sign_lines.append(vertical_chr + (' '*width) + vertical_chr)\n-\n-            pretty_sign = stringPict('')\n-            pretty_sign = prettyForm(*pretty_sign.stack(*sign_lines))\n-\n-            pretty_upper = self._print(lim[2])\n-            pretty_lower = self._print(Equality(lim[0], lim[1]))\n-\n-            max_upper = max(max_upper, pretty_upper.height())\n-\n-            if first:\n-                sign_height = pretty_sign.height()\n-\n-            pretty_sign = prettyForm(*pretty_sign.above(pretty_upper))\n-            pretty_sign = prettyForm(*pretty_sign.below(pretty_lower))\n-\n-            if first:\n-                pretty_func.baseline = 0\n-                first = False\n-\n-            height = pretty_sign.height()\n-            padding = stringPict('')\n-            padding = prettyForm(*padding.stack(*[' ']*(height - 1)))\n-            pretty_sign = prettyForm(*pretty_sign.right(padding))\n-\n-            pretty_func = prettyForm(*pretty_sign.right(pretty_func))\n-\n-        pretty_func.baseline = max_upper + sign_height//2\n-        pretty_func.binding = prettyForm.MUL\n-        return pretty_func\n+        lim_form = None # This will store the combined limit symbols\n+\n+        for lim in expr.limits:\n+            pu = self._print(lim[2]) # Upper limit\n+            pl = self._print(Equality(lim[0], lim[1])) # Lower limit (var=lower)\n+            max_upper = max(max_upper, pu.height())\n+\n+            # Create the symbol with limits for this iteration\n+            # Use the pre-calculated core symbol (pretty_sign)\n+            # Need to render/split to ensure we work with a copy for each limit\n+            current_lim_sign = prettyForm(*pretty_sign.render().split('\\n'))\n+            current_lim_sign = prettyForm(*current_lim_sign.above(pu))\n+            current_lim_sign = prettyForm(*current_lim_sign.below(pl))\n+\n+            if lim_form is None:\n+                lim_form = current_lim_sign # First limit symbol\n+            else:\n+                # Add subsequent symbols to the left\n+                lim_form = prettyForm(*lim_form.left(current_lim_sign))\n+\n+        # Choose the base symbol part (either with limits or the core symbol)\n+        if lim_form is None: # No limits were processed\n+            final_sign_part = pretty_sign\n+        else:\n+            final_sign_part = lim_form\n+\n+        # Combine the final sign part with the function\n+        final_form = prettyForm(*final_sign_part.right(pretty_func))\n+\n+        # Set the baseline: center of the core symbol, offset by max upper limit height\n+        final_form.baseline = max_upper + sign_height // 2\n+        final_form.binding = prettyForm.MUL\n+        return final_form\n+\n \n     def _print_Sum(self, expr):\n         ascii_mode = not self._use_unicode\n```\n"}
{"instance_id": "sympy__sympy-16503", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/pretty/pretty.py\n+++ b/sympy/printing/pretty/pretty.py\n@@ -1018,10 +1018,9 @@\n                 sign_height = prettySign.height()\n\n             if first:\n-                # change F baseline so it centers on the sign\n-                # XXX: Maybe this should be done *after* the loop?\n-                prettyF.baseline -= d - (prettyF.height()//2 -\n-                                         prettyF.baseline) - adjustment\n+                # The baseline of the function (`prettyF`) was previously adjusted here\n+                # to center it vertically relative to the summation symbol.\n+                # However, this adjustment could lead to incorrect baselines (e.g., negative)\n+                # and caused misalignment when the Sum was part of a larger expression (like Add).\n+                # Removing this adjustment ensures the function's natural baseline is used\n+                # for alignment, resolving the reported issue.\n                 first = False\n\n             # put padding to the right\n```\n"}
{"instance_id": "sympy__sympy-16792", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-16988", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-17022", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/lambdarepr.py\n+++ b/sympy/printing/lambdarepr.py\n@@ -16,7 +16,7 @@\n from sympy.core.compatibility import string_types, range\n from sympy.core.function import UndefinedFunction\n from sympy.external import import_module\n-from sympy.matrices import (MatrixBase, MatrixSlice, BlockMatrix, Identity,\n+from sympy.matrices import (MatrixBase, MatrixSlice, BlockMatrix, Identity,\n                             ZeroMatrix, OneMatrix, DiagonalMatrix, DiagonalOf)\n from sympy.tensor.array import NDimArray, DenseNDimArray, SparseNDimArray\n from sympy.utilities.decorator import doctest_depends_on\n@@ -216,6 +216,10 @@\n         # Tensorflow specific printing of DiagMatrix\n         return \"%s(%s)\" % (self._module_format('diag'), self._print(expr.diagonal()))\n\n+    def _print_Identity(self, expr):\n+        # Tensorflow specific printing of Identity matrix\n+        return \"%s(%s)\" % (self._module_format('eye'), self._print(expr.shape[0]))\n+\n     def _print_Min(self, expr):\n         return \"{0}({1})\".format(self._module_format('reduce_min'), ', '.join(map(self._print, expr.args)))\n\n@@ -305,6 +309,10 @@\n         # Numpy specific printing of DiagMatrix\n         return \"%s(%s)\" % (self._module_format('diag'), self._print(expr.diagonal()))\n\n+    def _print_Identity(self, expr):\n+        # Numpy specific printing of Identity matrix\n+        return \"%s(%s)\" % (self._module_format('eye'), self._print(expr.shape[0]))\n+\n     def _print_MatrixBase(self, expr):\n         # Handle zero dimensions:\n         if expr.rows == 0 or expr.cols == 0:\n@@ -441,6 +449,10 @@\n         # Mpmath specific printing of DiagMatrix\n         return \"%s(%s)\" % (self._module_format('diag'), self._print(expr.diagonal()))\n\n+    def _print_Identity(self, expr):\n+        # Mpmath specific printing of Identity matrix\n+        return \"%s(%s)\" % (self._module_format('eye'), self._print(expr.shape[0]))\n+\n     def _print_Piecewise(self, expr):\n         # Taken from NumPyPrinter._print_Piecewise\n         \"Piecewise function printer\"\n\n--- a/sympy/utilities/tests/test_lambdify.py\n+++ b/sympy/utilities/tests/test_lambdify.py\n@@ -1,6 +1,6 @@\n from __future__ import division, print_function, absolute_import\n\n-from sympy import (symbols, lambdify, Matrix, MatrixSymbol, Function, sin, cos,\n+from sympy import (symbols, lambdify, Matrix, MatrixSymbol, Function, sin, cos, I,\n                    tan, Mod, floor, ceiling, complex_valued, Dummy, Eq, Piecewise,\n                    Sum, factorial, gamma, loggamma, beta, polygamma, Abs, Max, Min,\n                    sign, conjugate, cot, acot, coth, acoth, sec, asec, csc, acsc,\n@@ -8,7 +8,7 @@\n                    re, im, arg, erf, erfc, erfinv, erfcinv, log, exp, sqrt, E,\n                    Rational, Float, Integer, Symbol, true, false, And, Or, Not, Xor,\n                    ITE, UnevaluatedExpr, oo, nan, zoo, pi, lowergamma, uppergamma,\n-                   LambertW, zeta, dirichlet_eta, KroneckerDelta, LeviCivita)\n+                   LambertW, zeta, dirichlet_eta, KroneckerDelta, LeviCivita, Identity)\n from sympy.codegen.cfunctions import Sqrt, log1p, expm1\n from sympy.core.compatibility import range, PY3\n from sympy.external import import_module\n@@ -1008,3 +1008,54 @@\n     assert f(1, 2) == 0\n     assert f(1, 1) == 1\n     assert f(2, 1) == 0\n+\n+\n+def test_lambdify_matrix_identity():\n+    # Test for issue #14719\n+    # Ensure Identity matrix prints correctly for numpy/mpmath/tensorflow\n+    n = symbols('n', integer=True)\n+    A = MatrixSymbol(\"A\", n, n)\n+    Idn = Identity(n)\n+    Id2 = Identity(2)\n+    B = MatrixSymbol(\"B\", 2, 2)\n+    x = symbols('x')\n+\n+    # Numpy\n+    np = import_module('numpy')\n+    if np:\n+        a = np.array([[1, 2], [3, 4]])\n+        # Case 1: Symbolic dimension n passed as argument\n+        f_sym = lambdify((A, n), A + Idn, modules='numpy')\n+        np.testing.assert_array_equal(f_sym(a, 2), np.array([[2., 2.], [3., 5.]]))\n+\n+        # Case 2: Concrete dimension in expression\n+        f_con = lambdify(B, B + Id2, modules='numpy')\n+        np.testing.assert_array_equal(f_con(a), np.array([[2., 2.], [3., 5.]]))\n+\n+        # Case 3: Check generated code uses 'eye'\n+        import inspect\n+        assert 'eye' in inspect.getsource(f_con)\n+\n+        # Case 4: Ensure sympy.I still works and prints 'I'\n+        f_I = lambdify(x, x + I, modules='numpy')\n+        assert f_I(1) == 1 + 1j\n+        assert 'I' in inspect.getsource(f_I) # Should print 'I' for sympy.I\n+        assert f_I.__globals__['I'] == 1j # Check namespace mapping\n+\n+    # Mpmath\n+    mpmath = import_module('mpmath')\n+    if mpmath:\n+        a = mpmath.matrix([[1, 2], [3, 4]])\n+        # Case 1: Symbolic dimension n passed as argument\n+        f_sym = lambdify((A, n), A + Idn, modules='mpmath')\n+        assert f_sym(a, 2) == mpmath.matrix([[2, 2], [3, 5]])\n+\n+        # Case 2: Concrete dimension in expression\n+        f_con = lambdify(B, B + Id2, modules='mpmath')\n+        assert f_con(a) == mpmath.matrix([[2, 2], [3, 5]])\n+\n+        # Case 4: Ensure sympy.I still works\n+        f_I = lambdify(x, x + I, modules='mpmath')\n+        assert f_I(1) == mpmath.mpc(1, 1) # mpmath complex number\n+\n+    # Tensorflow test could be added here if needed\n```\n"}
{"instance_id": "sympy__sympy-17139", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/simplify/fu.py\n+++ b/sympy/simplify/fu.py\n@@ -501,7 +501,7 @@\n         #         return rv\n         #     return nrv\n         # this was causing problems in test_trigsimp:\n-        if (rv.exp < 0) == True:\n+        if rv.exp.is_real and rv.exp < 0:\n             nrv = f1(rv.base.args[0])**rv.exp\n             if nrv != rv:\n                 return nrv # pragma: no cover\n```\n"}
{"instance_id": "sympy__sympy-17630", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/matrices/expressions/blockmatrix.py\n+++ b/sympy/matrices/expressions/blockmatrix.py\n@@ -110,6 +110,8 @@\n \n \n def bc_matmul(expr):\n+    from sympy.matrices.expressions.matexpr import ZeroMatrix\n+    from sympy import sympify\n     if isinstance(expr, MatPow):\n         if expr.args[1].is_Integer:\n             factor, matrices = (1, [expr.args[0]]*expr.args[1])\n@@ -124,7 +126,24 @@\n     while (i+1 < len(matrices)):\n         A, B = matrices[i:i+2]\n         if isinstance(A, BlockMatrix) and isinstance(B, BlockMatrix):\n-            matrices[i] = A._blockmul(B)\n+            # Calculate the product\n+            prod = A._blockmul(B)\n+\n+            # Ensure that scalar Zero blocks are replaced by ZeroMatrix\n+            # This prevents errors in subsequent multiplications where\n+            # block sizes are needed (e.g., accessing .rows/.cols on blocks)\n+            # The simplification to scalar Zero happens because BlockMatrix\n+            # uses ImmutableDenseMatrix internally, which simplifies ZeroMatrix.\n+            if isinstance(prod, BlockMatrix):\n+                new_blocks_list = prod.blocks.tolist() # Get blocks as list of lists\n+                rows, cols = prod.blockshape\n+                A_row_sizes = A.rowblocksizes\n+                B_col_sizes = B.colblocksizes\n+                for r in range(rows):\n+                    for c in range(cols):\n+                        if new_blocks_list[r][c] == 0: # Check for scalar Zero\n+                            new_blocks_list[r][c] = ZeroMatrix(sympify(A_row_sizes[r]), sympify(B_col_sizes[c]))\n+                matrices[i] = BlockMatrix(new_blocks_list) # Recreate with ZeroMatrix blocks\n+            else:\n+                matrices[i] = prod # Use original product if not BlockMatrix\n             matrices.pop(i+1)\n         elif isinstance(A, BlockMatrix):\n             matrices[i] = A._blockmul(BlockMatrix([[B]]))\n```\n"}
{"instance_id": "sympy__sympy-17655", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/geometry/point.py\n+++ b/sympy/geometry/point.py\n@@ -21,6 +21,7 @@\n \n from sympy.core import S, sympify, Expr\n from sympy.core.compatibility import is_sequence\n+from sympy.core.mul import Mul\n from sympy.core.containers import Tuple\n from sympy.simplify import nsimplify, simplify\n from sympy.geometry.exceptions import GeometryError\n@@ -215,11 +216,30 @@\n \n         \"\"\"\n         try:\n-            s, o = Point._normalize_dimension(self, Point(other, evaluate=False))\n+            pt_to_add = other\n+            if isinstance(other, Mul):\n+                # Handle cases like Point + scalar*Point\n+                # where `other` is Mul(scalar, Point)\n+                is_scalar_point_mul = False\n+                if len(other.args) == 2:\n+                    args = list(other.args)\n+                    if isinstance(args[0], Point) and not isinstance(args[1], Point):\n+                        is_scalar_point_mul = True\n+                        pt = args[0]\n+                        scalar = args[1]\n+                    elif isinstance(args[1], Point) and not isinstance(args[0], Point):\n+                        is_scalar_point_mul = True\n+                        pt = args[1]\n+                        scalar = args[0]\n+\n+                if is_scalar_point_mul:\n+                    try:\n+                        # Evaluate scalar * Point\n+                        pt_to_add = pt * scalar\n+                    except Exception:\n+                        pass # Fall back to original behavior\n+            s, o = Point._normalize_dimension(self, Point(pt_to_add, evaluate=False))\n         except TypeError:\n-            raise GeometryError(\"Don't know how to add {} and a Point object\".format(other))\n-\n-        coords = [simplify(a + b) for a, b in zip(s, o)]\n-        return Point(coords, evaluate=False)\n+            # Use original `other` in error message for clarity\n+            raise GeometryError(\"Don't know how to add {} and a Point object\".format(other))\n\n+        coords = [simplify(a + b) for a, b in zip(s, o)]\n+        return Point(coords, evaluate=False)\n\n     def __contains__(self, item):\n         return item in self.args\n@@ -264,11 +284,30 @@\n \n     def __sub__(self, other):\n         \"\"\"Subtract other from self.\"\"\"\n+        # Note: This logic mirrors __add__ to handle Point - scalar*Point\n         try:\n-            s, o = Point._normalize_dimension(self, Point(other, evaluate=False))\n+            pt_to_sub = other\n+            if isinstance(other, Mul):\n+                # Handle cases like Point - scalar*Point\n+                # where `other` is Mul(scalar, Point)\n+                is_scalar_point_mul = False\n+                if len(other.args) == 2:\n+                    args = list(other.args)\n+                    if isinstance(args[0], Point) and not isinstance(args[1], Point):\n+                        is_scalar_point_mul = True\n+                        pt = args[0]\n+                        scalar = args[1]\n+                    elif isinstance(args[1], Point) and not isinstance(args[0], Point):\n+                        is_scalar_point_mul = True\n+                        pt = args[1]\n+                        scalar = args[0]\n+\n+                if is_scalar_point_mul:\n+                    try:\n+                        pt_to_sub = pt * scalar\n+                    except Exception:\n+                        pass\n+            s, o = Point._normalize_dimension(self, Point(pt_to_sub, evaluate=False))\n         except TypeError:\n-            raise GeometryError(\"Don't know how to subtract {} from a Point object\".format(other))\n-\n-        coords = [simplify(a - b) for a, b in zip(s, o)]\n-        return Point(coords, evaluate=False)\n+            # Use original `other` in error message for clarity\n+            raise GeometryError(\"Don't know how to subtract {} from a Point object\".format(other))\n\n+        coords = [simplify(a - b) for a, b in zip(s, o)]\n+        return Point(coords, evaluate=False)\n\n     def __truediv__(self, divisor):\n         \"\"\"Divide point's coordinates by a factor.\"\"\"\n```\n"}
{"instance_id": "sympy__sympy-18057", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/core/sympify.py\n+++ b/sympy/core/sympify.py\n@@ -448,11 +448,13 @@\n     if hasattr(a, '_sympy_'):\n         return a._sympy_()\n \n-    if strict:\n-        raise SympifyError(a)\n-\n-    # XXX: Is this needed? Returning the object seems problematic\n-    # return a\n+    # If we reached here, 'a' is an unknown object without _sympy_\n+    # Raise an error for all unknown types, regardless of strict mode,\n+    # to prevent potential security issues with repr() or unexpected behavior.\n+    raise SympifyError(\n+        \"Cannot sympify %r of type %s. Sympy does not know how to convert this object. \"\n+        \"Add a _sympy_ method to the class, or ensure the object is of a known type \"\n+        \"(e.g., number, string, Sympy object).\" % (a, type(a).__name__))\n \n def _sympify(a):\n     \"\"\"Short version of sympify for internal usage.\n```\n"}
{"instance_id": "sympy__sympy-18087", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-18189", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/solvers/diophantine.py\n+++ b/sympy/solvers/diophantine.py\n@@ -100,9 +100,9 @@\n                     'syms should be given as a sequence, e.g. a list')\n             syms = [i for i in syms if i in var]\n             if syms != var:\n-                dict_sym_index = dict(zip(syms, range(len(syms))))\n+                # Solve using default var order (passing permute), then reorder\n+                dict_sym_index = dict(zip(syms, range(len(syms)))) # Map desired symbols to desired indices\n                 return {tuple([t[dict_sym_index[i]] for i in var])\n-                            for t in diophantine(eq, param)}\n+                            for t in diophantine(eq, param, permute=permute)}\n         n, d = eq.as_numer_denom()\n         if n.is_number:\n             return set()\n```\n"}
{"instance_id": "sympy__sympy-18199", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/ntheory/residue_ntheory.py\n+++ b/sympy/ntheory/residue_ntheory.py\n@@ -101,10 +101,18 @@\n     \"\"\"\n     from sympy.core.numbers import igcdex\n     a, n, p = as_int(a), as_int(n), as_int(p)\n+\n+    # Handle the trivial case x = 0 mod p\n+    a_mod_p = a % p\n+    if a_mod_p == 0:\n+        if all_roots:\n+            return [0]\n+        else:\n+            return 0\n+\n     if n == 2:\n         return sqrt_mod(a, p, all_roots)\n     # see Hackman \"Elementary Number Theory\" (2009), page 76\n-    if not is_nthpow_residue(a, n, p):\n+    if not is_nthpow_residue(a_mod_p, n, p): # Check residue for a mod p\n         return None\n     if not isprime(p):\n         raise NotImplementedError(\"Not implemented for composite p\")\n@@ -112,13 +120,13 @@\n     if (p - 1) % n == 0:\n         # _nthroot_mod1 assumes q | p-1 and finds roots of x**q = s mod p\n         # s should be a mod p\n-        return _nthroot_mod1(a, n, p, all_roots)\n+        return _nthroot_mod1(a_mod_p, n, p, all_roots)\n     # The roots of ``x**n - a = 0 (mod p)`` are roots of\n     # ``gcd(x**n - a, x**(p - 1) - 1) = 0 (mod p)``\n     pa = n\n     pb = p - 1\n     b = 1\n-    if pa < pb:\n+    a = a_mod_p # Use a mod p for the gcd algorithm ('a' represents the constant term)\n+    if pa < pb: # pragma: no cover\n         a, pa, b, pb = b, pb, a, pa\n     while pb:\n         # x**pa - a = 0; x**pb - b = 0\n@@ -136,12 +144,12 @@\n         res = [a]\n     else:\n         res = a\n-    elif pa == 2:\n+    elif pa == 2: # pragma: no cover\n         return sqrt_mod(a, p , all_roots)\n     else:\n         res = _nthroot_mod1(a, pa, p, all_roots)\n     return res\n-\n+\n \n def quadratic_residues(p):\n     \"\"\"\n```\n"}
{"instance_id": "sympy__sympy-18532", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/core/basic.py\n+++ b/sympy/core/basic.py\n@@ -21,14 +21,16 @@\n \n         \"\"\"\n         if types:\n-            types = tuple(\n+            _types = tuple(\n                 [t if isinstance(t, type) else type(t) for t in types])\n+            check = lambda expr: isinstance(expr, _types)\n         else:\n-            types = (Atom,)\n+            # Default behavior: objects with no args\n+            check = lambda expr: not expr.args\n+\n         result = set()\n         for expr in preorder_traversal(self):\n-            if isinstance(expr, types):\n+            if check(expr):\n                 result.add(expr)\n         return result\n \n--- a/sympy/matrices/common.py\n+++ b/sympy/matrices/common.py\n@@ -14,9 +14,6 @@\n         {x}\n         \"\"\"\n \n-        types = tuple(t if isinstance(t, type) else type(t) for t in types)\n-        if not types:\n-            types = (Atom,)\n         return self._eval_atoms(*types)\n \n     @property\n```\n"}
{"instance_id": "sympy__sympy-18621", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/matrices/expressions/blockmatrix.py\n+++ b/sympy/matrices/expressions/blockmatrix.py\n@@ -221,6 +221,41 @@\n         else:\n             return BlockMatrix._blockadd(self, other)\n \n+    def _entry(self, i, j, **kwargs):\n+        from sympy.core.singleton import S\n+        from sympy.matrices.matrices import MatrixBase\n+\n+        # Find row block index k\n+        row_offset = 0\n+        for k_row, rows in enumerate(self.rowblocksizes):\n+            if i < row_offset + rows:\n+                break\n+            row_offset += rows\n+        else:\n+            # This should not happen if i is within bounds\n+            raise IndexError(\"Row index out of bounds: %d >= %d\" % (i, self.rows))\n+\n+        # Find col block index k\n+        col_offset = 0\n+        for k_col, cols in enumerate(self.colblocksizes):\n+            if j < col_offset + cols:\n+                break\n+            col_offset += cols\n+        else:\n+            # This should not happen if j is within bounds\n+            raise IndexError(\"Column index out of bounds: %d >= %d\" % (j, self.cols))\n+\n+        # Check if indices fall in the same diagonal block\n+        if k_row != k_col:\n+            return S.Zero\n+\n+        # Indices fall in block k=k_row=k_col\n+        k = k_row\n+        block = self.args[k]\n+        i_local = i - row_offset\n+        j_local = j - col_offset\n+\n+        # Return the entry from the specific block\n+        if hasattr(block, '_entry'):\n+            # Use _entry for MatrixExpr using same kwargs\n+            return block._entry(i_local, j_local, **kwargs)\n+        else:\n+            # Use standard indexing for explicit matrices or other objects\n+            # Allow standard exceptions (e.g. IndexError) to propagate\n+            return block[i_local, j_local]\n+\n \n def block_collapse(expr):\n     \"\"\"Evaluates a block matrix expression\n```\n"}
{"instance_id": "sympy__sympy-18698", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/polys/sqfreetools.py\n+++ b/sympy/polys/sqfreetools.py\n@@ -100,6 +100,10 @@\n     # Wang/Trager modification of Yun's algorithm.\n     # See https://doi.org/10.1145%2F355722.355729 and Zippel, Factorization chapter.\n     # Algorithm:\n+    # f = F_1 * F_2**2 * F_3**3 * ... * F_k**k\n+    # The algorithm computes F_1, F_2, ..., F_k where each F_i is square-free\n+    # and contains the product of all irreducible factors of f that have multiplicity i.\n+    # The result is [(F_1, 1), (F_2, 2), ..., (F_k, k)].\n     # F = sqf_part(f)\n     # G = f / F\n     # i = 1\n@@ -127,6 +131,10 @@\n         h = dup_quo(F, g, K)    # h = F_i\n \n         if all or dup_degree(h) > 0:\n+            # h is the product of all irreducible factors with multiplicity i.\n+            # It should NOT be factored further at this stage.\n+            # E.g., if f = (x-2)**3 * (x-3)**3, then for i=3,\n+            # h should be (x-2)*(x-3) = x**2 - 5*x + 6.\n             result.append((h, i))\n \n         if dup_degree(g) < 0: # Check if g is constant (degree < 0 for DMP) or zero poly\n```\n"}
{"instance_id": "sympy__sympy-18835", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/utilities/iterables.py\n+++ b/sympy/utilities/iterables.py\n@@ -66,41 +66,66 @@\n     return False\n \n \n-def uniq(seq, result=None):\n+def uniq(seq):\n     \"\"\"\n-    Yield unique elements from ``seq`` as an iterator. The second\n-    parameter ``result``  is used internally; it is not necessary to pass\n-    anything for this.\n+    Yield unique elements from ``seq`` in order, preserving original order.\n+\n+    Handles both hashable and unhashable types.\n \n     Examples\n     ========\n \n     >>> from sympy.utilities.iterables import uniq\n     >>> dat = [1, 4, 1, 5, 4, 2, 1, 2]\n-    >>> type(uniq(dat)) in (list, tuple)\n-    False\n-\n     >>> list(uniq(dat))\n     [1, 4, 5, 2]\n     >>> list(uniq(x for x in dat))\n     [1, 4, 5, 2]\n     >>> list(uniq([[1], [2, 1], [1]]))\n     [[1], [2, 1]]\n+\n+    Modification of the sequence during iteration is detected for\n+    sized sequences:\n+\n+    >>> f = list('THISTLE')\n+    >>> it = uniq(f)\n+    >>> next(it)\n+    'T'\n+    >>> f.remove('T') # Modify the list\n+    >>> next(it) # doctest: +IGNORE_EXCEPTION_DETAIL\n+    Traceback (most recent call last):\n+      ...\n+    RuntimeError: list changed size during iteration\n     \"\"\"\n+    # Check if sequence size can be tracked\n+    initial_len = -1\n+    check_len = False\n     try:\n-        seen = set()\n-        result = result or []\n-        for i, s in enumerate(seq):\n-            if not (s in seen or seen.add(s)):\n+        initial_len = len(seq)\n+        check_len = True\n+    except TypeError:\n+        pass # Cannot check length (e.g., generator)\n+\n+    seen_hashable = set()\n+    seen_unhashable = []\n+\n+    # Use explicit iterator and loop with `next`\n+    seq_iterator = iter(seq)\n+\n+    while True:\n+        # Check for size change before getting the next item\n+        if check_len:\n+            try:\n+                current_len = len(seq)\n+                if current_len != initial_len:\n+                    try:\n+                        tname = type(seq).__name__\n+                    except AttributeError:\n+                        tname = \"sequence\"\n+                    raise RuntimeError(\"%s changed size during iteration\" % tname)\n+            except TypeError:\n+                # If len() worked initially but fails now, stop checking\n+                check_len = False\n+\n+        # Get next item\n+        try:\n+            s = next(seq_iterator)\n+        except StopIteration:\n+            break # End of sequence\n+\n+        # Try using the set for hashable items\n+        try:\n+            if s not in seen_hashable:\n+                seen_hashable.add(s)\n+                seen_unhashable.append(s) # Keep track for unhashable checks\n                 yield s\n-    except TypeError:\n-        if s not in result:\n-            yield s\n-            result.append(s)\n-        if hasattr(seq, '__getitem__'):\n-            for s in uniq(seq[i + 1:], result):\n+        except TypeError: # Unhashable item\n+            if s not in seen_unhashable:\n+                seen_unhashable.append(s)\n                 yield s\n-        else:\n-            for s in uniq(seq, result):\n-                yield s\n \n \n def generate_bell(n):\n```\n"}
{"instance_id": "sympy__sympy-19007", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/matrices/expressions/blockmatrix.py\n+++ b/sympy/matrices/expressions/blockmatrix.py\n@@ -1,11 +1,12 @@\n from __future__ import print_function, division\n \n from sympy import ask, Q\n-from sympy.core import Basic, Add\n+from sympy.core import Basic, Add, sympify\n from sympy.strategies import typed, exhaust, condition, do_one, unpack\n from sympy.strategies.traverse import bottom_up\n from sympy.utilities import sift\n from sympy.utilities.misc import filldedent\n+from sympy.core.numbers import Integer # Use SymPy Integer for type checks\n \n from sympy.matrices.expressions.matexpr import MatrixExpr, ZeroMatrix, Identity, MatrixElement\n from sympy.matrices.expressions.matadd import MatAdd\n@@ -14,7 +15,6 @@\n from sympy.matrices.expressions.transpose import Transpose\n from sympy.matrices.expressions.inverse import Inverse\n from sympy.matrices.expressions.special import GenericZeroMatrix\n-from sympy.matrices.matrices import MatrixBase\n \n \n class BlockMatrix(MatrixExpr):\n@@ -173,18 +172,56 @@\n         return self.args[0]\n \n     def _entry(self, i, j, **kwargs):\n-        # Find row entry\n-        row_block = 0\n-        for numrows in self.rowblocksizes:\n-            if (i < numrows) != False:\n+        i, j = sympify(i), sympify(j)\n+\n+        # If indices are symbolic (not concrete integers), return unevaluated MatrixElement\n+        if not i.is_Integer or not j.is_Integer:\n+            # TODO: Add simplification logic? E.g., if i=0 and block (0,0) is ZeroMatrix.\n+            # For now, return the abstract element, which is the required fix.\n+            return MatrixElement(self, i, j)\n+\n+        # Indices are concrete integers, convert to Python int for comparisons\n+        i_int, j_int = int(i), int(j)\n+\n+        # Store original SymPy Integer values for potential MatrixElement return\n+        # if block sizes are symbolic.\n+        i_orig, j_orig = i, j\n+\n+        # Find the row block index and the relative row index\n+        i_current = i_int\n+        row_block_idx = -1\n+        for idx, numrows in enumerate(self.rowblocksizes):\n+            # Check if the block size is symbolic\n+            if not numrows.is_Integer:\n+                # Cannot determine the block, return unevaluated MatrixElement\n+                return MatrixElement(self, i_orig, j_orig)\n+\n+            numrows_int = int(numrows)\n+            # Compare index with block size\n+            if i_current < numrows_int:\n+                row_block_idx = idx\n+                i_relative = i_current # Relative index within the block\n                 break\n             else:\n-                i -= numrows\n-                row_block += 1\n-        # Find col entry\n-        col_block = 0\n-        for numcols in self.colblocksizes:\n-            if (j < numcols) != False:\n+                i_current -= numrows_int\n+\n+        # Check if row index is out of bounds\n+        if row_block_idx == -1:\n+             # Calculate total rows if possible, otherwise provide less specific message\n+             total_rows = Add(*self.rowblocksizes)\n+             if total_rows.is_Integer:\n+                 raise IndexError(\"Row index {} out of bounds (total rows = {})\".format(i_int, int(total_rows)))\n+             else:\n+                 raise IndexError(\"Row index {} possibly out of bounds\".format(i_int))\n+\n+        # Find the column block index and the relative column index\n+        j_current = j_int\n+        col_block_idx = -1\n+        for idx, numcols in enumerate(self.colblocksizes):\n+            # Check if the block size is symbolic\n+            if not numcols.is_Integer:\n+                # Cannot determine the block, return unevaluated MatrixElement\n+                return MatrixElement(self, i_orig, j_orig)\n+\n+            numcols_int = int(numcols)\n+            # Compare index with block size\n+            if j_current < numcols_int:\n+                col_block_idx = idx\n+                j_relative = j_current # Relative index within the block\n                 break\n             else:\n-                j -= numcols\n-                col_block += 1\n-        return self.blocks[row_block, col_block][i, j]\n+                j_current -= numcols_int\n+\n+        # Check if column index is out of bounds\n+        if col_block_idx == -1:\n+             # Calculate total cols if possible\n+             total_cols = Add(*self.colblocksizes)\n+             if total_cols.is_Integer:\n+                 raise IndexError(\"Column index {} out of bounds (total cols = {})\".format(j_int, int(total_cols)))\n+             else:\n+                 raise IndexError(\"Column index {} possibly out of bounds\".format(j_int))\n+\n+        # Access the element within the identified block using relative indices\n+        target_block = self.blocks[row_block_idx, col_block_idx]\n+\n+        # Recursively call _entry on the sub-block\n+        return target_block._entry(i_relative, j_relative, **kwargs)\n \n     @property\n     def is_Identity(self):\n```\n"}
{"instance_id": "sympy__sympy-19254", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/polys/factortools.py\n+++ b/sympy/polys/factortools.py\n@@ -1,6 +1,7 @@\n \"\"\"Polynomial factorization tools\"\"\"\n \n from sympy.polys.densebasic import (\n+    dup_l2_norm,\n     dup_LC, dmp_LC, dmp_ground_LC,\n     dup_TC, dmp_TC, dmp_ground_TC,\n     dup_degree, dmp_degree, dmp_degree_in, dmp_degree_list,\n@@ -13,6 +14,7 @@\n )\n \n from sympy.polys.densetools import (\n+    dmp_l2_norm,\n     dmp_eval_in, dmp_diff_eval_in,\n     dmp_ground_primitive, dmp_ground_content,\n     dmp_ground_trunc,\n@@ -40,26 +42,30 @@\n     return _sort_factors(result)\n \n \n-def dup_zz_mignotte_bound(f, K):\n-    \"\"\"Mignotte bound for univariate polynomials in `K[x]`. \"\"\"\n-    a = dup_max_norm(f, K)\n-    b = abs(dup_LC(f, K))\n+def dup_zz_factor_bound(f, K):\n+    \"\"\"Knuth-Cohen bound for coefficients of factors of `f`.\n+\n+    For a univariate polynomial `f` in `Z[x]`, returns a bound `B`\n+    such that for any factor `g` of `f`, `||g||_inf <= B`.\n+\n+    The bound is computed as `2**n * ||f||_2`, where `n = deg(f)`\n+    and `||f||_2` is the Euclidean norm of `f`.\n+    \"\"\"\n     n = dup_degree(f)\n-\n-    return K.sqrt(K(n + 1))*2**n*a*b\n+    return K(2)**n * dup_l2_norm(f, K)\n \n \n-def dmp_zz_mignotte_bound(f, u, K):\n-    \"\"\"Mignotte bound for multivariate polynomials in `K[X]`. \"\"\"\n-    a = dmp_max_norm(f, u, K)\n-    b = abs(dmp_ground_LC(f, u, K))\n+def dmp_zz_factor_bound(f, u, K):\n+    \"\"\"Knuth-Cohen bound for coefficients of factors of `f`.\n+\n+    For a multivariate polynomial `f` in `Z[X]`, returns a bound `B`\n+    such that for any factor `g` of `f`, `||g||_inf <= B`.\n+\n+    The bound is computed as `2**n * ||f||_2`, where `n` is the\n+    total degree of `f` and `||f||_2` is the Euclidean norm of `f`.\n+    \"\"\"\n     n = sum(dmp_degree_list(f, u))\n-\n-    return K.sqrt(K(n + 1))*2**n*a*b\n-\n-\n-def dup_zz_hensel_step(m, f, g, h, s, t, K):\n+    return K(2)**n * dmp_l2_norm(f, u, K)\n+\n+\n+def dup_zz_hensel_step(m, f, g, h, s, t, K):\n     \"\"\"\n     One step in Hensel lifting in `Z[x]`.\n \n@@ -127,7 +133,7 @@\n     fc = f[-1]\n     A = dup_max_norm(f, K)\n     b = dup_LC(f, K)\n-    B = int(abs(K.sqrt(K(n + 1))*2**n*A*b))\n+    B = int(dup_zz_factor_bound(f, K))\n     C = int((n + 1)**(2*n)*A**(2*n - 1))\n     gamma = int(_ceil(2*_log(C, 2)))\n     bound = int(2*gamma*_log(gamma))\n```\n"}
{"instance_id": "sympy__sympy-19487", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/functions/elementary/complexes.py\n+++ b/sympy/functions/elementary/complexes.py\n@@ -87,3 +87,7 @@\n             if isinstance(arg, sign):\n                 return arg\n+\n+    def _eval_rewrite_as_Abs(self, arg, **kwargs):\n+        \"\"\"Rewrite sign(x) as x/Abs(x).\"\"\"\n+        return arg / Abs(arg)\n```\n"}
{"instance_id": "sympy__sympy-20049", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/physics/vector/point.py\n+++ b/sympy/physics/vector/point.py\n@@ -1,6 +1,7 @@\n from sympy.core.basic import Basic\n from sympy.core.compatibility import u\n from sympy.physics.vector.vector import Vector, _check_vector\n+from sympy.physics.vector.frame import ReferenceFrame, _check_frame\n \n __all__ = ['Point']\n \n@@ -26,6 +27,8 @@\n         self._acc_dict = {}\n         # Used for computation checking; see functions like v1pt_theory\n         self._pdlist = [self]\n+        # Stack to detect cycles during velocity calculation\n+        self._vel_calculation_stack = set()\n \n     def __str__(self):\n         return self.name\n@@ -155,27 +158,71 @@\n         return self.vel(outframe)\n \n     def vel(self, frame):\n-        \"\"\"The velocity Vector of this Point in the ReferenceFrame.\n+        \"\"\"The velocity Vector of this Point in the specified ReferenceFrame.\n+\n+        If the velocity has been defined previously using set_vel(), or\n+        calculated using one of the kinematic theorems (v1pt_theory,\n+        v2pt_theory), that vector is returned.\n+\n+        If the velocity has not been defined or calculated, the method\n+        attempts to calculate it based on the point's position relative to\n+        other points. If the position of this point (`self`) relative to\n+        another point `P` is known (`r = self.pos_from(P)`), and the velocity\n+        of `P` in the frame `F` (`P.vel(F)`) can be determined, then the\n+        velocity of `self` in `F` is calculated as:\n+\n+        `^F v^self = ^F v^P + ^F d/dt (r)`\n+\n+        This calculation is performed recursively up the chain of position\n+        dependencies until a point with a known velocity in the frame is\n+        reached, or until the velocity cannot be determined.\n \n         Parameters\n         ==========\n-\n         frame : ReferenceFrame\n             The frame in which the returned velocity vector will be defined in\n \n         Examples\n         ========\n-\n         >>> from sympy.physics.vector import Point, ReferenceFrame\n+        >>> from sympy.physics.vector import dynamicsymbols\n+        >>> from sympy.physics.vector import init_vprinting\n+        >>> init_vprinting(pretty_print=False)\n         >>> N = ReferenceFrame('N')\n         >>> p1 = Point('p1')\n         >>> p1.set_vel(N, 10 * N.x)\n         >>> p1.vel(N)\n         10*N.x\n+        >>> p2 = Point('p2')\n+        >>> q = dynamicsymbols('q')\n+        >>> p2.set_pos(p1, q * N.y) # Position of p2 relative to p1\n+        >>> p2.vel(N) # Automatically calculates p2's velocity in N\n+        10*N.x + q'*N.y\n+        >>> p3 = Point('p3')\n+        >>> p3.set_pos(p2, 5 * N.z) # Position of p3 relative to p2\n+        >>> p3.vel(N) # Calculates p3's velocity based on p2's velocity\n+        10*N.x + q'*N.y\n \n         \"\"\"\n-\n         _check_frame(frame)\n+        # Check if velocity is already known\n         if frame in self._vel_dict:\n             return self._vel_dict[frame]\n+\n+        # Guard against recursion: Initialize stack if needed, check for cycles.\n+        if not hasattr(self, '_vel_calculation_stack'):\n+            self._vel_calculation_stack = set()\n+        if frame in self._vel_calculation_stack:\n+            raise ValueError(f\"Cyclic dependency detected: Calculation of {self.name}.vel({frame.name}) \"\n+                             f\"depends on itself through position relationships.\")\n+\n+        self._vel_calculation_stack.add(frame)\n+\n+        try:\n+            # Try to calculate velocity using position relationships\n+            for otherpoint, pos_vec in self._pos_dict.items():\n+                try:\n+                    other_vel = otherpoint.vel(frame) # Recursive call\n+                    dpos_dt = pos_vec.dt(frame) # d/dt(pos_from(other)) in frame\n+                    calculated_vel = other_vel + dpos_dt\n+                    self.set_vel(frame, calculated_vel) # Store the calculated velocity\n+                    return self._vel_dict[frame]\n+                except ValueError:\n+                    # Cannot determine velocity via this path (e.g., otherpoint.vel failed), try next point\n+                    continue\n+\n+            # If velocity cannot be calculated from any known position relation\n+            raise ValueError('Velocity of point ' + self.name + ' has not been'\n+                             ' defined or calculated in ReferenceFrame ' + frame.name)\n+        finally:\n+            # Ensure the frame is removed from the stack\n+            self._vel_calculation_stack.discard(frame)\n+\n+    def acc(self, frame):\n+        \"\"\"The acceleration Vector of this Point in the ReferenceFrame.\"\"\"\n+        _check_frame(frame)\n         if not (frame in self._vel_dict):\n             raise ValueError('Velocity of point ' + self.name + ' has not been'\n                               ' defined in ReferenceFrame ' + frame.name)\n```\n"}
{"instance_id": "sympy__sympy-20154", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/utilities/iterables.py\n+++ b/sympy/utilities/iterables.py\n@@ -19,22 +19,12 @@\n      {2: 1, 4: 1}\n      {3: 2}\n \n-     Note that the _same_ dictionary object is returned each time.\n-     This is for speed:  generating each partition goes quickly,\n-     taking constant time, independent of n.\n-\n      >>> [p for p in partitions(6, k=2)]\n-     [{1: 6}, {1: 6}, {1: 6}, {1: 6}]\n-\n-     If you want to build a list of the returned dictionaries then\n-     make a copy of them:\n-\n-     >>> [p.copy() for p in partitions(6, k=2)]  # doctest: +SKIP\n      [{2: 3}, {1: 2, 2: 2}, {1: 4, 2: 1}, {1: 6}]\n-     >>> [(M, p.copy()) for M, p in partitions(6, k=2, size=True)]  # doctest: +SKIP\n+     >>> [p for M, p in partitions(6, k=2, size=True)]  # doctest: +SKIP\n+     [{2: 3}, {1: 2, 2: 2}, {1: 4, 2: 1}, {1: 6}]\n+     >>> [M for M, p in partitions(6, k=2, size=True)]  # doctest: +SKIP\n      [(3, {2: 3}), (4, {1: 2, 2: 2}), (5, {1: 4, 2: 1}), (6, {1: 6})]\n-\n      References\n      ==========\n \n@@ -71,9 +61,9 @@\n          keys.append(r)\n      room = m - q - bool(r)\n      if size:\n-         yield sum(ms.values()), ms\n+         yield sum(ms.values()), ms.copy()\n      else:\n-         yield ms\n+         yield ms.copy()\n \n      while keys != [1]:\n          # Reuse any 1's.\n@@ -117,9 +107,9 @@\n              break\n          room -= need\n          if size:\n-             yield sum(ms.values()), ms\n+             yield sum(ms.values()), ms.copy()\n          else:\n-             yield ms\n+             yield ms.copy()\n \n \n def ordered_partitions(n, m=None, sort=True):\n```\n"}
{"instance_id": "sympy__sympy-20212", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/core/power.py\n+++ b/sympy/core/power.py\n@@ -158,6 +158,9 @@\n                     return S.NaN\n                 if b is S.ComplexInfinity:  # zoo**(-oo) is undefined\n                     return S.NaN\n+                # special case 0**(-oo) is zoo\n+                if b.is_zero:\n+                    return S.ComplexInfinity\n                 # Try to use abs(b) if possible, but it may fail for symbols\n                 # Cannot determine |b| for symbols for example\n                 try:\n```\n"}
{"instance_id": "sympy__sympy-20322", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-20442", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/physics/units/util.py\n+++ b/\n"}
{"instance_id": "sympy__sympy-20590", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/core/assumptions.py\n+++ b/sympy/core/assumptions.py\n@@ -16,6 +16,8 @@\n \n \n class AssumeMeths():\n+    __slots__ = ()\n+\n     \"\"\"\n     Methods for processing assumptions on Basic objects.\n     \"\"\"\n--- a/sympy/core/expr.py\n+++ b/sympy/core/expr.py\n@@ -34,6 +34,8 @@\n \n \n class Expr(Basic):\n+    __slots__ = ()\n+\n     \"\"\"Base class for mathematical expressions.\n \n     Superclass for Add, Mul, Pow, Function, Relational, ...\n@@ -115,6 +117,8 @@\n \n \n class AtomicExpr(Expr):\n+    __slots__ = ()\n+\n     \"\"\"A parent class for atomic things, e.g. Symbols and Numbers.\n     \"\"\"\n     is_Atom = True\n```\n"}
{"instance_id": "sympy__sympy-20639", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/pretty/pretty.py\n+++ b/sympy/printing/pretty/pretty.py\n@@ -1403,10 +1403,6 @@\n                 # print integer root\n                 return prettyForm(*root(e.base, pform_b))\n\n-            # Is the exponent 1/symbol?\n-            if isinstance(e, Pow) and e.base.is_Symbol and e.exp == -1:\n-                # print symbol root\n-                return prettyForm(*root(self._print(e.exp), pform_b))\n-\n         # Add parens if base requires them. Generally, we need parens\n         # if the base has lower precedence than power, e.g. -2**2 = -4, but\n         # (-2)**2 = 4. Also, if the base is complex, e.g. (1+I)**2.\n```\n"}
{"instance_id": "sympy__sympy-21055", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/assumptions/refine.py\n+++ b/sympy/assumptions/refine.py\n@@ -1,9 +1,10 @@\n from typing import Dict, Callable\n \n-from sympy.core import S, Add, Expr, Basic, Mul\n+from sympy.core import S, Add, Expr, Basic, Mul, pi\n from sympy.logic.boolalg import Boolean\n \n from sympy.assumptions import ask, Q  # type: ignore\n+from sympy.core.numbers import nan\n \n \n def refine(expr, assumptions=True):\n@@ -71,6 +72,49 @@\n     return None\n \n \n+def refine_arg(expr, assumptions):\n+    \"\"\"\n+    Handler for the arg() function.\n+\n+    Examples\n+    ========\n+\n+    >>> from sympy.assumptions.refine import refine_arg\n+    >>> from sympy import Symbol, Q, arg, I, pi, im\n+    >>> x = Symbol('x')\n+    >>> refine_arg(arg(x), Q.positive(x))\n+    0\n+    >>> refine_arg(arg(x), Q.negative(x))\n+    pi\n+    >>> y = Symbol('y', imaginary=True)\n+    >>> refine_arg(arg(y), Q.imaginary(y) & Q.positive(im(y)))\n+    pi/2\n+    >>> refine_arg(arg(y), Q.imaginary(y) & Q.negative(im(y)))\n+    -pi/2\n+    >>> refine_arg(arg(Symbol('z', zero=True)), Q.zero(Symbol('z', zero=True)))\n+    nan\n+    \"\"\"\n+    from sympy.functions.elementary.complexes import im\n+    arg_expr = expr.args[0]\n+\n+    if ask(Q.zero(arg_expr), assumptions):\n+        return nan\n+    if ask(Q.positive(arg_expr), assumptions):\n+        return S.Zero\n+    if ask(Q.negative(arg_expr), assumptions):\n+        return S.Pi\n+    if ask(Q.real(arg_expr), assumptions):\n+        if ask(Q.nonnegative(arg_expr), assumptions):\n+             return S.Zero\n+        if ask(Q.nonpositive(arg_expr), assumptions):\n+             return S.Pi\n+    if ask(Q.imaginary(arg_expr), assumptions):\n+        im_part = im(arg_expr)\n+        if ask(Q.positive(im_part), assumptions):\n+            return pi / 2\n+        if ask(Q.negative(im_part), assumptions):\n+            return -pi / 2\n+    return None\n+\n def refine_im(expr, assumptions):\n     \"\"\"\n     Handler for the im() function.\n@@ -146,7 +190,7 @@\n # Handler dictionary mapping classes to functions. Every function should\n # be of the form ``refine_FUNCTION(expr, assumptions)`` and return\n # the refined expression or ``None`` if it is not possible to refine it.\n-from sympy.functions.elementary.complexes import Abs, sign, re, im\n+from sympy.functions.elementary.complexes import Abs, sign, re, im, arg\n from sympy.functions.elementary.piecewise import Piecewise\n from sympy.logic.boolalg import Boolean\n from sympy.core.relational import Relational\n@@ -156,6 +200,7 @@\n     Abs: refine_Abs,\n     sign: refine_sign,\n     re: refine_re,\n+    arg: refine_arg,\n     im: refine_im,\n     Piecewise: refine_Piecewise,\n     Boolean: refine_Boolean,\n```\n"}
{"instance_id": "sympy__sympy-21171", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -1150,8 +1150,11 @@\n         return r\"\\delta\\left( %s \\right)\" % self._print(expr.args[0])\n \n     def _print_SingularityFunction(self, expr):\n+    def _print_SingularityFunction(self, expr, exp=None):\n         x, a, n = expr.args\n         tex = r\"{\\left\\langle %s - %s \\right\\rangle}^{%s}\" % (self._print(x), self._print(a), self._print(n))\n+        if exp is not None:\n+            tex = r\"\\left( %s \\right)^{%s}\" % (tex, exp)\n         return tex\n \n     def _print_Heaviside(self, expr, exp=None):\n```\n"}
{"instance_id": "sympy__sympy-21379", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-21612", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-21614", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/core/function.py\n+++ b/sympy/core/function.py\n@@ -2163,6 +2163,16 @@\n     # Note: This is important for Derivative subclasses\n     # to work with AppliedUndef. Otherwise Derivative specific\n     # properties are not available. See issue #15522.\n+\n+    @property\n+    def kind(self):\n+        \"\"\"\n+        Return the kind of the derivative.\n+\n+        The kind of the derivative is the same as the kind of the\n+        expression being differentiated.\n+        \"\"\"\n+        return self.expr.kind\n \n     def __new__(cls, expr, *variables, **kwargs):\n         from sympy.matrices.expressions.matexpr import MatrixExpr\n```\n"}
{"instance_id": "sympy__sympy-21627", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/functions/elementary/hyperbolic.py\n+++ b/sympy/functions/elementary/hyperbolic.py\n@@ -101,11 +101,13 @@\n         # cosh(z) = 0 iff z = I*(pi/2 + n*pi)\n         # We need to check if arg/I - pi/2 is a multiple of pi\n         res = (arg/S.ImaginaryUnit - S.Pi/2)/S.Pi\n-        if res.is_integer:\n+        check = res.is_integer\n+        if check is True:\n             return True\n-        if res.is_integer == False:\n+        if check is False:\n             return False\n+        # If check is None, we can't determine if it's zero\n+        return None\n \n     def _eval_is_positive(self):\n         # cosh(x+I*y) = cos(y)*cosh(x) + I*sin(y)*sinh(x)\n@@ -246,11 +248,13 @@\n         # sinh(z) = 0 iff z = n*pi*I\n         # Check if arg/(pi*I) is an integer\n         res = arg / (S.Pi * S.ImaginaryUnit)\n-        if res.is_integer:\n+        check = res.is_integer\n+        if check is True:\n             return True\n-        if res.is_integer == False:\n+        if check is False:\n             return False\n+        # If check is None, we can't determine if it's zero\n+        return None\n \n     def _eval_is_negative(self):\n         z = self.args[0]\n```\n"}
{"instance_id": "sympy__sympy-21847", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/polys/monomials.py\n+++ b/sympy/polys/monomials.py\n@@ -40,31 +40,29 @@\n         # Force to list in case of passed tuple or other incompatible collection\n         variables = list(variables) + [S.One]\n         if all(variable.is_commutative for variable in variables):\n-            monomials_list_comm = []\n+            generated = set()\n             for item in combinations_with_replacement(variables, max_degree):\n-                powers = dict()\n-                for variable in variables:\n-                    powers[variable] = 0\n-                for variable in item:\n-                    if variable != 1:\n-                        powers[variable] += 1\n-                if max(powers.values()) >= min_degree:\n-                    monomials_list_comm.append(Mul(*item))\n-            yield from set(monomials_list_comm)\n+                # Calculate total degree\n+                current_degree = sum(1 for v in item if v != S.One)\n+                # Check if total degree is within the desired range\n+                if current_degree >= min_degree:\n+                    monom = Mul(*item)\n+                    # Yield unique monomials\n+                    if monom not in generated:\n+                        yield monom\n+                        generated.add(monom)\n         else:\n-            monomials_list_non_comm = []\n+            generated = set()\n             for item in product(variables, repeat=max_degree):\n-                powers = dict()\n-                for variable in variables:\n-                    powers[variable] = 0\n-                for variable in item:\n-                    if variable != 1:\n-                        powers[variable] += 1\n-                if max(powers.values()) >= min_degree:\n-                    monomials_list_non_comm.append(Mul(*item))\n-            yield from set(monomials_list_non_comm)\n+                # Calculate total degree\n+                current_degree = sum(1 for v in item if v != S.One)\n+                # Check if total degree is within the desired range\n+                if current_degree >= min_degree:\n+                    monom = Mul(*item)\n+                    # Yield unique monomials\n+                    if monom not in generated:\n+                         yield monom\n+                         generated.add(monom)\n     else:\n         # Case where max_degrees and min_degrees are lists (per variable degree)\n         if any(i < 0 for i in max_degrees):\n```\n"}
{"instance_id": "sympy__sympy-22005", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n<patch>\n--- a/sympy/solvers/polysys.py\n+++ b/sympy/solvers/polysys.py\n@@ -216,7 +216,15 @@ def _solve_reduced_system(system, gens, entry=False):\n             return []\n\n         if len(basis) == 1:\n-            return [(zero,) for zero in zeros]\n+            # Basis consists solely of f, which is univariate in gen.\n+            if len(gens) == 1:\n+                # Univariate system, univariate basis. Correct case.\n+                return [(zero,) for zero in zeros]\n+            else:\n+                # Basis is [f], f univariate in gen, but len(gens) > 1. Infinite solutions.\n+                raise NotImplementedError(filldedent('''\n+                    only zero-dimensional systems supported\n+                    (finite number of solutions)'''))\n\n         solutions = []\n\n--- a/sympy/solvers/tests/test_polysys.py\n+++ b/sympy/solvers/tests/test_polysys.py\n@@ -48,6 +48,10 @@ def test_solve_poly_system():\n     raises(NotImplementedError, lambda: solve_poly_system(\n         [z, -2*x*y**2 + x + y**2*z, y**2*(-z - 4) + 2]))\n     raises(PolynomialError, lambda: solve_poly_system([1/x], x))\n+    # Test detection of infinite solutions (non-zero-dimensional systems)\n+    raises(NotImplementedError, lambda: solve_poly_system(\n+        (x - 1,), x, y))\n+    raises(NotImplementedError, lambda: solve_poly_system(\n+        (y - 1,), x, y))\n\n\n def test_solve_biquadratic():\n</patch>\n```\n"}
{"instance_id": "sympy__sympy-22714", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-22840", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
{"instance_id": "sympy__sympy-23117", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/tensor/array/ndim_array.py\n+++ b/sympy/tensor/array/ndim_array.py\n@@ -31,14 +31,17 @@\n     @classmethod\n     def _scan_iterable_shape(cls, iterable):\n         def f(pointer):\n+            # Check if element is scalar:\n             if not isinstance(pointer, Iterable):\n                 return [pointer], ()\n \n+            # Check if sub-iterable is empty:\n+            if not pointer:\n+                # Represents a dimension of size 0.\n+                # E.g. [] gets shape (0,), [[]] gets shape (1, 0), [[], []] gets shape (2, 0).\n+                return [], (0,)\n+\n             result = []\n             elems, shapes = zip(*[f(i) for i in pointer])\n             if len(set(shapes)) != 1:\n                 raise ValueError(\"could not determine shape unambiguously\")\n             for i in elems:\n```\n"}
{"instance_id": "sympy__sympy-23191", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/physics/vector/vector.py\n+++ b/sympy/physics/vector/vector.py\n@@ -17,79 +17,68 @@\n \n     def _pretty(self, printer):\n         \"\"\"Pretty Printing method. \"\"\"\n-        from sympy.printing.pretty.stringpict import prettyForm\n+        # Delayed import to avoid cyclic imports\n+        from sympy.printing.pretty.stringpict import prettyForm, stringPict\n+        from sympy.core.add import Add\n+        from sympy.core.numbers import S\n+\n         e = self\n+        # Get the arguments (list of tuples (measure_number, basis_vector))\n+        ar = e.args\n \n-        class Fake:\n+        # Handle the zero vector case\n+        if len(ar) == 0:\n+            # Use printer._print to ensure styles/settings are applied correctly\n+            return printer._print(S.Zero)\n \n-            def render(self, *args, **kwargs):\n-                ar = e.args  # just to shorten things\n-                if len(ar) == 0:\n-                    return str(0)\n-                pforms = []  # output list, to be concatenated to a string\n-                for i, v in enumerate(ar):\n-                    for j in 0, 1, 2:\n-                        # if the coef of the basis vector is 1, we skip the 1\n-                        if ar[i][0][j] == 1:\n-                            pform = printer._print(ar[i][1].pretty_vecs[j])\n-                        # if the coef of the basis vector is -1, we skip the 1\n-                        elif ar[i][0][j] == -1:\n-                            pform = printer._print(ar[i][1].pretty_vecs[j])\n-                            pform = prettyForm(*pform.left(\" - \"))\n-                            bin = prettyForm.NEG\n-                            pform = prettyForm(binding=bin, *pform)\n-                        elif ar[i][0][j] != 0:\n-                            # If the basis vector coeff is not 1 or -1,\n-                            # we might wrap it in parentheses, for readability.\n-                            pform = printer._print(ar[i][0][j])\n+        pforms = []  # List to hold prettyForm objects for each non-zero term\n+        for i, v in enumerate(ar): # Iterate through coordinate systems\n+            for j in 0, 1, 2: # Iterate through x, y, z components\n+                coeff = ar[i][0][j] # The coefficient (measure number)\n+                bvec_sym = ar[i][1].pretty_vecs[j] # The basis vector symbol (e.g., i_C)\n \n-                            if isinstance(ar[i][0][j], Add):\n-                                tmp = pform.parens()\n-                                pform = prettyForm(tmp[0], tmp[1])\n+                # Skip terms with zero coefficient\n+                if coeff == 0:\n+                    continue\n \n-                            pform = prettyForm(*pform.right(\n+                # Get the stringPict representation of the basis vector\n+                bvec_pform = printer._print(bvec_sym)\n+\n+                if coeff == 1:\n+                    # Coefficient is 1, term is just the basis vector\n+                    pform = bvec_pform\n+                    # Set binding for multiplication (it's a positive term)\n+                    pform = prettyForm(pform, binding=prettyForm.MUL)\n+                elif coeff == -1:\n+                    # Coefficient is -1, term is \"- bvec\"\n+                    # Prepend \" - \" to the basis vector stringPict\n+                    pform = bvec_pform.left(\" - \")\n+                    # Set binding for a negative term\n+                    pform = prettyForm(pform, binding=prettyForm.NEG)\n+                else:\n+                    # General coefficient (non-zero, non +/- 1)\n+                    # Get the stringPict representation of the coefficient\n+                    coeff_pform = printer._print(coeff)\n+\n+                    # Wrap the coefficient in parentheses if it's an Add instance\n+                    # to ensure correct precedence, e.g., (a+b)*i\n+                    if isinstance(coeff, Add):\n+                        # stringPict.parens() returns (open_paren, content, close_paren)\n+                        # stringPict.next joins them horizontally\n+                        coeff_pform = stringPict.next(*coeff_pform.parens())\n+\n+                    # Combine the coefficient and basis vector stringPicts horizontally\n+                    # with a space in between. Use `right` for proper alignment.\n+                    pform = coeff_pform.right(\" \", bvec_pform)\n+\n+                    # Determine the binding based on the coefficient's sign\n+                    # This helps prettyForm.__add__ place '+' or '-' correctly.\n+                    if coeff.is_negative:\n+                        binding = prettyForm.NEG\n+                    else:\n+                        binding = prettyForm.MUL\n+\n+                    # Create the final prettyForm for this term with the calculated binding\n+                    pform = prettyForm(pform, binding=binding)\n+\n+                pforms.append(pform)\n+\n+        # If all terms were zero (vector is zero)\n+        if not pforms:\n+            return printer._print(S.Zero)\n+\n+        # Join the individual term prettyForms using prettyForm.__add__\n+        # This method handles adding \" + \" or \" - \" between terms based on binding.\n+        final_pform = prettyForm.__add__(*pforms)\n+        return final_pform\n```\n"}
{"instance_id": "sympy__sympy-23262", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/utilities/lambdify.py\n+++ b/sympy/utilities/lambdify.py\n@@ -651,15 +651,18 @@\n     from sympy.utilities.iterables import NotIterable, iterable\n\n     if isinstance(arg, (list, tuple, Tuple)):\n+        # Recursively apply to elements\n+        elements_str = [_recursive_to_string(doprint, e) for e in arg]\n+\n         if isinstance(arg, list):\n             left, right = '[]'\n+            content = ', '.join(elements_str)\n         elif isinstance(arg, (tuple, Tuple)):\n             left, right = \"()\"\n-        else:\n-            # This branch is technically unreachable due to the outer if condition\n-            raise NotImplementedError(\"unhandled type: %s, %s\" % (type(arg), arg))\n-        # TODO: Rework this function, it's hacky\n-        # Check for single element tuple\n-        return left +', '.join(_recursive_to_string(doprint, e) for e in arg) + right\n+            if len(arg) == 1:\n+                content = elements_str[0] + ',' # Add trailing comma for single element tuple\n+            else:\n+                content = ', '.join(elements_str)\n+        return left + content + right\n     elif isinstance(arg, str):\n         return arg\n     else:\n```\n"}
{"instance_id": "sympy__sympy-24066", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/physics/units/unitsystem.py\n+++ b/sympy/physics/units/unitsystem.py\n@@ -1,12 +1,17 @@\n from typing import Dict as tDict, Optional, Set as tSet, Tuple as tTuple, Union as tUnion\n \n-from sympy import Expr, S, Add, Mul, Pow, Function, Derivative\n+from sympy import Expr, S, Add, Mul, Pow, Function, Derivative, Abs\n from sympy.core.sympify import sympify\n from sympy.physics.units.dimensions import Dimension, DimensionSystem\n from sympy.physics.units.quantities import Quantity\n from sympy.utilities.exceptions import SymPyDeprecationWarning\n from sympy.utilities.iterables import sift\n from sympy.utilities.typing import Set as tSet\n+\n+# Imports needed for Function handling in _collect_factor_and_dimension\n+from sympy.functions.elementary.transcendental import (\n+    Log, Exp, HyperbolicFunction) # Add others like LambertW?\n+from sympy.functions.elementary.trigonometric import TrigonometricFunction\n \n \n class UnitSystem:\n@@ -147,11 +152,36 @@\n                 dim /= idim**count\n             return factor, dim\n         elif isinstance(expr, Function):\n-            fds = [self._collect_factor_and_dimension(\n-                arg) for arg in expr.args]\n-            return (expr.func(*(f[0] for f in fds)),\n-                    *(d[1] for d in fds))\n+            # Process arguments first\n+            arg_factors = []\n+            arg_dims = []\n+            for arg in expr.args:\n+                factor, dim = self._collect_factor_and_dimension(arg)\n+                arg_factors.append(factor)\n+                arg_dims.append(dim)\n+\n+            # Apply dimension rules based on function type\n+            # Functions requiring dimensionless arguments (e.g., exp, sin, cos, log):\n+            # Add other relevant classes like inverse trig/hyperbolic, etc. if necessary\n+            if isinstance(expr, (Exp, Log, TrigonometricFunction, HyperbolicFunction)):\n+                # Assuming unary functions for now, extend if needed.\n+                if len(expr.args) != 1:\n+                    # TODO: Handle multi-argument functions like log(x, b)?\n+                    # For now, raise error or assume all args must be dimensionless?\n+                    # Let's assume all args must be dimensionless for now.\n+                    pass # Fall through to check all args below\n+\n+                for i, arg_dim in enumerate(arg_dims):\n+                    if not self.get_dimension_system().is_dimensionless(arg_dim):\n+                        raise ValueError(\n+                            f\"Argument {i} of function '{expr.func.__name__}' must be dimensionless, \"\n+                            f\"but argument '{expr.args[i]}' has dimension {arg_dim}\")\n+                # Result is dimensionless\n+                result_factor = expr.func(*arg_factors)\n+                result_dim = Dimension(1)\n+                return result_factor, result_dim\n+            elif isinstance(expr, Abs):\n+                if len(expr.args) != 1:\n+                    raise ValueError(\"Abs takes exactly one argument\")\n+                # Result has the same dimension as the argument\n+                return Abs(arg_factors[0]), arg_dims[0]\n+            # Add handlers for other specific functions here (e.g., sqrt is handled by Pow)\n+            else:\n+                # Default for unknown functions: raise an error as behavior is undefined.\n+                raise TypeError(f\"Cannot determine dimension rules for Function '{expr.func.__name__}'.\")\n         elif isinstance(expr, Dimension):\n             return S.One, expr\n         else:\n```\n"}
{"instance_id": "sympy__sympy-24102", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/parsing/mathematica.py\n+++ b/sympy/parsing/mathematica.py\n@@ -100,7 +100,7 @@\n     _re_comment = r\"\\(\\*.*?\\*\\)\"\n     _re_number = r\"((?:[0-9]+\\.?[0-9]*|\\.[0-9]+)(?:[eE][-+]?[0-9]+)?)\"\n     _re_string = r'\"(?:\\\\.|[^\"])*\"'\n-    _re_name = r\"[a-zA-Z][a-zA-Z0-9]*\"\n+    _re_name = r\"[^\\W\\d_]\\w*\"  # Allows Unicode identifiers starting with a letter\n     _re_operator = r\"(\\+\\+|--|\\|\\||&&|>=|<=|!=|===|->|=>|:=|\\.|\\+|-|\\*|/|\\^|>|<|=|!|@|~|\\?|:|&)\"\n     _re_paren = r\"(\\(|\\)|\\[|\\]|\\{|\\})\"\n     _re_space = r\"(\\s+)\"\n```\n"}
{"instance_id": "sympy__sympy-24152", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/physics/quantum/tensorproduct.py\n+++ b/sympy/physics/quantum/tensorproduct.py\n@@ -1,16 +1,18 @@\n from sympy import Expr, Add, Mul, Pow, sympify, Matrix, Symbol\n from sympy.core.compatibility import u, is_sequence\n from sympy.core.decorators import _sympifyit\n+from sympy.core.mul import Mul\n from sympy.matrices import matrix_tensor_product\n from sympy.printing.pretty.stringpict import prettyForm\n \n from sympy.physics.quantum.dagger import Dagger\n from sympy.physics.quantum.operator import Operator, OuterProduct\n from sympy.physics.quantum.state import KetBase, BraBase, StateBase, Ket, Bra\n-from sympy.physics.quantum.qexpr import QExpr\n+from sympy.physics.quantum.qexpr import QExpr, _qsympify_sequence\n from sympy.physics.quantum.matrixutils import numpy_ndarray, scipy_sparse_matrix\n from sympy.physics.quantum.trace import Tr\n from sympy.physics.quantum.represent import represent\n+\n \n # Needed for partial trace\n from sympy.physics.quantum.density import Density\n@@ -100,9 +102,10 @@\n             if isinstance(args[i], Add):\n                 for aa in args[i].args:\n                     tp = TensorProduct(*args[:i] + (aa,) + args[i + 1:])\n-                    if isinstance(tp, TensorProduct):\n-                        tp = tp._eval_expand_tensorproduct()\n-                    add_args.append(tp)\n+                    c_part, nc_part = tp.args_cnc()\n+                    if len(nc_part)==1 and isinstance(nc_part[0], TensorProduct):\n+                        nc_part = (nc_part[0]._eval_expand_tensorproduct(), )\n+                    add_args.append(Mul(*c_part)*Mul(*nc_part))\n                 break\n \n         if add_args:\n```\n"}
{"instance_id": "sympy__sympy-24213", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "```patch\n--- a/sympy/physics/units/unitsystem.py\n+++ b/sympy/physics/units/unitsystem.py\n@@ -170,13 +170,16 @@\n             return factor ** exp_factor, dim ** (exp_factor * exp_dim)\n         elif isinstance(expr, Add):\n             factor, dim = self._collect_factor_and_dimension(expr.args[0])\n+            dimsys = self.get_dimension_system()\n+            dim_deps = dimsys.get_dimensional_dependencies(dim)\n             for addend in expr.args[1:]:\n                 addend_factor, addend_dim = \\\n                     self._collect_factor_and_dimension(addend)\n-                if dim != addend_dim:\n+                addend_dim_deps = dimsys.get_dimensional_dependencies(addend_dim)\n+                if dim_deps != addend_dim_deps:\n                     raise ValueError(\n                         'Dimension of \"{}\" is {}, '\n-                        'but it should be {}'.format(\n+                        'but it should be equivalent to {}'.format(\n                             addend, addend_dim, dim))\n                 factor += addend_factor\n             return factor, dim\n```\n"}
{"instance_id": "sympy__sympy-24909", "model_name_or_path": "swebench-lite_fixed-chunking_BGE-base_gemini-2.5-pro_generations_swebench-lite", "model_patch": "\n"}
