```patch
--- a/sklearn/ensemble/_iforest.py
+++ b/sklearn/ensemble/_iforest.py
@@ -1,6 +1,7 @@
 """Isolation Forest."""
 
 # Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>
+#          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
 #          Gael Varoquaux
 # License: BSD 3 clause
 
@@ -10,7 +11,7 @@
 from ..tree import ExtraTreeRegressor
 from ..utils import check_random_state, check_array, gen_batches
 from ..utils.validation import check_is_fitted, _num_samples
-from ._bagging import BaseBagging
+from ._bagging import BaseBagging, _get_n_samples_bootstrap
 
 
 __all__ = ["IsolationForest"]
@@ -117,6 +118,11 @@
     max_features : int or float, optional (default=1.0)
         The number of features to draw from X to train each base estimator.
 
+        - If int, then draw `max_features` features.
+        - If float, then draw `max_features * X.shape[1]` features.
+
+        Note: using a float number less than 1.0 or integer less than number
+        of features will enable feature subsampling and leads to a longer
         - If int, then draw `max_features` features.
         - If float, then draw `max_features * X.shape[1]` features.
 
@@ -135,6 +141,11 @@
     verbose : int, optional (default=0)
         Controls the verbosity of the tree building process.
 
+    warm_start : bool, optional (default=False)
+        When set to ``True``, reuse the solution of the previous call to fit
+        and add more estimators to the ensemble, otherwise, just fit a whole
+        new forest. See :term:`the Glossary <warm_start>`.
+
     Attributes
     ----------
     estimators_ : list of DecisionTreeClassifier
@@ -171,7 +182,8 @@
                  max_features=1.0,
                  bootstrap=False,
                  n_jobs=None,
-                 random_state=None, verbose=0):
+                 random_state=None, verbose=0,
+                 warm_start=False):
         super().__init__(
             base_estimator=ExtraTreeRegressor(
                 max_depth=None,
@@ -188,7 +200,8 @@
             max_features=max_features,
             bootstrap=bootstrap,
             bootstrap_features=False,
-            oob_score=False)
+            oob_score=False,
+            warm_start=warm_start)
 
         self.behaviour = behaviour
         self.contamination = contamination
@@ -200,6 +213,7 @@
         self.n_jobs = n_jobs
         self.random_state = random_state
         self.verbose = verbose
+        self.warm_start = warm_start
 
     def _set_oob_score(self, X, y):
         raise NotImplementedError("OOB score not supported by iforest")
@@ -210,6 +224,10 @@
         # behaviour parameter.
         if self.behaviour != 'old':
             self.contamination = contamination
+
+        if not isinstance(self.warm_start, bool):
+            raise TypeError("warm_start must be bool")
+
         return self
 
     def fit(self, X, y=None, sample_weight=None):
@@ -249,6 +267,10 @@
         # ensure that max_sample is in [1, n_samples]:
         n_samples = X.shape[0]
 
+        if not (0. < self.max_samples <= 1. or
+                1 <= self.max_samples <= n_samples):
+            raise ValueError("max_samples must be in (0, 1] or [1, {}]. "
+                             "Got {}" .format(n_samples, self.max_samples))
         if isinstance(self.max_samples, numbers.Integral):
             max_samples = self.max_samples
         else:  # float
@@ -256,10 +278,6 @@
 
         self.max_samples_ = max_samples
 
-        if not (0. < self.max_samples_ <= n_samples):
-            raise ValueError("max_samples must be in (0, 1], got %r" %
-                             self.max_samples)
-
         super().fit(X, y, sample_weight=sample_weight)
         return self
 
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -10,7 +10,7 @@
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raises, assert_true
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_less
 from sklearn.utils.testing import assert_no_warnings
@@ -20,7 +20,7 @@
 from sklearn.model_selection import ParameterGrid
 from sklearn.externals import six
 
-from sklearn.ensemble import IsolationForest
+from sklearn.ensemble import IsolationForest, RandomForestRegressor
 from sklearn.ensemble.iforest import _average_path_length
 from sklearn.metrics import roc_auc_score
 from sklearn.utils.random import sample_without_replacement
@@ -297,3 +297,27 @@
     assert_raises(ValueError, clf.fit, X)
     clf = IsolationForest(max_samples=1000)
     assert_raises(ValueError, clf.fit, X)
+
+
+def test_iforest_warm_start():
+    """Test Isolation Forest warm start."""
+    X, y = make_blobs(n_samples=100, random_state=0)
+    X = X.astype(np.float32)
+
+    # Test warm_start=True
+    clf = IsolationForest(n_estimators=5, max_samples=10, random_state=1,
+                          warm_start=True)
+    clf.fit(X)
+
+    assert_equal(len(clf.estimators_), 5)
+    assert_equal(clf.n_estimators, 5)
+
+    clf.n_estimators += 5
+    clf.fit(X)
+
+    assert_equal(len(clf.estimators_), 10)
+    assert_equal(clf.n_estimators, 10)
+
+    # Test warm_start=False
+    clf = IsolationForest(n_estimators=5, random_state=1, warm_start=False)
+    clf.fit(X)
```
