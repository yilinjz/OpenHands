```patch
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -1,5 +1,6 @@
 """Ridge regression"""
 
+# Author: Mathieu Blondel <mathieu@mblondel.org>
 #         Reuben Fletcher-Costin <reuben.fletchercostin@gmail.com>
 #         Fabian Pedregosa <fabian.pedregosa@inria.fr>
 #         Michael Eickenberg <michael.eickenberg@nsi.edu>
@@ -12,6 +13,7 @@
 import warnings
 
 import numpy as np
+from numpy import linalg
 from scipy import linalg
 from scipy import sparse
 from scipy.sparse import linalg as sp_linalg
@@ -24,6 +26,7 @@
 from ..utils.sparsefuncs import mean_variance_axis
 from ..utils import check_random_state
 from ..externals import six
+from ..model_selection import GridSearchCV
 from ..multiclass import OneVsRestClassifier
 from ..preprocessing._data import _handle_zeros_in_scale, _scale_axis1
 from ..utils.multiclass import check_classification_targets
@@ -1268,15 +1271,26 @@
     def __init__(self, alphas=(0.1, 1.0, 10.0),
                  fit_intercept=True, normalize=False, scoring=None,
                  cv=None, class_weight=None):
+                 store_cv_values=False):
         super(RidgeClassifierCV, self).__init__(
             alpha=0.0, fit_intercept=fit_intercept, normalize=normalize,
             class_weight=class_weight)
         self.alphas = np.asarray(alphas)
         self.scoring = scoring
         self.cv = cv
+        self.store_cv_values = store_cv_values
 
     # ... (rest of __init__ docstring) ...
 
+    store_cv_values : boolean, default=False
+        Flag indicating if the cross-validation values corresponding to
+        each alpha should be stored in the `cv_values_` attribute (see
+        below). This flag is only compatible with `cv=None` (i.e. using
+        Leave-One-Out Cross-Validation).
+
     Attributes
     ----------
-    cv_values_ : array, shape = [n_samples, n_alphas] or \
-                 shape = [n_samples, n_responses, n_alphas], optional
-        Cross-validation values for each alpha (if store_cv_values=True and
-        cv=None). After fit() has been called, this attribute will contain
-        the mean squared errors (by default) or the values of the {loss,score}_func
-        function (if provided in the constructor).
+    cv_values_ : array, shape = [n_samples, n_alphas] or \
+                 shape = [n_samples, n_classes, n_alphas], optional
+        Cross-validation values for each alpha. This is only available
+        if `cv=None` and `store_cv_values=True`. The shape is
+        `(n_samples, n_alphas)` for binary classification (or when only
+        one class is present), and `(n_samples, n_classes, n_alphas)`
+        for multiclass classification. Each value corresponds to the output
+        of the scoring function (or mean squared error if no score func is
+        specified) applied to the internal `RidgeCV` model when `cv=None`.
 
     coef_ : array, shape = [n_features] or [n_classes, n_features]
         Weight vector(s).
@@ -1286,11 +1300,14 @@
         ``fit_intercept = False``.
 
     alpha_ : float
-        Estimated regularization parameter.
+        Estimated regularization parameter. For multiclass classification, this
+        is the average of the estimated `alpha_` per class.
 
     n_iter_ : int
         Number of iterations. Only available for sparse matrix and solver
         'sparse_cg'.
+
+    estimators_ : list of n_classes estimators
+        Estimators used for predictions.
 
     Examples
     --------
@@ -1314,6 +1331,11 @@
     def fit(self, X, y, sample_weight=None):
         """Fit Ridge classifier with built-in cross-validation.
 
+        Parameters
+        ----------
+        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
+            Training vectors, where n_samples is the number of samples
+            and n_features is the number of features.
         Parameters
         ----------
         X : array-like, shape = [n_samples, n_features]
@@ -1330,6 +1352,10 @@
         self : object
         """
         check_classification_targets(y)
+        if self.cv is not None and self.store_cv_values:
+            raise ValueError("cv!=None and store_cv_values=True "
+                             " are incompatible")
+
         self.classes_ = np.unique(y)
         class_weight = self.class_weight
 
@@ -1344,7 +1370,8 @@
         if self.cv is None:
             estimator = RidgeCV(alphas=self.alphas, fit_intercept=self.fit_intercept,
                                 normalize=self.normalize, scoring=self.scoring,
-                                cv=None)
+                                cv=None,
+                                store_cv_values=self.store_cv_values)
             fit_params = {}
         else:
             estimator = Ridge(fit_intercept=self.fit_intercept,
@@ -1364,6 +1391,16 @@
         self.intercept_ = ovr.intercept_
         if len(self.estimators_) == 1:
             self.alpha_ = self.estimators_[0].alpha_
+            if self.store_cv_values:
+                # cv=None case, RidgeCV was used
+                self.cv_values_ = self.estimators_[0].cv_values_
+        elif self.store_cv_values:
+            # cv=None case, RidgeCV was used, multiclass
+            # Stack cv_values_ from each binary classifier
+            cv_values_list = [est.cv_values_ for est in self.estimators_]
+            self.cv_values_ = np.stack(cv_values_list, axis=1)
+            # Store average alpha like RidgeCV does for multi-output
+            self.alpha_ = np.mean([est.alpha_ for est in self.estimators_])
+        else:
+            # Store average alpha like RidgeCV does for multi-output
+            self.alpha_ = np.mean([est.alpha_ for est in self.estimators_])
 
         return self
```
