```patch
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -126,6 +126,10 @@
     verbose : int, optional (default=0)
         Controls the verbosity of the tree building process.
 
+    warm_start : bool, optional (default=False)
+        When set to ``True``, reuse the solution of the previous call to fit
+        and add more estimators to the ensemble, otherwise, just fit a whole
+        new forest. See :term:`the Glossary <warm_start>`.
     Attributes
     ----------
     estimators_ : list of DecisionTreeClassifier
@@ -147,7 +151,8 @@
                  n_jobs=None,
                  behaviour='old',
                  random_state=None,
-                 verbose=0):
+                 verbose=0,
+                 warm_start=False):
         super().__init__(
             base_estimator=ExtraTreeRegressor(
                 max_features=1,
@@ -161,7 +166,8 @@
             max_features=max_features,
             n_jobs=n_jobs,
             random_state=random_state,
-            verbose=verbose)
+            verbose=verbose,
+            warm_start=warm_start)
 
         self.behaviour = behaviour
         self.contamination = contamination
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -1,10 +1,12 @@
 import numpy as np
 import pytest
 
-from sklearn.utils._testing import assert_array_equal, assert_raises
+from sklearn.utils._testing import (assert_array_equal, assert_raises,
+                                    assert_warns)
 from sklearn.ensemble import IsolationForest
 from sklearn.model_selection import ParameterGrid
 from sklearn.datasets import load_boston
+from sklearn.utils import check_random_state
 
 
 # load the boston dataset
@@ -162,3 +164,50 @@
     assert_raises(ValueError, clf.fit, X, y)
     assert_raises(ValueError, clf.fit, X, sample_weight=np.ones(len(X) + 1))
     assert_raises(ValueError, clf.fit, X, sample_weight=-np.ones(len(X)))
+
+
+def test_iforest_warm_start():
+    """Test Isolation Forest warm start."""
+    X, y = load_boston(return_X_y=True)
+    rnd = check_random_state(0)
+
+    # Trained with 100 estimators
+    est = IsolationForest(n_estimators=100, random_state=rnd)
+    est.fit(X)
+
+    # Add 100 more estimators to the forest
+    est.set_params(n_estimators=200, warm_start=True)
+    est.fit(X)
+    assert len(est.estimators_) == 200
+    assert est.n_estimators == 200
+
+    # Should be the same as fitting 200 estimators from scratch
+    est_200 = IsolationForest(n_estimators=200, random_state=rnd)
+    est_200.fit(X)
+
+    # Check trees are the same
+    assert len(est.estimators_) == len(est_200.estimators_)
+    for tree1, tree2 in zip(est.estimators_, est_200.estimators_):
+        assert_array_equal(tree1.tree_.value, tree2.tree_.value)
+        assert_array_equal(tree1.tree_.feature, tree2.tree_.feature)
+        assert_array_equal(tree1.tree_.threshold, tree2.tree_.threshold)
+
+    # Check predictions are the same
+    assert_array_equal(est.decision_function(X), est_200.decision_function(X))
+    assert_array_equal(est.score_samples(X), est_200.score_samples(X))
+    assert_array_equal(est.predict(X), est_200.predict(X))
+
+    # Test warm start honours random state
+    est1 = IsolationForest(n_estimators=100, random_state=0, warm_start=False)
+    est1.fit(X)
+
+    est2 = IsolationForest(n_estimators=100, random_state=0, warm_start=False)
+    est2.fit(X)
+    # Add 100 more estimators to the forest
+    est2.set_params(n_estimators=200, warm_start=True)
+    est2.fit(X)
+
+    est3 = IsolationForest(n_estimators=200, random_state=0, warm_start=False)
+    est3.fit(X)
+
+    assert_array_equal(est2.decision_function(X), est3.decision_function(X))
+
+    # Test warm start with smaller n_estimators
+    est = IsolationForest(n_estimators=100, random_state=0, warm_start=True)
+    est.fit(X)
+    est.set_params(n_estimators=50)
+    assert_raises(ValueError, est.fit, X)
+
+    # Test warm start with no increase in n_estimators
+    est = IsolationForest(n_estimators=100, random_state=0, warm_start=True)
+    est.fit(X)
+    assert_warns(UserWarning, est.fit, X)
```
